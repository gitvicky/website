@inproceedings{conformalized_quantile_regression,
 author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Conformalized Quantile Regression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{gentle_introduction_CP,
author = {Angelopoulos, Anastasios N. and Bates, Stephen},
title = {Conformal Prediction: A Gentle Introduction},
year = {2023},
issue_date = {Mar 2023},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {16},
number = {4},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000101},
doi = {10.1561/2200000101},
abstract = {Black-box machine learning models are now routinely used
        in high-risk settings, like medical diagnostics, which demand
        uncertainty quantification to avoid consequential model failures.
        Conformal prediction (a.k.a. conformal inference) is
        a user-friendly paradigm for creating statistically rigorous
        uncertainty sets/intervals for the predictions of such models.
        Critically, the sets are valid in a distribution-free sense: they
        possess explicit, non-asymptotic guarantees even without
        distributional assumptions or model assumptions. One can
        use conformal prediction with any pre-trained model, such
        as a neural network, to produce sets that are guaranteed
        to contain the ground truth with a user-specified probability,
        such as 90\%. It is easy-to-understand, easy-to-use,
        and general, applying naturally to problems arising in the
        fields of computer vision, natural language processing, deep
        reinforcement learning, and so on.This hands-on introduction is aimed to provide the reader a
        working understanding of conformal prediction and related
        distribution-free uncertainty quantification techniques with
        one self-contained document. We lead the reader through
        practical theory for and examples of conformal prediction
        and describe its extensions to complex machine learning
        tasks involving structured outputs, distribution shift, timeseries,
        outliers, models that abstain, and more. Throughout,
        there are many explanatory illustrations, examples, and
        code samples in Python. With each code sample comes a
        Jupyter notebook implementing the method on a real-data
        example; the notebooks can be accessed and easily run by
        following the code footnotes.},
journal = {Found. Trends Mach. Learn.},
month = {mar},
pages = {494–591},
numpages = {114}
}


@inproceedings{papadopoulos2007conformal,
  title={Conformal prediction with neural networks},
  author={Papadopoulos, Harris and Vovk, Volodya and Gammerman, Alex},
  booktitle={19th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2007)},
  volume={2},
  pages={388--395},
  year={2007},
  organization={IEEE}
}

@book{haykin1994neural,
  title={Neural Networks: A Comprehensive Foundation},
  author={Haykin, Simon},
  year={1994},
  publisher={Prentice Hall PTR}
}

@book{koenker_2005, place={Cambridge}, series={Econometric Society Monographs}, 
title={Quantile Regression}, DOI={10.1017/CBO9780511754098}, 
publisher={Cambridge University Press}, author={Koenker, Roger}, 
year={2005}, collection={Econometric Society Monographs}}





@article{error_residual,
author = {Jing Lei and Max G’Sell and Alessandro Rinaldo and Ryan J. Tibshirani and Larry Wasserman},
title = {Distribution-Free Predictive Inference for Regression},
journal = {Journal of the American Statistical Association},
volume = {113},
number = {523},
pages = {1094-1111},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1307116},
URL = {https://doi.org/10.1080/01621459.2017.1307116},
eprint = {https://doi.org/10.1080/01621459.2017.1307116}
}


@article{shafer2008tutorial,
  title={A Tutorial on Conformal Prediction.},
  author={Shafer, Glenn and Vovk, Vladimir},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={3},
  year={2008}
}

@book{vovk2005algorithmic,
  title={Algorithmic Learning in a Random World},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  year={2005},
  publisher={Springer}
}

@incollection{papadopoulos2008inductive,
    author = {Harris Papadopoulos},
    title = {Inductive Conformal Prediction: Theory and Application to Neural Networks},
    booktitle = {Tools in Artificial Intelligence},
    publisher = {IntechOpen},
    address = {Rijeka},
    year = {2008},
    editor = {Paula Fritzsche},
    chapter = {18},
    doi = {10.5772/6078},
}

@article{tibshirani2019conformal,
  title={Conformal prediction under covariate shift},
  author={Tibshirani, Ryan J. and Foygel Barber, Rina and Candes, Emmanuel and Ramdas, Aaditya},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@InProceedings{exchangeable,
author="Aldous, David J.",
title="Exchangeability and related topics",
booktitle="{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIII --- 1983",
year="1985",
pages="1--198",
isbn="978-3-540-39316-0"
}


@book{bellan2006fundamentals,
  title={Fundamentals of Plasma Physics},
  author={Bellan, Paul M.},
  isbn={9780511562105},
  url={https://books.google.co.uk/books?id=CoPtzQEACAAJ},
  year={2006},
  publisher={Cambridge University Press}
}



@article{Hoelzl2021jorek,
doi = {10.1088/1741-4326/abf99f},
url = {https://dx.doi.org/10.1088/1741-4326/abf99f},
year = {2021},
publisher = {IOP Publishing},
volume = {61},
number = {6},
pages = {065001},
author = {M. Hoelzl and G. T. A. Huijsmans and S. J. P. Pamela and M. Bécoulet and E. Nardon and F.J. Artola and B. Nkonga and C. V. Atanasiu and V. Bandaru and A. Bhole and D. Bonfiglio and A. Cathey and O. Czarny and A. Dvornova and T. Fehér and A. Fil and E. Franck and S. Futatani and M. Gruca and H. Guillard and J. W. Haverkort and I. Holod and D. Hu and S. K. Kim and S. Q. Korving and L. Kos and I. Krebs and L. Kripner and G. Latu and F. Liu and P. Merkel and D. Meshcheriakov and V. Mitterauer and S. Mochalskyy and J. A. Morales and R. Nies and N. Nikulsin and F. Orain and J. Pratt and R. Ramasamy and P. Ramet and C. Reux and K. Särkimäki and N. Schwarz and P. Singh Verma and S. F. Smith and C. Sommariva and E. Strumberger and D. C. van Vugt and M. Verbeek and E. Westerhof and F. Wieschollek and J. Zielinski},
title = {The JOREK non-linear extended MHD code and applications to large-scale instabilities and their control in magnetically confined fusion plasmas},
journal = {Nuclear Fusion},
abstract = {JOREK is a massively parallel fully implicit non-linear extended magneto-hydrodynamic (MHD) code for realistic tokamak X-point plasmas. It has become a widely used versatile simulation code for studying large-scale plasma instabilities and their control and is continuously developed in an international community with strong involvements in the European fusion research programme and ITER organization. This article gives a comprehensive overview of the physics models implemented, numerical methods applied for solving the equations and physics studies performed with the code. A dedicated section highlights some of the verification work done for the code. A hierarchy of different physics models is available including a free boundary and resistive wall extension and hybrid kinetic-fluid models. The code allows for flux-surface aligned iso-parametric finite element grids in single and double X-point plasmas which can be extended to the true physical walls and uses a robust fully implicit time stepping. Particular focus is laid on plasma edge and scrape-off layer (SOL) physics as well as disruption related phenomena. Among the key results obtained with JOREK regarding plasma edge and SOL, are deep insights into the dynamics of edge localized modes (ELMs), ELM cycles, and ELM control by resonant magnetic perturbations, pellet injection, as well as by vertical magnetic kicks. Also ELM free regimes, detachment physics, the generation and transport of impurities during an ELM, and electrostatic turbulence in the pedestal region are investigated. Regarding disruptions, the focus is on the dynamics of the thermal quench (TQ) and current quench triggered by massive gas injection and shattered pellet injection, runaway electron (RE) dynamics as well as the RE interaction with MHD modes, and vertical displacement events. Also the seeding and suppression of tearing modes (TMs), the dynamics of naturally occurring TQs triggered by locked modes, and radiative collapses are being studied.}
}

@Inbook{Hackbusch2017,
author="Hackbusch, Wolfgang",
title="The Poisson Equation",
bookTitle="Elliptic Differential Equations: Theory and Numerical Treatment",
year="2017",
publisher="Springer",
address="Berlin, Heidelberg",
pages="29--42",
abstract="In Section 3.1 the Poisson equation --$\Delta$u=f is introduced, and the uniqueness of the solution is proved. The Green function is defined in Section 3.2. It allows the representation (3.6) of the solution, provided it is existing. Concerning the existence, Theorem 3.13 contains a negative statement (cf. Section 3.3): The Poisson equation with a continuous right-hand side f may possess no classical solution. A sufficient condition for a classical solution is the H{\"o}lder continuity of f as stated in Theorem 3.18. Section 3.4 introduces Green's function for the ball. In the two-dimensional case, Riemann's mapping theorem allows the construction of the Green function for a large class of domains. In Section 3.5 we replace the Dirichlet boundary condition by the Neumann condition. The final Section 3.6 is a short introduction into the integral equation method. The solution of the boundary-value problem can indirectly be obtained by solving an integral equation.",
isbn="978-3-662-54961-2",
doi="10.1007/978-3-662-54961-2_3",
url="https://doi.org/10.1007/978-3-662-54961-2_3"
}


@article{py-pde,
    Author = {David Zwicker},
    Doi = {10.21105/joss.02158},
    Journal = {Journal of Open Source Software},
    Number = {48},
    Pages = {2158},
    Publisher = {The Open Journal},
    Title = {py-pde: A python package for solving partial differential equations},
    Url = {https://doi.org/10.21105/joss.02158},
    Volume = {5},
    Year = {2020}
}

@InProceedings{ronneberger2015unet,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}



@article{
gupta2022multispatiotemporalscale,
title={Towards Multi-spatiotemporal-scale Generalized {PDE} Modeling},
author={Jayesh K Gupta and Johannes Brandstetter},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=dPSTDbGtBY},
note={}
}


@article{GOPAKUMAR2023100464,
title = {Loss landscape engineering via data regulation on PINNs},
journal = {Machine Learning with Applications},
volume = {12},
pages = {100464},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100464},
url = {https://www.sciencedirect.com/science/article/pii/S2666827023000178},
author = {Vignesh Gopakumar and Stanislas Pamela and Debasmita Samaddar},
keywords = {Physics-Informed Neural Networks, Loss landscape, Sparse regularisation, Partial differential equations, Optimisation},
abstract = {Physics-Informed Neural Networks have shown unique utility in parameterising the solution of a well-defined partial differential equation using automatic differentiation and residual losses. Though they provide theoretical guarantees of convergence, in practice the required training regimes tend to be exacting and demanding. Through the course of this paper, we take a deep dive into understanding the loss landscapes associated with a PINN and how that offers some insight as to why PINNs are fundamentally hard to optimise for. We demonstrate how PINNs can be forced to converge better towards the solution, by way of feeding in sparse or coarse data as a regulator. The data regulates and morphs the topology of the loss landscape associated with the PINN to make it easily traversable for the minimiser. Data regulation of PINNs helps ease the optimisation required for convergence by invoking a hybrid unsupervised–supervised training approach, where the labelled data pushes the network towards the vicinity of the solution, and the unlabelled regime fine-tunes it to the solution.}
}


@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
li2021fourier,
title={Fourier Neural Operator for Parametric Partial Differential Equations},
author={Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=c8P9NQVtmnO}
}



@article{balch2019satellite,
  title={Satellite conjunction analysis and the false confidence theorem},
  author={Balch, Michael S. and Martin, Ryan and Ferson, Scott},
  journal={Proceedings of the Royal Society A},
  volume={475},
  number={2227},
  pages={20180565},
  year={2019},
  publisher={The Royal Society Publishing}
}

@article{cella2022validity,
  title={Validity, consonant plausibility measures, and conformal prediction},
  author={Cella, Leonardo and Martin, Ryan},
  journal={International Journal of Approximate Reasoning},
  volume={141},
  pages={110--130},
  year={2022},
  publisher={Elsevier}
}

@article{balch2012mathematical,
  title={Mathematical foundations for a theory of confidence structures},
  author={Balch, Michael S.},
  journal={International Journal of Approximate Reasoning},
  volume={53},
  number={7},
  pages={1003--1019},
  year={2012},
  publisher={Elsevier}
}

@article{hose2021universal,
  title={A universal approach to imprecise probabilities in possibility theory},
  author={Hose, Dominik and Hanss, Michael},
  journal={International Journal of Approximate Reasoning},
  volume={133},
  pages={133--158},
  year={2021},
  publisher={Elsevier}
}

@article{LALONDE2021104696,
title = {Comparison of neural network types and architectures for generating a surrogate aerodynamic wind turbine blade model},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {216},
pages = {104696},
year = {2021},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2021.104696},
url = {https://www.sciencedirect.com/science/article/pii/S0167610521001793},
author = {Eric Rowland Lalonde and Benjamin Vischschraper and Girma Bitsuamlak and Kaoshan Dai},
keywords = {Aerodynamic wind turbine blade model, Multilayer perceptron, Long short-term memory, Convolutional neural network, Surrogate model},
}
@article{Manek_2023,
doi = {10.1088/2632-2153/acb2b3},
url = {https://dx.doi.org/10.1088/2632-2153/acb2b3},
year = {2023},
publisher = {IOP Publishing},
volume = {4},
number = {1},
pages = {015008},
author = {Petr Mánek and Graham Van Goffrier and Vignesh Gopakumar and Nikos Nikolaou and Jonathan Shimwell and Ingo Waldmann},
title = {Fast regression of the tritium breeding ratio in fusion reactors},
journal = {Machine Learning: Science and Technology},
abstract = {The tritium breeding ratio (TBR) is an essential quantity for the design of modern and next-generation D-T fueled nuclear fusion reactors. Representing the ratio between tritium fuel generated in breeding blankets and fuel consumed during reactor runtime, the TBR depends on reactor geometry and material properties in a complex manner. In this work, we explored the training of surrogate models to produce a cheap but high-quality approximation for a Monte Carlo (MC) TBR model in use at the UK Atomic Energy Authority. We investigated possibilities for dimensional reduction of its feature space, reviewed 9 families of surrogate models for potential applicability, and performed hyperparameter optimization. Here we present the performance and scaling properties of these models, the fastest of which, an artificial neural network, demonstrated  and a mean prediction time of , representing a relative speedup of  with respect to the expensive MC model. We further present a novel adaptive sampling algorithm, Quality-Adaptive Surrogate Sampling, capable of interfacing with any of the individually studied surrogates. Our preliminary testing on a toy TBR theory has demonstrated the efficacy of this algorithm for accelerating the surrogate modelling process.}
}


@Article{Baldi2016,
author={Baldi, Pierre
and Cranmer, Kyle
and Faucett, Taylor
and Sadowski, Peter
and Whiteson, Daniel},
title={Parameterized neural networks for high-energy physics},
journal={The European Physical Journal C},
year={2016},
month={04},
day={27},
volume={76},
number={5},
pages={235},
abstract={We investigate a new structure for machine learning classifiers built with neural networks and applied to problems in high-energy physics by expanding the inputs to include not only measured features but also physics parameters. The physics parameters represent a smoothly varying learning task, and the resulting parameterized classifier can smoothly interpolate between them and replace sets of classifiers trained at individual values. This simplifies the training process and gives improved performance at intermediate values, even for complex problems requiring deep learning. Applications include tools parameterized in terms of theoretical model parameters, such as the mass of a particle, which allow for a single network to provide improved discrimination across a range of masses. This concept is simple to implement and allows for optimized interpolatable results.},
issn={1434-6052},
doi={10.1140/epjc/s10052-016-4099-4},
url={https://doi.org/10.1140/epjc/s10052-016-4099-4}
}

@article{autoencoders,
author = {Geoffrey E. Hinton  and Ruslan Salakhutdinov },
title = {Reducing the dimensionality of data with neural networks},
journal = {Science},
volume = {313},
number = {5786},
pages = {504--507},
year = {2006},
doi = {10.1126/science.1127647},
URL = {https://www.science.org/doi/abs/10.1126/science.1127647},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1127647},
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.}}





@INPROCEEDINGS{resnets,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Deep residual learning for image recognition}, 
  year={2016},
  doi={10.1109/CVPR.2016.90}}


@article{Wen_2023,
	doi = {10.1039/d2ee04204e},
  
	url = {https://doi.org/10.10392Fd2ee04204e},
  
	year = 2023,
	publisher = {Royal Society of Chemistry ({RSC})},
  
	volume = {16},
  
	number = {4},
  
	pages = {1732--1741},
  
	author = {Gege Wen and Zongyi Li and Qirui Long and Kamyar Azizzadenesheli and Anima Anandkumar and Sally M. Benson},
  
	title = {Real-time high-resolution {CO}$_2$ geological storage prediction using nested Fourier neural operators},
  
	journal = {Energy \& Environmental Science}
}

@Article{PIML,
author={Karniadakis, George Em
and Kevrekidis, Ioannis G.
and Lu, Lu
and Perdikaris, Paris
and Wang, Sifan
and Yang, Liu},
title={Physics-informed machine learning},
journal={Nature Reviews Physics},
year={2021},
volume={3},
number={6},
pages={422--440},
abstract={Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
issn={2522-5820},
doi={10.1038/s42254-021-00314-5},
url={https://doi.org/10.1038/s42254-021-00314-5}
}

@article{simintelligence,
  doi = {10.48550/ARXIV.2112.03235},
  
  url = {https://arxiv.org/abs/2112.03235},
  
  author = {Lavin, Alexander and Krakauer, David and Zenil, Hector and Gottschlich, Justin and Mattson, Tim and Brehmer, Johann and Anandkumar, Anima and Choudry, Sanjay and Rocki, Kamil and Baydin, Atılım Güneş and Prunkl, Carina and Paige, Brooks and Isayev, Olexandr and Peterson, Erik and McMahon, Peter L. and Macke, Jakob and Cranmer, Kyle and Zhang, Jiaxin and Wainwright, Haruko and Hanuka, Adi and Veloso, Manuela and Assefa, Samuel and Zheng, Stephan and Pfeffer, Avi},
  
  keywords = {Artificial Intelligence (cs.AI), Computational Engineering, Finance, and Science (cs.CE), Machine Learning (cs.LG), Mathematical Software (cs.MS), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Simulation intelligence: Towards a new generation of scientific methods},
  
  journal = {arXiv:2112.03235},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{lam2022graphcast,
    author = {Remi Lam  and Alvaro Sanchez-Gonzalez  and Matthew Willson  and Peter Wirnsberger  and Meire Fortunato  and Ferran Alet  and Suman Ravuri  and Timo Ewalds  and Zach Eaton-Rosen  and Weihua Hu  and Alexander Merose  and Stephan Hoyer  and George Holland  and Oriol Vinyals  and Jacklynn Stott  and Alexander Pritzel  and Shakir Mohamed  and Peter Battaglia },
    title = {Learning skillful medium-range global weather forecasting},
    journal = {Science},
    volume = {382},
    number = {6677},
    pages = {1416-1421},
    year = {2023},
    doi = {10.1126/science.adi2336},
    URL = {https://www.science.org/doi/abs/10.1126/science.adi2336},
    eprint = {https://www.science.org/doi/pdf/10.1126/science.adi2336}
}



@inproceedings{pathak2022fourcastnet,
author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
title = {FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592979.3593412},
doi = {10.1145/3592979.3593412},
abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {13},
numpages = {11},
keywords = {deep learning, fourier neural operator, transformer, extreme weather, climate change},
location = {Davos, Switzerland},
series = {PASC '23}
}

@article{jiang2020meshfreeflownet,
      title={MeshfreeFlowNet: A physics-constrained deep continuous space-time super-resolution framework}, 
      author={Chiyu M. Jiang and Soheil Esmaeilzadeh and Kamyar Azizzadenesheli and Karthik Kashinath and Mustafa Mustafa and Hamdi A. Tchelepi and Philip Marcus and Prabhat and Anima Anandkumar},
      year={2020},
      journal={arXiv:2005.01463},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
pfaff2021learning,
title={Learning Mesh-Based Simulation with Graph Networks},
author={Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter Battaglia},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=roNqYL0_XP}
}

@article{van_de_Plassche_2020,
	doi = {10.1063/1.5134126},
  
	url = {https://doi.org/10.10632F1.5134126},
  
	year = 2020,
 
	publisher = {{AIP} Publishing},
  
	volume = {27},
  
	number = {2},
  
	pages = {022310},
  
	author = {K. L. van de Plassche and J. Citrin and C. Bourdelle and Y. Camenen and F. J. Casson and V. I. Dagnelie and F. Felici and A. Ho and S. Van Mulders and},
  
	title = {Fast modeling of turbulent transport in fusion plasmas using neural networks},
  
	journal = {Physics of Plasmas}
}

@article{Gopakumar_2020,
doi = {10.1088/2632-2153/ab5639},
url = {https://dx.doi.org/10.1088/2632-2153/ab5639},
year = {2020},
publisher = {IOP Publishing},
volume = {1},
number = {1},
pages = {015006},
author = {Vignesh Gopakumar and Debasmita Samaddar},
title = {Image mapping the temporal evolution of edge characteristics in Tokamaks using neural networks},
journal = {Machine Learning: Science and Technology},
abstract = {We propose a method for data-driven modelling of the temporal evolution of the plasma and neutral characteristics at the edge of a tokamak using neural networks. Our method proposes a novel fully convolutional network to serve as function approximators in modelling complex nonlinear phenomenon observed in the multi-physics representations of high energy physics. More specifically, we target the evolution of the temperatures, densities and parallel velocities of the electrons, ions and neutral particles at the edge. The central challenge in this context is in modelling together the different physics principles encapsulated in the evolution of plasma and the neutrals. We demonstrate that the inherent differences in nonlinear behaviour can be addressed by forking the network to process the plasma and neutral information individually before integrating as a holistic system. Our approach takes into account the spatial dependencies of the physics parameters across the grid while performing the temporal mappings, ensuring that the underlying physics is factored in and not lost to the black-box. Having used the conventional edge plasma-neutral solver code SOLPS to build the synthetic dataset, our method demonstrates a computational gain of over 5 orders of magnitude over it without a considerable compromise on accuracy.}
}

@article{GENEVA2020109056,
title = {Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks},
journal = {Journal of Computational Physics},
volume = {403},
pages = {109056},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.109056},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119307612},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Physics-informed machine learning, Auto-regressive model, Deep neural networks, Convolutional encoder-decoder, Uncertainty quantification, Dynamic partial differential equations},
abstract = {In recent years, deep learning has proven to be a viable methodology for surrogate modeling and uncertainty quantification for a vast number of physical systems. However, in their traditional form, such models can require a large amount of training data. This is of particular importance for various engineering and scientific applications where data may be extremely expensive to obtain. To overcome this shortcoming, physics-constrained deep learning provides a promising methodology as it only utilizes the governing equations. In this work, we propose a novel auto-regressive dense encoder-decoder convolutional neural network to solve and model non-linear dynamical systems without training data at a computational cost that is potentially magnitudes lower than standard numerical solvers. This model includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest at each time-step. We rigorously test this model on several non-linear transient partial differential equation systems including the turbulence of the Kuramoto-Sivashinsky equation, multi-shock formation and interaction with 1D Burgers' equation and 2D wave dynamics with coupled Burgers' equations. For each system, the predictive results and uncertainty are presented and discussed together with comparisons to the results obtained from traditional numerical analysis methods.}
}

@article{ZHU2018415,
title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
journal = {Journal of Computational Physics},
volume = {366},
pages = {415-447},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118302341},
author = {Yinhao Zhu and Nicholas Zabaras},
keywords = {Uncertainty quantification, Bayesian neural networks, Convolutional encoder–decoder networks, Deep learning, Porous media flows},
abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder–decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data-intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification tasks for flow in heterogeneous media using limited training data consisting of permeability realizations and the corresponding velocity and pressure fields. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to 4225 where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.}
}

@article{ALHAJERI202234,
title = {Physics-informed machine learning modeling for predictive control using noisy data},
journal = {Chemical Engineering Research and Design},
volume = {186},
pages = {34-49},
year = {2022},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2022.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0263876222003847},
author = {Mohammed S. Alhajeri and Fahim Abdullah and Zhe Wu and Panagiotis D. Christofides},
keywords = {Process control, Model predictive control, Nonlinear processes, Machine Learning, Recurrent neural networks, Aspen Plus Dynamics},
abstract = {Due to the occurrence of over-fitting at the learning phase, the modeling of chemical processes via artificial neural networks (ANN) by using corrupted data (i.e., noisy data) is an ongoing challenge. Therefore, this work investigates the effect of both Gaussian and non-Gaussian noise on the performance of process-structure based recurrent neural networks (RNN) models, which take the form of partially-connected RNN models in this work, that are used to approximate a class of multi-input-multi-outputs nonlinear systems. Furthermore, two different techniques, specifically Monte Carlo dropout and co-teaching, are utilized in the development of partially-connected RNN models. These two techniques are employed to reduce the over-fitting in ANNs when noisy data is used in the training process and, hence, to improve the open-loop accuracy as well as the closed-loop performance under a Lyapunov-based model predictive controller (MPC). Aspen Plus Dynamics, a well-known high-fidelity process simulator, is used to simulate a large-scale chemical process application in order to demonstrate the anticipated improvements in both open-loop approximation and closed-loop controller performance in the presence of Gaussian and non-Gaussian noise in the data set using physics-informed RNNs.}
}

@inproceedings{gal2016dropout,
      title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      booktitle = {International Conference on Machine Learning}
}



@misc{lakshminarayanan2017simple,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@INPROCEEDINGS{uq_survey,
  author={Sudret, Bruno and Marelli, Stefano and Wiart, Joe},
  booktitle={2017 11th European Conference on Antennas and Propagation (EUCAP)}, 
  title={Surrogate models for uncertainty quantification: An overview}, 
  year={2017},
  volume={},
  number={},
  pages={793-797},
  doi={10.23919/EuCAP.2017.7928679}}

  @article{ABDAR2021243,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243--297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental res4earch challenges and directions associated with UQ.}
}

@article{martin2019false,
  title={False confidence, non-additive beliefs, and valid statistical inference},
  author={Martin, Ryan},
  journal={International Journal of Approximate Reasoning},
  volume={113},
  pages={39--73},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{vovk2012conditional,
  title={Conditional validity of inductive conformal predictors},
  author={Vovk, Vladimir},
  booktitle={Asian Conference on Machine Learning},
  year={2012},
}


@article{zou2022neuraluq,
author = {Zou, Zongren and Meng, Xuhui and Psaros, Apostolos F. and Karniadakis, George E.},
title = {NeuralUQ: A Comprehensive Library for Uncertainty Quantification in Neural Differential Equations and Operators},
journal = {SIAM Review},
volume = {66},
number = {1},
pages = {161-190},
year = {2024},
doi = {10.1137/22M1518189},
URL = { 
        https://doi.org/10.1137/22M1518189},
eprint = {https://doi.org/10.1137/22M1518189},
    abstract = { Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs. }
}


@Article{Bertone2019,
  author       = {Gianfranco Bertone and Marc P. Deisenroth and Jong S. Kim and Sebastian Liem and Roberto {Ruiz de Austri} and Max Welling},
  journaltitle = {Physics of the Dark Universe},
  title        = {Accelerating the BSM interpretation of LHC data with machine learning},
  pages        = {100293},
  volume       = {24},
  journal      = {Physics of the Dark Universe},
  year         = {2019},
}

@article{Psaros2023,
title = {Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
journal = {Journal of Computational Physics},
volume = {477},
pages = {111902},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111902},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122009652},
author = {Apostolos F. Psaros and Xuhui Meng and Zongren Zou and Ling Guo and George Em Karniadakis},
keywords = {Scientific machine learning, Stochastic partial differential equations, Uncertainty quantification, Physics-informed neural networks, Neural operator learning, Bayesian framework},
abstract = {Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. This is because in addition to aleatoric uncertainty associated with noisy data, there is also uncertainty due to limited data, but also due to NN hyperparameters, overparametrization, optimization and sampling errors as well as model misspecification. Although there are some recent works on uncertainty quantification (UQ) in NNs, there is no systematic investigation of suitable methods towards quantifying the total uncertainty effectively and efficiently even for function approximation, and there is even less work on solving partial differential equations and learning operator mappings between infinite-dimensional function spaces using NNs. In this work, we present a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches. To demonstrate the applicability and reliability of our framework, we present an extensive comparative study in which various methods are tested on prototype problems, including problems with mixed input-output data, and stochastic problems in high dimensions. In the Appendix, we include a comprehensive description of all the UQ methods employed. Further, to help facilitate the deployment of UQ in Scientific Machine Learning research and practice, we present and develop in [1] an open-source Python library (github.com/Crunch-UQ4MI/neuraluq), termed NeuralUQ, that is accompanied by an educational tutorial and additional computational experiments.}
}

@Article{Courtois2023,
author={Courtois, Adrien
and Morel, Jean-Michel
and Arias, Pablo},
title={Can neural networks extrapolate? Discussion of a theorem by Pedro Domingos},
journal={Revista de la Real Academia de Ciencias Exactas, F{\'i}sicas y Naturales. Serie A. Matem{\'a}ticas},
year={2023},
month={Mar},
day={02},
volume={117},
number={2},
pages={79},
abstract={Neural networks trained on large datasets by minimizing a loss have become the state-of-the-art approach for resolving data science problems, particularly in computer vision, image processing and natural language processing. In spite of their striking results, our theoretical understanding about how neural networks operate is limited. In particular, what are the extrapolation capabilities of trained neural networks if any? In this paper we discuss a theorem of Domingos stating that ``every machine learned by continuous gradient descent is approximately a kernel machine''. According to Domingos, this fact leads to conclude that all machines trained on data are mere kernel machines. We first extend Domingo's result in the discrete case and to networks with vector-valued output. We then study its relevance and significance on simple examples. We find that in simple cases, the ``neural tangent kernel'' arising in Domingos' theorem does provide understanding of the networks' predictions. When the task given to the network grows in complexity, the interpolation capability of the network can be effectively explained by Domingos' theorem, and no extrapolation capability of the network beyond its learning domain is found, even when the network's structure would allow for it. We illustrate this fact on a classic perception theory problem: recovering a shape from its boundary.},
issn={1579-1505},
doi={10.1007/s13398-023-01411-z},
url={https://doi.org/10.1007/s13398-023-01411-z}
}

@book{atmospheric_modeling_book, 
    place={Cambridge}, 
    title={Atmospheric Modeling, Data Assimilation and Predictability}, 
    publisher={Cambridge University Press}, 
    author={Kalnay, Eugenia}, 
    year={2002}
}


@article{panguweather,
  title={Accurate medium-range global weather forecasting with 3D neural networks},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={Nature},
  pages={1--6},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{keisler,
	title = {Forecasting Global Weather with Graph Neural Networks},
	author = {Keisler, Ryan},
	year = {2022},
    journal={arXiv preprint arXiv:2202.07575}
}

@article{neural_lam,
  title={Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks}, 
  author={Joel Oskarsson and Tomas Landelius and Marc Peter Deisenroth and Fredrik Lindsten},
  year={2024},
  journal={arXiv preprint arXiv:2406.04759}
}

@article{meps,
	title = {{AROME}-{MetCoOp}: A Nordic Convective-Scale Operational Weather Prediction Model},
	shorttitle = {{AROME}-{MetCoOp}},
	journaltitle = {Weather and Forecasting},
	journal = {Weather and Forecasting},
	author = {M\"{u}ller, Malte and Homleid, Mariken and Ivarsson, Karl-Ivar and K{\o}ltzow, Morten A. {\O} and Lindskog, Magnus and Midtb{\o}, Knut Helge and Andrae, Ulf and Aspelien, Trygve and Berggren, Lars and Bj{\o}rge, Dag and Dahlgren, Per and Kristiansen, J{\o}rn and Randriamampianina, Roger and Ridal, Martin and Vignes, Ole},
	date = {2017-04-01},
	year = {2017},
    publisher = {American Meteorological Society}
}

@article{fengwu,
	title = {{FengWu}: Pushing the Skillful Global Medium-range Weather Forecast beyond 10 Days Lead},
	author = {Chen, Kang and Han, Tao and Gong, Junchao and Bai, Lei and Ling, Fenghua and Luo, Jing-Jia and Chen, Xi and Ma, Leiming and Zhang, Tianning and Su, Rui and Ci, Yuanzheng and Li, Bin and Yang, Xiaokang and Ouyang, Wanli},
	year = {2023},
    journal={arXiv preprint arXiv:2304.02948}
}

@misc{weatherbench2,
      title={WeatherBench 2: A benchmark for the next generation of data-driven global weather models}, 
      author={Stephan Rasp and Stephan Hoyer and Alexander Merose and Ian Langmore and Peter Battaglia and Tyler Russel and Alvaro Sanchez-Gonzalez and Vivian Yang and Rob Carver and Shreya Agrawal and Matthew Chantry and Zied Ben Bouallegue and Peter Dueben and Carla Bromberg and Jared Sisk and Luke Barrington and Aaron Bell and Fei Sha},
      year={2024},
      eprint={2308.15560},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@misc{fuxi,
      title={FuXi: A cascade machine learning forecasting system for 15-day global weather forecast}, 
      author={Lei Chen and Xiaohui Zhong and Feng Zhang and Yuan Cheng and Yinghui Xu and Yuan Qi and Hao Li},
      year={2023},
      eprint={2306.12873},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@article{swin_vrnn,
author = {Hu, Yuan and Chen, Lei and Wang, Zhibin and Li, Hao},
title = {SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {15},
number = {2},
pages = {e2022MS003211},
keywords = {medium-range weather forecasting, data-driven method, ensemble forecast, learned distribution perturbation},
doi = {https://doi.org/10.1029/2022MS003211},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003211},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2022MS003211},
note = {e2022MS003211 2022MS003211},
year = {2023}
}

@book{fundamentals_of_nwp,
  title={Fundamentals of numerical weather prediction},
  author={Coiffier, Jean},
  year={2011},
  publisher={Cambridge University Press}
}

@ARTICLE{GNNs2009,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}


@InProceedings{Yin_2022_CVPR,
    author    = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M. and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
    title     = {A-ViT: Adaptive Tokens for Efficient Vision Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {06},
    year      = {2022},
    pages     = {10809-10818}
}

@article{GENEVA2022272,
title = {Transformers for modeling physical systems},
journal = {Neural Networks},
volume = {146},
pages = {272-289},
year = {2022},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021004500},
author = {Nicholas Geneva and Nicholas Zabaras},
keywords = {Transformers, Deep learning, Self-attention, Physics, Koopman, Surrogate modeling},
abstract = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.11Code available at: https://github.com/zabaras/transformer-physx.}
}


@InProceedings{pmlr-v119-sanchez-gonzalez20a,
  title = 	 {Learning to Simulate Complex Physics with Graph Networks},
  author =       {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8459--8468},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sanchez-gonzalez20a/sanchez-gonzalez20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sanchez-gonzalez20a.html},
  abstract = 	 {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework—which we term "Graph Network-based Simulators" (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.}
}

@inproceedings{
brandstetter2022message,
title={Message Passing Neural {PDE} Solvers},
author={Johannes Brandstetter and Daniel E. Worrall and Max Welling},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vSix3HPYKSU}
}

@article{
li2023transformer,
title={Transformer for Partial Differential Equations{\textquoteright} Operator Learning},
author={Zijie Li and Kazem Meidani and Amir Barati Farimani},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=EPPqt3uERT},
note={}
}

@article{Chandrasekhar,
  title = {Stochastic Problems in Physics and Astronomy},
  author = {Chandrasekhar, S.},
  journal = {Rev. Mod. Phys.},
  volume = {15},
  issue = {1},
  pages = {1--89},
  numpages = {0},
  year = {1943},
  month = {1},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.15.1},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.15.1}
}

@Book{thermodynamics_textbook,
author={Tipler, Paul Allen, 1933-},
title={Physics for scientists and engineers. Volume 1, Mechanics, oscillations and waves, thermodynamics},
year={2008},
publisher={Sixth edition. New York : W.H. Freeman, [2008] {\textcopyright}2008},
abstract={xxxi, 692 pages : illustrations ; 28 cm},
note={Contains chapters 1-20, R of complete sixth edition.;Includes bibliographical references and index.},
url={https://search.library.wisc.edu/catalog/9910077755502121}
}

@article{Gopakumar_2024,
doi = {10.1088/1741-4326/ad313a},
url = {https://dx.doi.org/10.1088/1741-4326/ad313a},
year = {2024},
month = {4},
publisher = {IOP Publishing},
volume = {64},
number = {5},
pages = {056025},
author = {Vignesh Gopakumar and Stanislas Pamela and Lorenzo Zanisi and Zongyi Li and Ander Gray and Daniel Brennand and Nitesh Bhatia and Gregory Stathopoulos and Matt Kusner and Marc Peter Deisenroth and Anima Anandkumar and the JOREK Team and MAST Team},
title = {Plasma surrogate modelling using Fourier neural operators},
journal = {Nuclear Fusion},
abstract = {Predicting plasma evolution within a Tokamak reactor is crucial to realizing the goal of sustainable fusion. Capabilities in forecasting the spatio-temporal evolution of plasma rapidly and accurately allow us to quickly iterate over design and control strategies on current Tokamak devices and future reactors. Modelling plasma evolution using numerical solvers is often expensive, consuming many hours on supercomputers, and hence, we need alternative inexpensive surrogate models. We demonstrate accurate predictions of plasma evolution both in simulation and experimental domains using deep learning-based surrogate modelling tools, viz., Fourier neural operators (FNO). We show that FNO has a speedup of six orders of magnitude over traditional solvers in predicting the plasma dynamics simulated from magnetohydrodynamic models, while maintaining a high accuracy (Mean Squared Error in the normalised domain ). Our modified version of the FNO is capable of solving multi-variable Partial Differential Equations, and can capture the dependence among the different variables in a single model. FNOs can also predict plasma evolution on real-world experimental data observed by the cameras positioned within the MAST Tokamak, i.e. cameras looking across the central solenoid and the divertor in the Tokamak. We show that FNOs are able to accurately forecast the evolution of plasma and have the potential to be deployed for real-time monitoring. We also illustrate their capability in forecasting the plasma shape, the locations of interactions of the plasma with the central solenoid and the divertor for the full (available) duration of the plasma shot within MAST. The FNO offers a viable alternative for surrogate modelling as it is quick to train and infer, and requires fewer data points, while being able to do zero-shot super-resolution and getting high-fidelity solutions.}
}

@Article{Azizzadenesheli2024,
author={Azizzadenesheli, Kamyar
and Kovachki, Nikola
and Li, Zongyi
and Liu-Schiaffini, Miguel
and Kossaifi, Jean
and Anandkumar, Anima},
title={Neural operators for accelerating scientific simulations and design},
journal={Nature Reviews Physics},
year={2024},
month={04},
day={08},
abstract={Scientific discovery and engineering design are currently limited by the time and cost of physical experiments. Numerical simulations are an alternative approach but are usually intractable for complex real-world problems. Artificial intelligence promises a solution through fast data-driven surrogate models. In particular, neural operators present a principled framework for learning mappings between functions defined on continuous domains, such as spatiotemporal processes and partial differential equations. Neural operators can extrapolate and predict solutions at new locations unseen during training. They can be integrated with physics and other domain constraints enforced at finer resolutions to obtain high-fidelity solutions and good generalization. Neural operators are differentiable, so they can directly optimize parameters for inverse design and other inverse problems. Neural operators can therefore augment, or even replace, existing numerical simulators in many applications, such as computational fluid dynamics, weather forecasting and material modelling, providing speedups of four to five orders of magnitude.},
issn={2522-5820},
doi={10.1038/s42254-024-00712-5},
url={https://doi.org/10.1038/s42254-024-00712-5}
}

@article{Kovachki2023,
  author  = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  title   = {Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {89},
  pages   = {1--97},
  url     = {http://jmlr.org/papers/v24/21-1524.html}
}

@misc{li2020neuraloperatorgraphkernel,
      title={Neural Operator: Graph Kernel Network for Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2020},
      eprint={2003.03485},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.03485}, 
}

@Article{Begoli2019,
author={Begoli, Edmon
and Bhattacharya, Tanmoy
and Kusnezov, Dimitri},
title={The need for uncertainty quantification in machine-assisted medical decision making},
journal={Nature Machine Intelligence},
year={2019},
month={1},
day={01},
volume={1},
number={1},
pages={20-23},
abstract={Medicine, even from the earliest days of artificial intelligence (AI) research, has been one of the most inspiring and promising domains for the application of AI-based approaches. Equally, it has been one of the more challenging areas to see an effective adoption. There are many reasons for this, primarily the reluctance to delegate decision making to machine intelligence in cases where patient safety is at stake. To address some of these challenges, medical AI, especially in its modern data-rich deep learning guise, needs to develop a principled and formal uncertainty quantification (UQ) discipline, just as we have seen in fields such as nuclear stockpile stewardship and risk management. The data-rich world of AI-based learning and the frequent absence of a well-understood underlying theory poses its own unique challenges to straightforward adoption of UQ. These challenges, while not trivial, also present significant new research opportunities for the development of new theoretical approaches, and for the practical applications of UQ in the area of machine-assisted medical decision making. Understanding prediction system structure and defensibly quantifying uncertainty is possible, and, if done, can significantly benefit both research and practical applications of AI in this critical domain.},
issn={2522-5839},
doi={10.1038/s42256-018-0004-1},
url={https://doi.org/10.1038/s42256-018-0004-1}
}

@book{dataset_shift_book,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press},
abstract = {Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series}
}


@inproceedings{attention2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{
mccabe2023multiple,
title={Multiple Physics Pretraining for Physical Surrogate Models},
author={Michael McCabe and Bruno R{\'e}galdo-Saint Blancard and Liam Parker and Ruben Ohana and Miles Cranmer and Alberto Bietti and Michael Eickenberg and Siavash Golkar and Geraud Krawezik and Francois Lanusse and Mariel Pettee and Tiberiu Tesileanu and Kyunghyun Cho and Shirley Ho},
booktitle={NeurIPS 2023 AI for Science Workshop},
year={2023},
url={https://openreview.net/forum?id=M12lmQKuxa}
}


@misc{alkin2024upt,
      title={Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators}, 
      author={Benedikt Alkin and Andreas Fürst and Simon Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},
      year={2024},
      eprint={2402.12365},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hao2024dpot,
      title={DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training}, 
      author={Zhongkai Hao and Chang Su and Songming Liu and Julius Berner and Chengyang Ying and Hang Su and Anima Anandkumar and Jian Song and Jun Zhu},
      year={2024},
      eprint={2403.03542},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bommasani2022opportunities,
      title={On the Opportunities and Risks of Foundation Models}, 
      author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
      year={2022},
      eprint={2108.07258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{yin2022avit,
    title={{A}-{V}i{T}: {A}daptive Tokens for Efficient Vision Transformer},
    author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2022}
}

@INPROCEEDINGS{safetycriticalsystems,
  author={Knight, J.C.},
  booktitle={Proceedings of the 24th International Conference on Software Engineering. ICSE 2002}, 
  title={Safety critical systems: challenges and directions}, 
  year={2002},
  volume={},
  number={},
  pages={547-550},
  keywords={Modems;Application software;Surgery;Aircraft;Aerospace control;Weapons;Software safety;Information security;Power system security;Pacemakers},
  doi={}}

@misc{sun2022conformal,
      title={Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A Survey}, 
      author={Sophia Sun},
      year={2022},
      eprint={2209.03580},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{conformaltimeserires,
 author = {Stankeviciute, Kamile and M. Alaa, Ahmed and van der Schaar, Mihaela},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {6216--6228},
 publisher = {Curran Associates, Inc.},
 title = {Conformal Time-series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/312f1ba2a72318edaaa995a67835fad5-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{Poels_2023,
doi = {10.1088/1741-4326/acf70d},
url = {https://dx.doi.org/10.1088/1741-4326/acf70d},
year = {2023},
month = {sep},
publisher = {IOP Publishing},
volume = {63},
number = {12},
pages = {126012},
author = {Yoeri Poels and Gijs Derks and Egbert Westerhof and Koen Minartz and Sven Wiesen and Vlado Menkovski},
title = {Fast dynamic 1D simulation of divertor plasmas with neural PDE surrogates},
journal = {Nuclear Fusion},
abstract = {Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural partial differential equation (PDE) surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D’s domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. State-of-the-art neural PDE surrogates are evaluated in a common framework and extended for properties of the DIV1D data. We evaluate (1) the speed-accuracy trade-off; (2) recreating non-linear behavior; (3) data efficiency; and (4) parameter inter- and extrapolation. Once trained, neural PDE surrogates can faithfully approximate DIV1D’s divertor plasma dynamics at sub real-time computation speeds: In the proposed configuration,  ms of plasma dynamics can be computed in  ms of wall-clock time, several orders of magnitude faster than DIV1D.}
}



@InProceedings{cp_dynamic_timeseries,
  title = 	 {Conformal prediction interval for dynamic time-series},
  author =       {Xu, Chen and Xie, Yao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11559--11569},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/xu21h/xu21h.pdf},
  url = 	 {https://proceedings.mlr.press/v139/xu21h.html},
  abstract = 	 {We develop a method to construct distribution-free prediction intervals for dynamic time-series, called \Verb|EnbPI| that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. \Verb|EnbPI| is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, \textit{approximately valid} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, \Verb|EnbPI| avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, \Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.}
}


@ARTICLE{CP_Wildfire,
  author={Xu, Chen and Xie, Yao and Vazquez, Daniel A. Zuniga and Yao, Rui and Qiu, Feng},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Spatio-Temporal Wildfire Prediction Using Multi-Modal Data}, 
  year={2023},
  volume={4},
  number={},
  pages={302-313},
  keywords={Predictive models;Sensors;Real-time systems;Spatiotemporal phenomena;Multisensor systems;Fire safety;Wildfires;Spatial-temporal point process;conformal prediction;multi-sensor network;fire safety},
  doi={10.1109/JSAIT.2023.3276054}}

@article{ma2024calibrated,
  title={Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction},
  author={Ma, Ziqi and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2402.01960},
  year={2024}
}

@article{ensembleforecasting_jcp,
title = {Ensemble forecasting},
journal = {Journal of Computational Physics},
volume = {227},
number = {7},
pages = {3515-3539},
year = {2008},
note = {Predicting weather, climate and extreme events},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2007.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0021999107000812},
author = {M. Leutbecher and T.N. Palmer},
keywords = {Uncertainty, Numerical weather prediction, Predictability},
abstract = {Numerical weather prediction models as well as the atmosphere itself can be viewed as nonlinear dynamical systems in which the evolution depends sensitively on the initial conditions. The fact that estimates of the current state are inaccurate and that numerical models have inadequacies, leads to forecast errors that grow with increasing forecast lead time. The growth of errors depends on the flow itself. Ensemble forecasting aims at quantifying this flow-dependent forecast uncertainty. The sources of uncertainty in weather forecasting are discussed. Then, an overview is given on evaluating probabilistic forecasts and their usefulness compared with single forecasts. Thereafter, the representation of uncertainties in ensemble forecasts is reviewed with an emphasis on the initial condition perturbations. The review is complemented by a detailed description of the methodology to generate initial condition perturbations of the Ensemble Prediction System (EPS) of the European Centre for Medium-Range Weather Forecasts (ECMWF). These perturbations are based on the leading part of the singular value decomposition of the operator describing the linearised dynamics over a finite time interval. The perturbations are flow-dependent as the linearisation is performed with respect to a solution of the nonlinear forecast model. The extent to which the current ECMWF ensemble prediction system is capable of predicting flow-dependent variations in uncertainty is assessed for the large-scale flow in mid-latitudes.}
}

@misc{Palmer2009,
  author = {T.N. Palmer and Roberto Buizza and F. Doblas-Reyes and T. Jung and Martin Leutbecher and G.J. Shutts and M. Steinheimer and Antje Weisheimer},
  title = {Stochastic parametrization and model uncertainty},
  abstract = {Stochastic parametrization provides a methodology for representing model uncertainty in ensemble forecasts, and also has the capability of reducing systematic error through the concept of nonlinear noise-induced rectification. The stochastically perturbed parametrization tendencies scheme and the stochastic backscatter scheme are described and their impact on medium-range forecast skill is discussed. The impact of these schemes on ensemble data assimilation and in seasonal forecasting is also considered. In all cases, the results are positive. Validation of the form of these stochastic parametrizations can be found by coarse-grain budgets of high resolution (e.g. cloud-resolving) models; some results are shown. Stochastic parametrization has been pioneered at ECMWF over the last decade, and now most operational centres use stochastic parametrization in their operational ensemble prediction systems - these are briefly discussed. The seamless prediction paradigm implies that serious consideration should now be given to the use of stochastic parametrization in next generation Earth System Models.},
  year = {2009},
  journal = {ECMWF Technical Memoranda},
  number = {598},
  pages = {42},
  month = {10/2009},
  publisher = {ECMWF},
  url = {https://www.ecmwf.int/node/11577},
  doi = {10.21957/ps8gbwbdv},
  language = {eng},
}

@article{Buizza2008,
author = {Buizza, Roberto and Leutbecher, Martin and Isaksen, Lars},
title = {Potential use of an ensemble of analyses in the ECMWF Ensemble Prediction System},
journal = {Quarterly Journal of the Royal Meteorological Society},
volume = {134},
number = {637},
pages = {2051-2066},
keywords = {ensemble prediction, ensemble data assimilation, predictability},
doi = {https://doi.org/10.1002/qj.346},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.346},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.346},
abstract = {Abstract One of the crucial aspects of the design of an ensemble prediction system is the definition of the ensemble of initial states. This work investigates the use of singular vectors, an ensemble of analyses, and a combination of the two types of perturbations in the ECMWF operational ensemble prediction system. First, the similarity between perturbations generated using initial-time singular vectors (SVs) and analyses from the ensemble data assimilation (EDA) system is assessed. Results show that the EDA perturbations are less localized geographically and have a better coverage of the Tropics. EDA perturbations have also smaller scales than SV-based perturbations, and have a less evident upshear vertical tilt, which explains why they grow less with forecast time. Then, the use of EDA-based perturbations in the ECMWF ensemble prediction system is studied. Results indicate that if used alone, EDA-based perturbations lead to an under-dispersive and less skilful ensemble then the one based on initial-time SVs only. Combining the EDA and the initial-time SVs gives a system with a better agreement between ensemble spread and the error of the ensemble mean, a smaller ensemble-mean error and more skilful probabilistic forecasts than the current operational system based on initial-time and evolved SVs. Copyright © 2008 Royal Meteorological Society},
year = {2008}
}

@misc{bulte2024uncertainty,
      title={Uncertainty quantification for data-driven weather models}, 
      author={Christopher Bülte and Nina Horat and Julian Quinting and Sebastian Lerch},
      year={2024},
      eprint={2403.13458},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@article{H_hlein_2024,
   title={Postprocessing of Ensemble Weather Forecasts Using Permutation-Invariant Neural Networks},
   volume={3},
   ISSN={2769-7525},
   url={http://dx.doi.org/10.1175/AIES-D-23-0070.1},
   DOI={10.1175/aies-d-23-0070.1},
   number={1},
   journal={Artificial Intelligence for the Earth Systems},
   publisher={American Meteorological Society},
   author={Höhlein, Kevin and Schulz, Benedikt and Westermann, Rüdiger and Lerch, Sebastian},
   year={2024},
   month=jan }


@Article{Bi2023,
author={Bi, Kaifeng
and Xie, Lingxi
and Zhang, Hengheng
and Chen, Xin
and Gu, Xiaotao
and Tian, Qi},
title={Accurate medium-range global weather forecasting with 3D neural networks},
journal={Nature},
year={2023},
month={Jul},
day={01},
volume={619},
number={7970},
pages={533-538},
abstract={Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39{\thinspace}years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world's best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
issn={1476-4687},
doi={10.1038/s41586-023-06185-3},
url={https://doi.org/10.1038/s41586-023-06185-3}
}

@article{Mariana2019,
author = {Clare, Mariana C.A. and Jamil, Omar and Morcrette, Cyril J.},
title = {Combining distribution-based neural networks to predict weather forecast probabilities},
journal = {Quarterly Journal of the Royal Meteorological Society},
volume = {147},
number = {741},
pages = {4337-4357},
keywords = {data exploration, deep learning, ensemble dropout, probabilistic weather forecasting, probability density functions, ResNet, stacked neural network},
doi = {https://doi.org/10.1002/qj.4180},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.4180},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4180},
abstract = {Abstract The success of deep learning techniques over the last decades has opened up a new avenue of research for weather forecasting. Here, we take the novel approach of using a neural network to predict full probability density functions at each point in space and time rather than a single output value, thus producing a probabilistic weather forecast. This enables the calculation of both uncertainty and skill metrics for the neural network predictions, and overcomes the common difficulty of inferring uncertainty from these predictions. This approach is data-driven and the neural network is trained on the WeatherBench dataset (processed ERA5 data) to forecast geopotential and temperature 3 and 5 days ahead. Data exploration leads to the identification of the most important input variables. In order to increase computational efficiency, several neural networks are trained on small subsets of these variables. The outputs are then combined through a stacked neural network, the first time such a technique has been applied to weather data. Our approach is found to be more accurate than some coarse numerical weather prediction models and as accurate as more complex alternative neural networks, with the added benefit of providing key probabilistic information necessary for making informed weather forecasts.},
year = {2021}
}

@article{price2024gencast,
  title={Gencast: Diffusion-based ensemble forecasting for medium-range weather},
  author={Price, Ilan and Sanchez-Gonzalez, Alvaro and Alet, Ferran and Ewalds, Timo and El-Kadi, Andrew and Stott, Jacklynn and Mohamed, Shakir and Battaglia, Peter and Lam, Remi and Willson, Matthew},
  journal={arXiv preprint arXiv:2312.15796},
  year={2023}
}

@misc{keisler2022forecasting,
      title={Forecasting Global Weather with Graph Neural Networks}, 
      author={Ryan Keisler},
      year={2022},
      eprint={2202.07575},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}


@Article{Chen2023,
author={Chen, Lei
and Zhong, Xiaohui
and Zhang, Feng
and Cheng, Yuan
and Xu, Yinghui
and Qi, Yuan
and Li, Hao},
title={FuXi: a cascade machine learning forecasting system for 15-day global weather forecast},
journal={npj Climate and Atmospheric Science},
year={2023},
month={Nov},
day={16},
volume={6},
number={1},
pages={190},
abstract={Over the past few years, the rapid development of machine learning (ML) models for weather forecasting has led to state-of-the-art ML models that have superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES), which is widely considered as the world's best physics-based weather forecasting system. Specifically, ML models have outperformed HRES in 10-day forecasts with a spatial resolution of 0.25∘. However, the challenge remains in mitigating the accumulation of forecast errors for longer effective forecasts, such as achieving comparable performance to the ECMWF ensemble in 15-day forecasts. Despite various efforts to reduce accumulation errors, such as implementing autoregressive multi-time step loss, relying on a single model has been found to be insufficient for achieving optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts at a temporal resolution of 6 hours and a spatial resolution of 0.25∘. FuXi is developed using 39 years of the ECMWF ERA5 reanalysis dataset. The performance evaluation demonstrates that FuXi has forecast performance comparable to ECMWF ensemble mean (EM) in 15-day forecasts. FuXi surpasses the skillful forecast lead time achieved by ECMWF HRES by extending the lead time for Z500 from 9.25 to 10.5 days and for T2M from 10 to 14.5 days. Moreover, the FuXi ensemble is created by perturbing initial conditions and model parameters, enabling it to provide forecast uncertainty and demonstrating promising results when compared to the ECMWF ensemble.},
issn={2397-3722},
doi={10.1038/s41612-023-00512-1},
url={https://doi.org/10.1038/s41612-023-00512-1}
}

@misc{nguyen2023climax,
      title={ClimaX: A foundation model for weather and climate}, 
      author={Tung Nguyen and Johannes Brandstetter and Ashish Kapoor and Jayesh K. Gupta and Aditya Grover},
      year={2023},
      eprint={2301.10343},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lessig2023atmorep,
      title={AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning}, 
      author={Christian Lessig and Ilaria Luise and Bing Gong and Michael Langguth and Scarlet Stadtler and Martin Schultz},
      year={2023},
      eprint={2308.13280},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}


@InProceedings{spherical_fno,
  title = 	 {Spherical {F}ourier Neural Operators: Learning Stable Dynamics on the Sphere},
  author =       {Bonev, Boris and Kurth, Thorsten and Hundt, Christian and Pathak, Jaideep and Baust, Maximilian and Kashinath, Karthik and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {2806--2823},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/bonev23a/bonev23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/bonev23a.html},
  abstract = 	 {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates by incorrectly assuming flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmo- spheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.}
}

@misc{kochkov2024neural,
      title={Neural General Circulation Models for Weather and Climate}, 
      author={Dmitrii Kochkov and Janni Yuval and Ian Langmore and Peter Norgaard and Jamie Smith and Griffin Mooers and Milan Klöwer and James Lottes and Stephan Rasp and Peter Düben and Sam Hatfield and Peter Battaglia and Alvaro Sanchez-Gonzalez and Matthew Willson and Michael P. Brenner and Stephan Hoyer},
      year={2024},
      eprint={2311.07222},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph}
}

@inproceedings{
verma2024climode,
title={Clim{ODE}: Climate and Weather Forecasting with Physics-informed Neural {ODE}s},
author={Yogesh Verma and Markus Heinonen and Vikas Garg},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xuY33XhEGR}
}

@inproceedings{calibration_of_large_neurwp,
  title={Calibration of Large Neural Weather Models},
  author={Graubner, Andre and Kamyar Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik and Anandkumar, Anima},
  booktitle={NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning},
  year={2022}
}

@article{neural_ensemble_svd,
	title = {Ensemble Methods for Neural Network-Based Weather Forecasts},
	journaltitle = {Journal of Advances in Modeling Earth Systems},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Scher, Sebastian and Messori, Gabriele},
	urldate = {2023-09-12},
	year = {2021}
}

@techreport{bodnar2024aurora,
author = {Bodnar, Cristian and Bruinsma, Wessel and Lucic, Ana and Stanley, Megan and Brandstetter, Johannes and Garvan , Patrick and Riechert, Maik and Weyn, Jonathan and Dong, Haiyu and Vaughan, Anna and Gupta, Jayesh and Tambiratnam, Kit and Archibald, Alex and Heider, Elizabeth and Welling, Max and Turner, Richard and Perdikaris, Paris},
title = {Aurora: A Foundation Model of the Atmosphere},
institution = {Microsoft Research AI for Science},
year = {2024},
month = {May},
url = {https://www.microsoft.com/en-us/research/publication/aurora-a-foundation-model-of-the-atmosphere/},
number = {MSR-TR-2024-16},
}

@article{era5,
	title = {The {ERA}5 global reanalysis},
	journaltitle = {Quarterly Journal of the Royal Meteorological Society},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
	year = {2020},
}

@misc{diquigiovanni2021conformal,
      title={Conformal Prediction Bands for Multivariate Functional Data}, 
      author={Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
      year={2021},
      eprint={2106.01792},
      archivePrefix={arXiv},
      primaryClass={id='stat.ME' full_name='Methodology' is_active=True alt_name=None in_archive='stat' is_general=False description='Design, Surveys, Model Selection, Multiple Testing, Multivariate Methods, Signal and Image Processing, Time Series, Smoothing, Spatial Statistics, Survival Analysis, Nonparametric and Semiparametric Methods'}
}

@article{messoudiCopula2021,
title = {Copula-based conformal prediction for multi-target regression},
journal = {Pattern Recognition},
volume = {120},
pages = {108101},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2021.108101},
url = {https://www.sciencedirect.com/science/article/pii/S0031320321002880},
author = {Soundouss Messoudi and Sébastien Destercke and Sylvain Rousseau},
keywords = {Inductive conformal prediction, Copula functions, Multi-target regression, Deep neural networks, Random forests},
abstract = {There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.}
}


@InProceedings{messoudiEllipsoidal2022,
  title = 	 {Ellipsoidal conformal inference for Multi-Target Regression},
  author =       {Messoudi, Soundouss and Destercke, S\'{e}bastien and Rousseau, Sylvain},
  booktitle = 	 {Proceedings of the Eleventh Symposium on Conformal and Probabilistic Prediction with Applications},
  pages = 	 {294--306},
  year = 	 {2022},
  editor = 	 {Johansson, Ulf and Boström, Henrik and An Nguyen, Khuong and Luo, Zhiyuan and Carlsson, Lars},
  volume = 	 {179},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {24--26 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v179/messoudi22a/messoudi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v179/messoudi22a.html},
  abstract = 	 {Quantifying the uncertainty of a predictive model output is of essential importance in learning scenarios involving critical applications. As the learning task becomes more complex, so does uncertainty quantification. In this paper, we consider the task of multi-target regression and propose a method to output ellipsoidal confidence regions whose shapes are tailored to each instance to predict. We also guarantee that those confidence regions are well-calibrated, i.e., that they cover the ground truth with a specified probability. To achieve such a feat, we propose a conformal prediction method outputting ellipsoidal prediction regions. Experiments on both simulated and real-world data sets show that our methods outperform existing ones.  }
}

@INPROCEEDINGS{normalisedCP2021,
  author={Johansson, Ulf and Boström, Henrik and Löfström, Tuwe},
  booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Investigating Normalized Conformal Regressors}, 
  year={2021},
  volume={},
  number={},
  pages={01-08},
  keywords={Training;Computational modeling;Estimation;Forestry;Predictive models;Turning;Boosting;Conformal prediction;Predictive regression;Random forest;Gradient boosting},
  doi={10.1109/SSCI50451.2021.9659853}}



@article{Kirk2006,
doi = {10.1088/0741-3335/48/12B/S41},
url = {https://dx.doi.org/10.1088/0741-3335/48/12B/S41},
year = {2006},
month = {nov},
publisher = {},
volume = {48},
number = {12B},
pages = {B433},
author = {A Kirk and N Ben Ayed and G Counsell and B Dudson and T Eich and A Herrmann and B Koch and R Martin and A Meakins and S Saarelma and R Scannell and S Tallents and M Walsh and H R Wilson and the MAST team},
title = {Filament structures at the plasma edge on MAST},
journal = {Plasma Physics and Controlled Fusion},
abstract = {The boundary of the tokamak core plasma, or scrape-off layer, is normally characterized in terms of average parameters such as density, temperature and e-folding lengths suggesting diffusive losses. However, as is shown in this paper, localized filamentary structures play an important role in determining the radial efflux in both L mode and during edge localized modes (ELMs) on MAST. Understanding the size, poloidal and toroidal localization and the outward radial extent of these filaments is crucial in order to calculate their effect on power loading both on the first wall and the divertor target plates in future devices. The spatial and temporal evolution of filaments observed on MAST in L-mode and ELMs have been compared and contrasted in order to confront the predictions of various models that have been proposed to predict filament propagation and in particular ELM energy losses.}
}



@inproceedings{Ham2022,
	author = {Chris Ham and Andrew Kirk and Kevin Veruagh},
        title = {Insights on disruption physics in MAST using high speed visible camera data},
	booktitle = {IAEA Second Technical Meeting on Plasma Disruptions and their Mitigation},
	year = {2022},
	url = {https://conferences.iaea.org/event/281/contributions/24423/}
}



@Article{Walkden2022,
    author={Walkden, Nicholas
    and Riva, Fabio
    and Harrison, James
    and Militello, Fulvio
    and Farley, Thomas
    and Omotani, John
    and Lipschultz, Bruce},
    title={The physics of turbulence localised to the tokamak divertor volume},
    journal={Communications Physics},
    year={2022},
    month={Jun},
    day={01},
    volume={5},
    number={1},
    pages={139},
    abstract={Fusion power plant designs based on magnetic confinement, such as the tokamak design, offer a promising route to sustainable fusion power but require robust exhaust solutions capable of tolerating intense heat and particle fluxes from the plasma at the core of the device. Turbulent plasma transport in the region where the interface between the plasma and the materials of the device is handled - called the divertor volume - is poorly understood, yet impacts several key factors ultimately affecting device performance. In this article a comprehensive study of the underlying physics of turbulence in the divertor volume is conducted using data collected in the final experimental campaign of the Mega Ampere Spherical Tokamak device, compared to high fidelity nonlinear simulations. The physics of the turbulence is shown to be strongly dependant on the geometry of the divertor volume - a potentially important result as the community looks to advanced divertor designs with complex geometry for future fusion power plants. These results lay the foundations of a first-principles physics basis for turbulent transport in the tokamak divertor, providing a critical step towards a predictive understanding of tokamak divertor plasma solutions.},
    issn={2399-3650},
    doi={10.1038/s42005-022-00906-2},
    url={https://doi.org/10.1038/s42005-022-00906-2}
}

@article{giudicelli2024moose,
   title = {3.0 - {MOOSE}: Enabling massively parallel multiphysics simulations},
   author = {Guillaume Giudicelli and Alexander Lindsay and Logan Harbour and Casey Icenhour and
             Mengnan Li and Joshua E. Hansel and Peter German and Patrick Behne and Oana Marin and
             Roy H. Stogner and Jason M. Miller and Daniel Schwen and Yaqi Wang and Lynn Munday and
             Sebastian Schunert and Benjamin W. Spencer and Dewen Yushu and Antonio Recuero and
             Zachary M. Prince and Max Nezdyur and Tianchen Hu and Yinbin Miao and
             Yeon Sang Jung and Christopher Matthews and April Novak and Brandon Langley and
             Timothy Truster and Nuno Nobre and Brian Alger and David Andr{\v{s}} and
             Fande Kong and Robert Carlsen and Andrew E. Slaughter and John W. Peterson and
             Derek Gaston and Cody Permann},
    year = {2024},
 journal = {{SoftwareX}},
  volume = {26},
   pages = {101690},
    issn = {2352-7110},
     doi = {https://doi.org/10.1016/j.softx.2024.101690},
     url = {https://www.sciencedirect.com/science/article/pii/S235271102400061X},
keywords = {Framework, Finite-element, Finite-volume, Parallel, Multiphysics, Multiscale},
}

@ARTICLE{Hospital_Adam2015-bw,
  title    = "Molecular dynamics simulations: advances and applications",
  author   = "{Hospital, Adam} and Go{\~n}i, Josep Ramon and Orozco, Modesto
              and Gelp{\'\i}, Josep L",
  abstract = "Molecular dynamics simulations have evolved into a mature
              technique that can be used effectively to understand
              macromolecular structure-to-function relationships. Present
              simulation times are close to biologically relevant ones.
              Information gathered about the dynamic properties of
              macromolecules is rich enough to shift the usual paradigm of
              structural bioinformatics from studying single structures to
              analyze conformational ensembles. Here, we describe the
              foundations of molecular dynamics and the improvements made in
              the direction of getting such ensemble. Specific application of
              the technique to three main issues (allosteric regulation,
              docking, and structure refinement) is discussed.",
  journal  = "Adv Appl Bioinform Chem",
  volume   =  8,
  pages    = "37--47",
  month    =  nov,
  year     =  2015,
  address  = "New Zealand",
  keywords = "allostery; conformational ensembles; docking; molecular dynamics;
              refinement; structure prediction",
  language = "en"
}
@article{cesm2,
author = {Danabasoglu, G. and Lamarque, J.-F. and Bacmeister, J. and Bailey, D. A. and DuVivier, A. K.  and Edwards, J. and Emmons, L. K. and Fasullo, J. and Garcia, R. and Gettelman, A. and Hannay, C. and Holland, M. M. and Large, W. G. and Lauritzen, P. H. and Lawrence, D. M. and Lenaerts, J. T. M. and Lindsay, K. and Lipscomb, W. H. and Mills, M. J. and Neale, R. and Oleson, K. W. and Otto-Bliesner, B. and Phillips, A. S. and Sacks, W. and Tilmes, S. and van Kampenhout, L. and Vertenstein, M. and Bertini, A. and Dennis, J. and Deser, C. and Fischer, C. and Fox-Kemper, B. and Kay, J. E. and Kinnison, D. and Kushner, P. J. and Larson, V. E. and Long, M. C. and Mickelson, S. and Moore, J. K. and Nienhouse, E. and Polvani, L. and Rasch, P. J. and Strand, W. G.},
title = {The Community Earth System Model Version 2 (CESM2)},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {12},
number = {2},
pages = {e2019MS001916},
keywords = {Community Earth System Model (CESM), global coupled Earth system modeling, preindustrial and historical simulations, coupled model development and evaluation},
doi = {https://doi.org/10.1029/2019MS001916},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001916},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001916},
note = {e2019MS001916 2019MS001916},
abstract = {Abstract An overview of the Community Earth System Model Version 2 (CESM2) is provided, including a discussion of the challenges encountered during its development and how they were addressed. In addition, an evaluation of a pair of CESM2 long preindustrial control and historical ensemble simulations is presented. These simulations were performed using the nominal 1° horizontal resolution configuration of the coupled model with both the “low-top” (40 km, with limited chemistry) and “high-top” (130 km, with comprehensive chemistry) versions of the atmospheric component. CESM2 contains many substantial science and infrastructure improvements and new capabilities since its previous major release, CESM1, resulting in improved historical simulations in comparison to CESM1 and available observations. These include major reductions in low-latitude precipitation and shortwave cloud forcing biases; better representation of the Madden-Julian Oscillation; better El Niño-Southern Oscillation-related teleconnections; and a global land carbon accumulation trend that agrees well with observationally based estimates. Most tropospheric and surface features of the low- and high-top simulations are very similar to each other, so these improvements are present in both configurations. CESM2 has an equilibrium climate sensitivity of 5.1–5.3 °C, larger than in CESM1, primarily due to a combination of relatively small changes to cloud microphysics and boundary layer parameters. In contrast, CESM2's transient climate response of 1.9–2.0 °C is comparable to that of CESM1. The model outputs from these and many other simulations are available to the research community, and they represent CESM2's contributions to the Coupled Model Intercomparison Project Phase 6.},
year = {2020}
}

@ARTICLE{cc_extremeweather,
  title    = "Extreme Weather and Climate Change: Population Health and Health
              System Implications",
  author   = "Ebi, Kristie L and Vanos, Jennifer and Baldwin, Jane W and Bell,
              Jesse E and Hondula, David M and Errett, Nicole A and Hayes,
              Katie and Reid, Colleen E and Saha, Shubhayu and Spector, June
              and Berry, Peter",
  abstract = "Extreme weather and climate events, such as heat waves, cyclones,
              and floods, are an expression of climate variability. These
              events and events influenced by climate change, such as
              wildfires, continue to cause significant human morbidity and
              mortality and adversely affect mental health and well-being.
              Although adverse health impacts from extreme events declined over
              the past few decades, climate change and more people moving into
              harm's way could alter this trend. Long-term changes to Earth's
              energy balance are increasing the frequency and intensity of many
              extreme events and the probability of compound events, with
              trends projected to accelerate under certain greenhouse gas
              emissions scenarios. While most of these events cannot be
              completely avoided, many of the health risks could be prevented
              through building climate-resilient health systems with improved
              risk reduction, preparation, response, and recovery. Conducting
              vulnerability and adaptation assessments and developing health
              system adaptation plans can identify priority actions to
              effectively reduce risks, such as disaster risk management and
              more resilient infrastructure. The risks are urgent, so action is
              needed now.",
  journal  = "Annu Rev Public Health",
  volume   =  42,
  pages    = "293--315",
  month    =  jan,
  year     =  2021,
  address  = "United States",
  keywords = "climate change; climate variability; extreme events; health
              systems; population health",
  language = "en"
}

@article{gcm_error_growth,
author = {Sheshadri, Aditi and Borrus, Marshall and Yoder, Mark and Robinson, Thomas},
title = {Midlatitude Error Growth in Atmospheric GCMs: The Role of Eddy Growth Rate},
journal = {Geophysical Research Letters},
volume = {48},
number = {23},
pages = {e2021GL096126},
doi = {https://doi.org/10.1029/2021GL096126},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2021GL096126},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2021GL096126},
note = {e2021GL096126 2021GL096126},
abstract = {Abstract Several studies have established that atmospheric flows have a finite range of predictability, which may be reasonably considered a consequence of the underlying dynamics. In the midlatitudes, error growth is predominantly associated with baroclinic disturbances. We consider midlatitude error growth in two models: an idealized dry dynamical core and a comprehensive atmospheric general circulation model (GCM). By systematically varying equator to pole temperature gradients in the dynamical core, we show that with increasing Eady growth rates, the time elapsed before errors saturate decreases, shortening the window in which weather predictions may be useful. We also consider the limits of midlatitude predictability in the comprehensive moist GCM in a range of climates. Our results show that the times to error saturation are shorter in warmer climates than colder climates, suggesting that warmer climates are inherently less predictable.},
year = {2021}
}


@article {ddw_risk,
      author = "Zied Ben Bouallègue and Mariana C. A. Clare and Linus Magnusson and Estibaliz Gascón and Michael Maier-Gerber and Martin Janoušek and Mark Rodwell and Florian Pinault and Jesper S. Dramsch and Simon T. K. Lang and Baudouin Raoult and Florence Rabier and Matthieu Chevallier and Irina Sandu and Peter Dueben and Matthew Chantry and Florian Pappenberger",
      title = "The Rise of Data-Driven Weather Forecasting: A First Statistical Assessment of Machine Learning–Based Weather Forecasts in an Operational-Like Context",
      journal = "Bulletin of the American Meteorological Society",
      year = "2024",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "105",
      number = "6",
      doi = "10.1175/BAMS-D-23-0162.1",
      pages=      "E864 - E883",
      url = "https://journals.ametsoc.org/view/journals/bams/105/6/BAMS-D-23-0162.1.xml"
}

@inproceedings{
li2023geometryinformed,
title={Geometry-Informed Neural Operator for Large-Scale 3D {PDE}s},
author={Zongyi Li and Nikola Borislavov Kovachki and Chris Choy and Boyi Li and Jean Kossaifi and Shourya Prakash Otta and Mohammad Amin Nabian and Maximilian Stadler and Christian Hundt and Kamyar Azizzadenesheli and Anima Anandkumar},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=86dXbqT5Ua}
}

@article{SHUKLAdnoairfoil,
title = {Deep neural operators as accurate surrogates for shape optimization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {129},
pages = {107615},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107615},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623017992},
author = {Khemraj Shukla and Vivek Oommen and Ahmad Peyvan and Michael Penwarden and Nicholas Plewacki and Luis Bravo and Anindya Ghoshal and Robert M. Kirby and George Em Karniadakis},
keywords = {Neural operators, DeepONet, Airfoil shape optimization, Navier–Stokes equations, Surrogate models},
abstract = {Deep neural operators, such as DeepONet, have changed the paradigm in high-dimensional nonlinear regression, paving the way for significant generalization and speed-up in computational engineering applications. Here, we investigate the use of DeepONet to infer flow fields around unseen airfoils with the aim of shape constrained optimization, an important design problem in aerodynamics that typically taxes computational resources heavily. We present results that display little to no degradation in prediction accuracy while reducing the online optimization cost by orders of magnitude. We consider NACA airfoils as a test case for our proposed approach, as the four-digit parameterization can easily define their shape. We successfully optimize the constrained NACA four-digit problem with respect to maximizing the lift-to-drag ratio and validate all results by comparing them to a high-order CFD solver. We find that DeepONets have a low generalization error, making them ideal for generating solutions of unseen shapes. Specifically, pressure, density, and velocity fields are accurately inferred at a fraction of a second, hence enabling the use of general objective functions beyond the maximization of the lift-to-drag ratio considered in the current work. Finally, we validate the ability of DeepONet to handle a complex 3D waverider geometry at hypersonic flight by inferring shear stress and heat flux distributions on its surface at unseen angles of attack. The main contribution of this paper is a modular integrated design framework that uses an over-parametrized neural operator as a surrogate model with good generalizability coupled seamlessly with multiple optimization solvers in a plug-and-play mode.}
}

@article{Yin_2023,
   title={Solving multiphysics-based inverse problems with learned surrogates and constraints},
   volume={10},
   ISSN={2213-7467},
   url={http://dx.doi.org/10.1186/s40323-023-00252-0},
   DOI={10.1186/s40323-023-00252-0},
   number={1},
   journal={Advanced Modeling and Simulation in Engineering Sciences},
   publisher={Springer Science and Business Media LLC},
   author={Yin, Ziyi and Orozco, Rafael and Louboutin, Mathias and Herrmann, Felix J.},
   year={2023},
   month=oct }


@article{LEREDE2023101144,
title = {Analysis of the possible contribution of different nuclear fusion technologies to the global energy transition},
journal = {Energy Strategy Reviews},
volume = {49},
pages = {101144},
year = {2023},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2023.101144},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X23000949},
author = {D. Lerede and M. Nicoli and L. Savoldi and A. Trotta},
keywords = {Nuclear fusion, Energy system optimization, Energy scenarios, Electricity mix},
abstract = {Despite the huge uncertainties related to the possibility of a quick development of nuclear fusion technologies - being disputed that it may come too late to effectively contribute to emission mitigation - research is focusing on a wide set of options for fusion reactors. This paper presents a global scenario analysis using the energy system optimization model EUROfusion TIMES to analyze the possible future role of fusion according to three different technologies and using capacity curves based on historical trends for the electricity sector. The analyzed fusion options are based on ARC, EU-DEMO and Asian-DEMO reactor concepts, characterized in terms of techno-economic features according to publicly available literature and considering a set of educated growth rate for their penetration. Results concerning installed capacity trends and contribution to the electricity mix are presented up to 2100 in three socio-economic storylines and for different scenarios considering either the availability of competing technologies or delays in the development of fusion plants. Despite not contributing at all to the energy transition in Europe and the US, fusion may gain share in contexts characterized by highly growing electricity demand, contributing to satisfy stringent environmental constraints together with other low-carbon technologies in the second half of the century.}
}

@Article{Kates-Harbeck2019,
author={Kates-Harbeck, Julian
and Svyatkovskiy, Alexey
and Tang, William},
title={Predicting disruptive instabilities in controlled fusion plasmas through deep learning},
journal={Nature},
year={2019},
month={Apr},
day={01},
volume={568},
number={7753},
pages={526-531},
abstract={Nuclear fusion power delivered by magnetic-confinement tokamak reactors holds the promise of sustainable and clean energy1. The avoidance of large-scale plasma instabilities called disruptions within these reactors2,3 is one of the most pressing challenges4,5, because disruptions can halt power production and damage key components. Disruptions are particularly harmful for large burning-plasma systems such as the multibillion-dollar International Thermonuclear Experimental Reactor (ITER) project6 currently under construction, which aims to be the first reactor that produces more power from fusion than is injected to heat the plasma. Here we present a method based on deep learning for forecasting disruptions. Our method extends considerably the capabilities of previous strategies such as first-principles-based5 and classical machine-learning7--11 approaches. In particular, it delivers reliable predictions for machines other than the one on which it was trained---a crucial requirement for future large reactors that cannot afford training disruptions. Our approach takes advantage of high-dimensional training data to boost predictive performance while also engaging supercomputing resources at the largest scale to improve accuracy and speed. Trained on experimental data from the largest tokamaks in the United States (DIII-D12) and the world (Joint European Torus, JET13), our method can also be applied to specific tasks such as prediction with long warning times: this opens up the possibility of moving from passive disruption prediction to active reactor control and optimization. These initial results illustrate the potential for deep learning to accelerate progress in fusion-energy science and, more generally, in the understanding and prediction of complex physical systems.},
issn={1476-4687},
doi={10.1038/s41586-019-1116-4},
url={https://doi.org/10.1038/s41586-019-1116-4}
}

@article{PFC,
    author = {Linke, Jochen and Du, Juan and Loewenhoff, Thorsten and Pintsuk, Gerald and Spilker, Benjamin and Steudel, Isabel and Wirtz, Marius},
    title = "{Challenges for plasma-facing components in nuclear fusion}",
    journal = {Matter and Radiation at Extremes},
    volume = {4},
    number = {5},
    pages = {056201},
    year = {2019},
    month = {08},
    abstract = "{ The interaction processes between the burning plasma and the first wall in a fusion reactor are diverse: the first wall will be exposed to extreme thermal loads of up to several tens of megawatts per square meter during quasistationary operation, combined with repeated intense thermal shocks (with energy densities of up to several megajoules per square meter and pulse durations on a millisecond time scale). In addition to these thermal loads, the wall will be subjected to bombardment by plasma ions and neutral particles (D, T, and He) and by energetic neutrons with energies up to 14 MeV. Hopefully, ITER will not only demonstrate that thermonuclear fusion of deuterium and tritium is feasible in magnetic confinement regimes; it will also act as a first test device for plasma-facing materials (PFMs) and plasma-facing components (PFCs) under realistic synergistic loading scenarios that cover all the above-mentioned load types. In the absence of an integrated test device, material tests are being performed primarily in specialized facilities that concentrate only on the most essential material properties. New multipurpose test facilities are now available that can also focus on more complex loading scenarios and thus help to minimize the risk of an unexpected material or component failure. Thermonuclear fusion—both with magnetic and with inertial confinement—is making great progress, and the goal of scientific break-even will be reached soon. However, to achieve that end, significant technical problems, particularly in the field of high-temperature and radiation-resistant materials, must be solved. With ITER, the first nuclear reactor that burns a deuterium–tritium plasma with a fusion power gain Q ≥ 10 will start operation in the next decade. To guarantee safe operation of this rather sophisticated fusion device, new PFMs and PFCs that are qualified to withstand the harsh environments in such a tokamak reactor have been developed and are now entering the manufacturing stage. }",
    issn = {2468-2047},
    doi = {10.1063/1.5090100},
    url = {https://doi.org/10.1063/1.5090100},
    eprint = {https://pubs.aip.org/aip/mre/article-pdf/doi/10.1063/1.5090100/13886520/056201\_1\_online.pdf},
}



@Article{Degrave2022,
author={Degrave, Jonas
and Felici, Federico
and Buchli, Jonas
and Neunert, Michael
and Tracey, Brendan
and Carpanese, Francesco
and Ewalds, Timo
and Hafner, Roland
and Abdolmaleki, Abbas
and de las Casas, Diego
and Donner, Craig
and Fritz, Leslie
and Galperti, Cristian
and Huber, Andrea
and Keeling, James
and Tsimpoukelli, Maria
and Kay, Jackie
and Merle, Antoine
and Moret, Jean-Marc
and Noury, Seb
and Pesamosca, Federico
and Pfau, David
and Sauter, Olivier
and Sommariva, Cristian
and Coda, Stefano
and Duval, Basil
and Fasoli, Ambrogio
and Kohli, Pushmeet
and Kavukcuoglu, Koray
and Hassabis, Demis
and Riedmiller, Martin},
title={Magnetic control of tokamak plasmas through deep reinforcement learning},
journal={Nature},
year={2022},
month={Feb},
day={01},
volume={602},
number={7897},
pages={414-419},
abstract={Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
issn={1476-4687},
doi={10.1038/s41586-021-04301-9},
url={https://doi.org/10.1038/s41586-021-04301-9}
}
@misc{carey2025dataefficiencylongtermprediction,
      title={Data efficiency and long-term prediction capabilities for neural operator surrogate models of edge plasma simulations}, 
      author={N. Carey and L. Zanisi and S. Pamela and V. Gopakumar and J. Omotani and J. Buchanan and J. Brandstetter and F. Paischer and G. Galletti and P. Setinek},
      year={2025},
      eprint={2502.17386},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2502.17386}, 
}
@misc{pamela2024neuralpararealdynamicallytrainingneural,
      title={Neural-Parareal: Dynamically Training Neural Operators as Coarse Solvers for Time-Parallelisation of Fusion MHD Simulations}, 
      author={S. J. P. Pamela and N. Carey and J. Brandstetter and R. Akers and L. Zanisi and J. Buchanan and V. Gopakumar and M. Hoelzl and G. Huijsmans and K. Pentland and T. James and G. Antonucci and the JOREK Team},
      year={2024},
      eprint={2405.01355},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2405.01355}, 
}



@misc{dehoop2022costaccuracytradeoffoperatorlearning,
      title={The Cost-Accuracy Trade-Off In Operator Learning With Neural Networks}, 
      author={Maarten V. de Hoop and Daniel Zhengyu Huang and Elizabeth Qian and Andrew M. Stuart},
      year={2022},
      eprint={2203.13181},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2203.13181}, 
}


@InProceedings{Kato_ncf_review_2024,
  title = 	 {A Review of Nonconformity Measures for Conformal
 Prediction in Regression},
  author =       {Kato, Yuko and Tax, David M.J. and Loog, Marco},
  booktitle = 	 {Proceedings of the Twelfth Symposium on Conformal
 and Probabilistic Prediction with Applications},
  pages = 	 {369--383},
  year = 	 {2023},
  editor = 	 {Papadopoulos, Harris and Nguyen, Khuong An and Boström, Henrik and Carlsson, Lars},
  volume = 	 {204},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Sep},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v204/kato23a/kato23a.pdf},
  url = 	 {https://proceedings.mlr.press/v204/kato23a.html},
  abstract = 	 {Conformal prediction provides distribution-free
 uncertainty quantification under minimal
 assumptions. An important ingredient in conformal
 prediction is the so-called nonconformity measure,
 which quantifies how the test sample differs from
 the rest of the data. In this paper, existing
 nonconformity measures from the current literature
 are collected and their underlying ideas are
 analyzed. Furthermore, the influence of different
 factors on the performance of conformal prediction
 are pointed out by focusing on the relation between
 the influencing factors and the choice of
 nonconformity measures. Lastly, we provide
 suggestions for future work with regard to currently
 existing knowledge gaps and development of new
 nonconformity measures.}
}

@misc{raonic2023convolutionalneuraloperatorsrobust,
      title={Convolutional Neural Operators for robust and accurate learning of PDEs}, 
      author={Bogdan Raonić and Roberto Molinaro and Tim De Ryck and Tobias Rohner and Francesca Bartolucci and Rima Alaifari and Siddhartha Mishra and Emmanuel de Bézenac},
      year={2023},
      eprint={2302.01178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.01178}, 
}

@misc{cao2023lnolaplaceneuraloperator,
      title={LNO: Laplace Neural Operator for Solving Differential Equations}, 
      author={Qianying Cao and Somdatta Goswami and George Em Karniadakis},
      year={2023},
      eprint={2303.10528},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.10528}, 
}

@article{Tripura2023WaveletNO,
title = {Wavelet Neural Operator for solving parametric partial differential equations in computational mechanics problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {404},
pages = {115783},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115783},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522007393},
author = {Tapas Tripura and Souvik Chakraborty},
keywords = {Nonlinear mappings, Operator learning, Wavelet, Wavelet neural operator, Scientific machine learning},
abstract = {With massive advancements in sensor technologies and Internet-of-things (IoT), we now have access to terabytes of historical data; however, there is a lack of clarity on how to best exploit the data to predict future events. One possible alternative in this context is to utilize an operator learning algorithm that directly learns the nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator (WNO) that blends integral kernel with wavelet transformation. WNO harnesses the superiority of the wavelets in time–frequency localization of the functions and enables accurate tracking of patterns in the spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, WNO can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed WNO are illustrated on a wide array of problems involving Burger’s equation, Darcy flow, Navier–Stokes equation, Allen–Cahn equation, and Wave advection equation. A comparative study with respect to existing operator learning frameworks is presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth’s air temperature based on available historical data.}
}

@Article{Lu2021DeepOnet,
author={Lu, Lu
and Jin, Pengzhan
and Pang, Guofei
and Zhang, Zhongqiang
and Karniadakis, George Em},
title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
journal={Nature Machine Intelligence},
year={2021},
month={Mar},
day={01},
volume={3},
number={3},
pages={218-229},
abstract={It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
issn={2522-5839},
doi={10.1038/s42256-021-00302-5},
url={https://doi.org/10.1038/s42256-021-00302-5}
}




@article{Youcef_GMRES_1986,
author = {Saad, Youcef and Schultz, Martin H.},
title = {GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {7},
number = {3},
pages = {856-869},
year = {1986},
doi = {10.1137/0907058},

URL = { 
    
        https://doi.org/10.1137/0907058
    
    

},
eprint = { 
    
        https://doi.org/10.1137/0907058
    
    

}
,
    abstract = { We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saunders’ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR. }
}




@book{iserles2009first,
  title={A first course in the numerical analysis of differential equations},
  author={Iserles, Arieh},
  year={2009},
  publisher={Cambridge university press}
}

@book{Pinder2018,
author={Pinder, George F.},
title={Numerical methods for solving partial differential equations : a comprehensive introduction for scientists and engineers},
year={2018},
publisher={John Wiley and Sons, Inc. : Wiley},
address={Hoboken, NJ},
}

@article{TOLSMA1998475,
title = {On computational differentiation},
journal = {Computers and Chemical Engineering},
volume = {22},
number = {4},
pages = {475-490},
year = {1998},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(97)00264-0},
url = {https://www.sciencedirect.com/science/article/pii/S0098135497002640},
author = {John E. Tolsma and Paul I. Barton},
abstract = {Numerical derivatives play an important role in many computations. In many applications, the cost associated with the evaluation of numerical derivatives may be significant. Dramatic improvements in the speed of such calculations can be obtained through careful consideration of how these derivatives are computed. This paper reviews several ways in which numerical derivatives can be evaluated: hand-coding, finite difference approximations, reverse polish notation evaluation, symbolic differentiation, and automatic differentiation. It is concluded that automatic differentiation has significant advantages over all other approaches. Several ways of improving the efficiency of obtaining derivatives in an interpretive, symbolic environment are discussed. Example problems are compared to illustrate these improvements.}
}

@misc{gopakumar2024uncertaintyquantificationsurrogatemodels,
      title={Uncertainty Quantification of Surrogate Models using Conformal Prediction}, 
      author={Vignesh Gopakumar and Ander Gray and Joel Oskarsson and Lorenzo Zanisi and Stanislas Pamela and Daniel Giles and Matt Kusner and Marc Peter Deisenroth},
      year={2024},
      eprint={2408.09881},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.09881}, 
}

@article{DiquigiovanniCP_MV2022,
title = {Conformal prediction bands for multivariate functional data},
journal = {Journal of Multivariate Analysis},
volume = {189},
pages = {104879},
year = {2022},
issn = {0047-259X},
doi = {https://doi.org/10.1016/j.jmva.2021.104879},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001573},
author = {Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
keywords = {Conformal Prediction, Distribution-free prediction set, Exact prediction set, Finite-sample prediction set, Functional data, Prediction band},
abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.}
}

@article{Casella_acceptrejectsampling_2004,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4356322},
 abstract = {This paper extends the Accept--Reject algorithm to allow the proposal distribution to change at each iteration. We first establish a necessary and sufficient condition for this generalized Accept--Reject algorithm to be valid, and then show how the resulting estimator can be improved by Rao-Blackwellization. An application of these results is to the perfect sampling technique of Fill (1998), which is a generalized Accept--Reject algorithm.},
 author = {George Casella and Christian P. Robert and Martin T. Wells},
 journal = {Lecture Notes-Monograph Series},
 pages = {342--347},
 publisher = {Institute of Mathematical Statistics},
 title = {Generalized Accept-Reject Sampling Schemes},
 urldate = {2024-09-11},
 volume = {45},
 year = {2004}
}

@misc{diquigiovanni2021importancebandfinitesampleexact,
      title={The Importance of Being a Band: Finite-Sample Exact Distribution-Free Prediction Sets for Functional Data}, 
      author={Jacopo Diquigiovanni and Matteo Fontana and Simone Vantini},
      year={2021},
      eprint={2102.06746},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2102.06746}, 
}

@INPROCEEDINGS{Actor2020-ng,
  title           = "Identification of kernels in a convolutional neural
                     network: connections between the level set equation and
                     deep learning for image segmentation",
  booktitle       = "Medical Imaging 2020: Image Processing",
  author          = "Actor, Jonas and Fuentes, David T and Riviere, Beatrice",
  editor          = "Landman, Bennett A and I{\v s}gum, Ivana",
  publisher       = "SPIE",
  month           =  mar,
  year            =  2020,
  conference      = "Image Processing",
  location        = "Houston, United States"
}

@misc{chen2024usingailibrariesincompressible,
      title={Using AI libraries for Incompressible Computational Fluid Dynamics}, 
      author={Boyang Chen and Claire E. Heaney and Christopher C. Pain},
      year={2024},
      eprint={2402.17913},
      archivePrefix={arXiv},
      primaryClass={physics.flu-dyn},
      url={https://arxiv.org/abs/2402.17913}, 
}

@article{CHEN2024116974,
title = {Solving the discretised multiphase flow equations with interface capturing on structured grids using machine learning libraries},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {426},
pages = {116974},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116974},
url = {https://www.sciencedirect.com/science/article/pii/S0045782524002305},
author = {Boyang Chen and Claire E. Heaney and Jefferson L.M.A. Gomes and Omar K. Matar and Christopher C. Pain},
keywords = {Artificial Intelligence, Partial differential equations, Convolutional neural networks, U-Net, Graphics Processing Units, Finite Element Method},
abstract = {This paper solves the discretised multiphase flow equations using tools and methods from machine-learning libraries. The idea comes from the observation that convolutional layers can be used to express a discretisation as a neural network whose weights are determined by the numerical method, rather than by training, and hence, we refer to this approach as Neural Networks for PDEs (NN4PDEs). To solve the discretised multiphase flow equations, a multigrid solver is implemented through a convolutional neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier–Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov–Galerkin for accuracy and designed with NN4PDEs in mind. High-order finite-element based schemes are chosen to model a collapsing water column and a rising bubble. Results compare well with experimental data and other numerical results from the literature, demonstrating that, for the first time, finite element discretisations of multiphase flows can be solved using an approach based on (untrained) convolutional neural networks. A benefit of expressing numerical discretisations as neural networks is that the code can run, without modification, on CPUs, GPUs or the latest accelerators designed especially to run AI codes.}
}

@article{Gilbert_Topelitz_1986,
author = {Strang, Gilbert},
title = {A Proposal for Toeplitz Matrix Calculations},
journal = {Studies in Applied Mathematics},
volume = {74},
number = {2},
pages = {171-176},
doi = {https://doi.org/10.1002/sapm1986742171},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sapm1986742171},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sapm1986742171},
abstract = {In contrast to the usual (and successful) direct methods for Toeplitz systems Ax = b, we propose an algorithm based on the conjugate gradient method. The preconditioner is a circulant, so that all matrices have constant diagonals and all matrix-vector multiplications use the Fast Fourier Transform. We also suggest a technique for the eigenvalue problem, where current methods are less satisfactory. If the first indications are supported by further experiment, this new approach may have useful applications—including nearly Toeplitz systems, and parallel computations.},
year = {1986}
}

@Article{Fiorentino1991,
author={Fiorentino, G.
and Serra, S.},
title={Multigrid methods for toeplitz matrices},
journal={CALCOLO},
year={1991},
month={Sep},
day={01},
volume={28},
number={3},
pages={283-305},
abstract={We introduce a class of Multigrid methods for solving banded, symmetric Toeplitz systems Ax=b. We use a, special choice of the projection operator whose coefficients simply depend on some spectral properties of A. This choice leads to an iterative Multigrid method with convergence rate smaller than 1 independent of the condition number K2(A) and of the dimension of the matrix. In the second part the B0 class is introduced: this class, of Toeplitz matrices contains the linear space generated by the matrices arising from the finite differences discretization of the differential operators, m∈N+. To sum up we present an adaptive algorithm which has a input the coefficients of A and return an iterative Multigrid method with convergence speed independent of the mesh spacing h and with an asymptotical cost of O(n).},
issn={1126-5434},
doi={10.1007/BF02575816},
url={https://doi.org/10.1007/BF02575816}
}

@misc{paszke2019pytorchimperativestylehighperformance,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{Crank_Nicolson_1947, title={A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type}, volume={43}, DOI={10.1017/S0305004100023197}, number={1}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Crank, J. and Nicolson, P.}, year={1947}, pages={50–67}}

@book{canuto2007spectral,
  title={Spectral Methods: Evolution to Complex Geometries and Applications to Fluid Dynamics},
  author={Canuto, C. and Hussaini, M.Y. and Quarteroni, A. and Zang, T.A.},
  isbn={9783540307280},
  lccn={2007924823},
  series={Scientific Computation},
  url={https://books.google.co.uk/books?id=7COgEw5_EBQC},
  year={2007},
  publisher={Springer Berlin Heidelberg}
}

@Inbook{Gruber1985,
author="Gruber, Ralf
and Rappaz, Jacques",
title="The Ideal MHD Model",
bookTitle="Finite Element Methods in Linear Ideal Magnetohydrodynamics",
year="1985",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="34--41",
abstract="The plasma state is often called the fourth state following the solid, liquid and gaseous states. If a gas is heated above, say, 10 000 K, the gas is ionized due to collisions between particles. Such a mixture of ions and electrons is called a plasma (Chen 1974). If the temperature is raised to temperatures necessary in a thermonuclear reactor (T ≧ 10 keV ≈ 108 K) almost all electrons are free and the plasma becomes a very good conductor. In order to reach such high temperatures, the ionized hot gas has to be kept away from material walls, and the plasma has to be confined. This is done by applying a strong magnetic field, which acts on the charged particles in such a way that it provides a counterpressure to the gas pressure. A plasma in a magnetic bottle behaves like a mixture of fluids which can be described by the fluid equations coupled to the Maxwell equations. If one neglects the relative motion between ions and electrons and considers the plasma as only one averaged fluid, one uses the magnetohydrodynamic (MHD) equations. In the special case of an infinitely good conducting gas (with resistivity = 0), one uses what are called the ideal MHD equations. It is exactly this most simple model that we will treat numerically in the following chapters. It describes surprisingly well the equilibrium state of a magnetically confined plasma, and the rapid unstable global motions which can destroy the confinement on a microsecond timescale.",
isbn="978-3-642-86708-8",
doi="10.1007/978-3-642-86708-8_3",
url="https://doi.org/10.1007/978-3-642-86708-8_3"
}

@article{Orszag_Tang_1979, title={Small-scale structure of two-dimensional magnetohydrodynamic turbulence}, volume={90}, DOI={10.1017/S002211207900210X}, number={1}, journal={Journal of Fluid Mechanics}, author={Orszag, Steven A. and Tang, Cha-Mei}, year={1979}, pages={129–143}} 

@Article{ALFVÉN1942,
author={Alfv{\'e}n, H.},
title={Existence of Electromagnetic-Hydrodynamic Waves},
journal={Nature},
year={1942},
month={Oct},
day={01},
volume={150},
number={3805},
pages={405-406},
abstract={IF a conducting liquid is placed in a constant magnetic field, every motion of the liquid gives rise to an E. M. F. which produces electric currents. Owing to the magnetic field, these currents give mechanical forces which change the state of motion of the liquid. Thus a kind of combined electromagnetic-hydro-dynamic wave is produced which, so far as I know, has as yet attracted no attention.},
issn={1476-4687},
doi={10.1038/150405d0},
url={https://doi.org/10.1038/150405d0}
}

@article{Mocz_MHD_2014,
    author = {Mocz, Philip and Vogelsberger, Mark and Hernquist, Lars},
    title = "{A constrained transport scheme for MHD on unstructured static and moving meshes}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {442},
    number = {1},
    pages = {43-55},
    year = {2014},
    month = {06},
    abstract = "{Magnetic fields play an important role in many astrophysical systems and a detailed understanding of their impact on the gas dynamics requires robust numerical simulations. Here we present a new method to evolve the ideal magnetohydrodynamic (MHD) equations on unstructured static and moving meshes that preserves the magnetic field divergence-free constraint to machine precision. The method overcomes the major problems of using a cleaning scheme on the magnetic fields instead, which is non-conservative, not fully Galilean invariant, does not eliminate divergence errors completely, and may produce incorrect jumps across shocks. Our new method is a generalization of the constrained transport (CT) algorithm used to enforce the ∇ · B = 0 condition on fixed Cartesian grids. Preserving ∇ · B = 0 at the discretized level is necessary to maintain the orthogonality between the Lorentz force and B. The possibility of performing CT on a moving mesh provides several advantages over static mesh methods due to the quasi-Lagrangian nature of the former (i.e. the mesh generating points move with the flow), such as making the simulation automatically adaptive and significantly reducing advection errors. Our method preserves magnetic fields and fluid quantities in pure advection exactly.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stu865},
    url = {https://doi.org/10.1093/mnras/stu865},
    eprint = {https://academic.oup.com/mnras/article-pdf/442/1/43/4072384/stu865.pdf},
}

@incollection{eymard2000finite,
  author = {Eymard, Robert and Gallouët, Thierry and Herbin, Raphaèle},
  title = {Finite Volume Methods},
  booktitle = {Solution of Equation in Rn (Part 3), Techniques of Scientific Computing (Part 3)},
  editor = {Lions, J. L. and Ciarlet, Philippe},
  series = {Handbook of Numerical Analysis},
  volume = {7},
  pages = {713--1020},
  year = {2000},
  publisher = {Elsevier},
  isbn = {9780444503503},
  doi = {10.1016/S1570-8659(00)070058},
  hal_id = {hal-02100732v2}
}



@article{Raissi2019PINNs,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{LiPino2024,
author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
title = {Physics-Informed Neural Operator for Learning Partial Differential Equations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3648506},
doi = {10.1145/3648506},
abstract = {In this article, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, that is, being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, for example, in multi-scale dynamic systems such as Kolmogorov flows.PROBLEM STATEMENTMachine learning methods have recently shown promise in solving partial differential equations (PDEs) raised in science and engineering. They can be classified into two broad categories: approximating the solution function  and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this paper, we consider a new learning paradigm, aiming to overcome the optimization challenge in PINN and relieve the data requirement in FNO.METHODSIn this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric PDEs.In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family using training data and physics constraints. In the instance-wise fine-tuning phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE using the physics constraints only.Specifically, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator.RESULTSThe resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data.Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. With the equation constraints, PINO requires few to no data to learn the Burgers, Darcy, and Navier-Stokes equation. In particular, PINO accurately solves long temporal transient flows and  Kolmogorov flows where other baseline methods fail to converge.SIGNIFICANCEPINO uses the neural operator framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed. These advantages could lead to applications such as weather forecast, airfoil designs, and turbulence control.},
journal = {ACM / IMS J. Data Sci.},
month = {may},
articleno = {9},
numpages = {27},
keywords = {Neural operators, physics informed learning, partial differential equations}
}

@inproceedings{
du2024neural,
title={Neural Spectral Methods: Self-supervised learning in the spectral domain},
author={Yiheng Du and Nithin Chalapathi and Aditi S. Krishnapriyan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=2DbVeuoa6a}
}


@inproceedings{
chalapathi2024scaling,
title={Scaling physics-informed hard constraints with mixture-of-experts},
author={Nithin Chalapathi and Yiheng Du and Aditi S. Krishnapriyan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=u3dX2CEIZb}
}

@article{ZhuPCDLUQ2019,
title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
journal = {Journal of Computational Physics},
volume = {394},
pages = {56-81},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
author = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
keywords = {Physics-constrained, Normalizing flow, Conditional generative model, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.}
}

@misc{musekamp2024activelearningneuralpde,
      title={Active Learning for Neural PDE Solvers}, 
      author={Daniel Musekamp and Marimuthu Kalimuthu and David Holzmüller and Makoto Takamoto and Mathias Niepert},
      year={2024},
      eprint={2408.01536},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.01536}, 
}


@Article{AndrieuMCMC2003,
author={Andrieu, Christophe
and de Freitas, Nando
and Doucet, Arnaud
and Jordan, Michael I.},
title={An Introduction to MCMC for Machine Learning},
journal={Machine Learning},
year={2003},
month={Jan},
day={01},
volume={50},
number={1},
pages={5-43},
abstract={This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.},
issn={1573-0565},
doi={10.1023/A:1020281327116},
url={https://doi.org/10.1023/A:1020281327116}
}

@inproceedings{
teng2023predictive,
title={Predictive Inference with Feature Conformal Prediction},
author={Jiaye Teng and Chuan Wen and Dinghuai Zhang and Yoshua Bengio and Yang Gao and Yang Yuan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0uRm1YmFTu}
}

@misc{eliasof2020diffgcngraphconvolutionalnetworks,
      title={DiffGCN: Graph Convolutional Networks via Differential Operators and Algebraic Multigrid Pooling}, 
      author={Moshe Eliasof and Eran Treister},
      year={2020},
      eprint={2006.04115},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.04115}, 
}

@article{box1976science,
  author = {Box, George E. P.},
  title = {Science and Statistics},
  journal = {Journal of the American Statistical Association},
  volume = {71},
  number = {356},
  pages = {791--799},
  year = {1976},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1976.10480949}
}


@PHDTHESIS{Math_Guarantees_PIML,
	copyright = {In Copyright - Non-Commercial Use Permitted},
	year = {2024},
	type = {Doctoral Thesis},
	author = {De Ryck, Tim},
	size = {185 p.},
	abstract = {Physics-informed machine learning is a popular framework that allows the numerical simulation of both forward and inverse problems for partial differential equations without needing any (potentially expensive) training data. It consists of the optimization of a parametric model based on a PDE residual-based loss function, with the most famous example being physics-informed neural networks (PINNs). This thesis addresses the relatively paucity in mathematical guarantees concerning the performance of these physics-informed models by proposing a unified framework in which the numerical analysis of the various components of the incurred error can be effectively carried out. We develop rigorous results on approximation, generalization and training errors and investigate their behavior with respect to the dimension of the underlying domain and the type of the PDE, with the Navier-Stokes equations, high-dimensional linear Kolmogorov equations and inviscid scalar conservation laws serving as primary case studies. In order to provide bounds on the approximation error, we prove that neural networks can approximate Sobolev regular functions arbitrarily well in higher-order Sobolev norms and that generic approximation results for neural networks, PINNs, and (physics-informed) operator learning can be obtained from each other after verifying a few assumptions. We demonstrate that approximation results can be obtained even when PDE solutions are discontinuous and the physics-informed loss function is based on a weak PDE residual, such as for weak PINNs for scalar conservation laws. Next, we prove for a number of PDEs that a small physics-informed loss indeed implies a small L 2 -error and investigate whether such stability holds as well for extended PINNs (XPINNs) and conservative PINNs (cPINNs). We follow up by providing upper bounds on the generalization gap, leading to guidelines on the necessary size of the training set for the model to generalize well to the whole (unseen) domain. Finally, we investigate the behavior of gradient descent algorithms in physics-informed machine learning and find that the difficulty in training these models is closely related to the conditioning of a differential operator associated to the Hermitian square of the differential operator of the underlying PDE, and to the chosen model. If this operator is ill-conditioned, it results in slow or infeasible training, which suggest that preconditioning this operator is crucial. We employ both rigorous mathematical analysis and empirical evaluations to investigate various strategies, explaining how they better condition this critical operator, and consequently improve training.},
	language = {en},
	address = {Zurich},
	publisher = {ETH Zurich},
	DOI = {10.3929/ethz-b-000674112},
	title = {Mathematical guarantees for physics-informed machine learning},
	school = {ETH Zurich}
}

@Book{Reddy2006FEM,
author={Reddy, J. N.},
title={Introduction to the Finite Element Method, Third Edition},
year={2006},
edition={3rd edition.},
publisher={McGraw-Hill Education},
address={New York},
abstract={J. N. Reddy's An Introduction to the Finite Element Method, Third Edition, is an update of one of the most popular FEM textbooks available. The book retains its strong conceptual approach, clearly examining the mathematical underpinnings of FEM and providing a general approach to engineering application areas. Known for its detailed, carefully selected example problems and extensive selection of homework problems, this book comprehensively covers a wide range of engineering areas, making it appropriate for all engineering majors, and underscores the wide range of use FEM has in the professional world. A supplementary text Web site located at http://www.mhhe.com/reddy3e contains password-protected solutions to end-of-chapter problems, general textbook information, chapters on the FEM1D and FEM2D computer programs, and more!},
isbn={9780072466850},
url={https://www.accessengineeringlibrary.com/content/book/9780072466850},
language={en}
}

@article{carbonfootprint_CFD,
    author = {Horwitz, J. A. K.},
    title = "{Estimating the carbon footprint of computational fluid dynamics}",
    journal = {Physics of Fluids},
    volume = {36},
    number = {4},
    pages = {045109},
    year = {2024},
    month = {04},
    abstract = "{Computational resources have grown exponentially in the past few decades. These machines make possible research and design in fields as diverse as medicine, astronomy, and engineering. Despite ever-increasing computational capabilities, direct simulation of complex systems has remained challenging owing to the degrees of freedom involved. At the cusp of exascale computing, high-resolution simulation of practical problems with minimal model assumptions may soon experience a renaissance. However, growing reliance on modern computers comes at the cost of a growing carbon footprint. To illustrate this, we examine historic computations in fluid dynamics where larger computers have afforded the opportunity to simulate flows at increasingly relevant Reynolds numbers. Under a variety of flow configurations, the carbon footprint of such simulations is found to scale roughly with the fourth power of Reynolds number. This is primarily explained by the computation cost in core-hours, which is also described by similar scaling, though regional differences in renewable energy use also play a role. Using the established correlation, we examine a large database of simulations to develop estimates for the carbon footprint of computational fluid dynamics in a given year. Collectively, the analysis provides an additional benchmark for new computations where, in addition to balancing considerations of model fidelity, carbon footprint should also be considered.}",
    issn = {1070-6631},
    doi = {10.1063/5.0199350},
    url = {https://doi.org/10.1063/5.0199350},
    eprint = {https://pubs.aip.org/aip/pof/article-pdf/doi/10.1063/5.0199350/19864736/045109\_1\_5.0199350.pdf},
}



@misc{gopakumar2023fourierneuraloperatorplasma,
      title={Fourier Neural Operator for Plasma Modelling}, 
      author={Vignesh Gopakumar and Stanislas Pamela and Lorenzo Zanisi and Zongyi Li and Anima Anandkumar and MAST Team},
      year={2023},
      eprint={2302.06542},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2302.06542}, 
}

@inproceedings{2019MaddoxSWAG,
 author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/118921efba23fc329e6560b27861f0c2-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{
bartolucci2023representation,
title={Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning},
author={Francesca Bartolucci and Emmanuel de Bezenac and Bogdan Raonic and Roberto Molinaro and Siddhartha Mishra and Rima Alaifari},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=7LSEkvEGCM}
}

@article{
mccabe2023towards,
title={Towards Stability of Autoregressive Neural Operators},
author={Michael McCabe and Peter Harrington and Shashank Subramanian and Jed Brown},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=RFfUUtKYOG},
note={}
}

@misc{nunn2023shapingmagneticfieldcoils,
      title={Shaping of Magnetic Field Coils in Fusion Reactors using Bayesian Optimisation}, 
      author={Timothy Nunn and Vignesh Gopakumar and Sebastien Kahn},
      year={2023},
      eprint={2310.01455},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2310.01455}, 
}


@Inbook{Somov2012,
author="Somov, Boris V.",
title="Plasma Equilibrium in Magnetic Field",
bookTitle="Plasma Astrophysics, Part I: Fundamentals and Practice",
year="2012",
publisher="Springer New York",
address="New York, NY",
pages="403--427",
abstract="The concept of equilibrium is fundamental to any discussion of the energy contained in an astrophysical object or phenomenon. The MHD non-equilibrium is often related to the onset of dynamic phenomena in astrophysical plasma, for example, in the solar corona. In this chapter, we derive the virial theorem and the famous Shafranov theorem as well as we consider some general properties of equilibrium configurations.",
isbn="978-1-4614-4283-7",
doi="10.1007/978-1-4614-4283-7_19",
url="https://doi.org/10.1007/978-1-4614-4283-7_19"
}

@Article{Amorisco2024,
author={Amorisco, N. C.
and Agnello, A.
and Holt, G.
and Mars, M.
and Buchanan, J.
and Pamela, S.},
title={FreeGSNKE: A Python-based dynamic free-boundary toroidal plasma equilibrium solver},
journal={Physics of Plasmas},
year={2024},
month={Apr},
day={30},
volume={31},
number={4},
pages={042517},
abstract={We present a Python-based numerical solver for the two-dimensional dynamic plasma equilibrium problem. We model the time evolution of toroidally symmetric free-boundary tokamak plasma equilibria in the presence of the non-linear magnetohydrodynamic coupling with both currents in the ``active'' poloidal field coils, with assigned applied voltages, and eddy currents in the tokamak passive structures. FreeGSNKE (FreeGS Newton--Krylov Evolutive) builds and expands on the framework provided by the Python package FreeGS (Free boundary Grad--Shafranov). FreeGS solves the static free-boundary Grad--Shafranov (GS) problem, discretized in space using finite differences, by means of Picard iterations. FreeGSNKE introduces: (i) a solver for the static free-boundary GS problem based on the Newton--Krylov (NK) method, with improved stability and convergence properties; (ii) a solver for the linearized dynamic plasma equilibrium problem; and (iii) a solver for the non-linear dynamic problem, based on the NK method. We propose a novel ``staggered'' solution strategy for the non-linear problem, in which we make use of a set of equivalent formulations of the non-linear dynamic problem we derive. The alternation of NK solution steps in the currents and in the plasma flux lends this strategy an increased resilience to co-linearity and stagnation problems, resulting in favorable convergence properties. FreeGSNKE can be used for any user-defined tokamak geometry and coil configuration. FreeGSNKE's flexibility and ease of use make it a suitably robust control-oriented simulator of plasma magnetic equilibria. FreeGSNKE is entirely written in Python and easily interfaced with Python libraries, which facilitates machine learning based approaches to plasma control.},
issn={1070-664X},
doi={10.1063/5.0188467},
url={https://doi.org/10.1063/5.0188467}
}

@article{Lao_1985,
doi = {10.1088/0029-5515/25/11/007},
url = {https://dx.doi.org/10.1088/0029-5515/25/11/007},
year = {1985},
month = {nov},
publisher = {},
volume = {25},
number = {11},
pages = {1611},
author = {Lao, L.L. and St. John, H. and Stambaugh, R.D. and Kellman, A.G. and Pfeiffer, W.},
title = {Reconstruction of current profile parameters and plasma shapes in tokamaks},
journal = {Nuclear Fusion},
abstract = {An efficient method is given to reconstruct the current profile parameters, the plasma shape, and a current profile consistent with the magnetohydrodynamic equilibrium constraint from external magnetic measurements, based on a Picard iteration approach which approximately conserves the measurements. Computational efforts are reduced by parametrizing the current profile linearly in terms of a number of physical parameters. Results of detailed comparative calculations and a sensitivity study are described. Illustrative calculations to reconstruct the current profiles and plasma shapes in ohmically and auxiliarily heated Doublet III plasmas are given which show many interesting features of the current profiles.}
}

@Article{Joung2023,
author={Joung, Semin
and Ghim, Y.-C.
and Kim, Jaewook
and Kwak, Sehyun
and Kwon, Daeho
and Sung, C.
and Kim, D.
and Kim, Hyun-Seok
and Bak, J. G.
and Yoon, S. W.},
title={GS-DeepNet: mastering tokamak plasma equilibria with deep neural networks and the Grad--Shafranov equation},
journal={Scientific Reports},
year={2023},
month={Sep},
day={22},
volume={13},
number={1},
pages={15799},
abstract={The force-balanced state of magnetically confined plasmas heated up to 100 million degrees Celsius must be sustained long enough to achieve a burning-plasma state, such as in the case of ITER, a fusion reactor that promises a net energy gain. This force balance between the Lorentz force and the pressure gradient force, known as a plasma equilibrium, can be theoretically portrayed together with Maxwell's equations as plasmas are collections of charged particles. Nevertheless, identifying the plasma equilibrium in real time is challenging owing to its free-boundary and ill-posed conditions, which conventionally involves iterative numerical approach with a certain degree of subjective human decisions such as including or excluding certain magnetic measurements to achieve numerical convergence on the solution as well as to avoid unphysical solutions. Here, we introduce GS-DeepNet, which learns plasma equilibria through solely unsupervised learning, without using traditional numerical algorithms. GS-DeepNet includes two neural networks and teaches itself. One neural network generates a possible candidate of an equilibrium following Maxwell's equations and is taught by the other network satisfying the force balance under the equilibrium. Measurements constrain both networks. Our GS-DeepNet achieves reliable equilibria with uncertainties in contrast with existing methods, leading to possible better control of fusion-grade plasmas.},
issn={2045-2322},
doi={10.1038/s41598-023-42991-5},
url={https://doi.org/10.1038/s41598-023-42991-5}
}

@Article{Jang2024,
author={Jang, Byoungchan
and Kaptanoglu, Alan A.
and Gaur, Rahul
and Pan, Shaowu
and Landreman, Matt
and Dorland, William},
title={Grad--Shafranov equilibria via data-free physics informed neural networks},
journal={Physics of Plasmas},
year={2024},
month={Mar},
day={25},
volume={31},
number={3},
pages={032510},
abstract={A large number of magnetohydrodynamic (MHD) equilibrium calculations are often required for uncertainty quantification, optimization, and real-time diagnostic information, making MHD equilibrium codes vital to the field of plasma physics. In this paper, we explore a method for solving the Grad--Shafranov equation by using physics-informed neural networks (PINNs). For PINNs, we optimize neural networks by directly minimizing the residual of the partial differential equation as a loss function. We show that PINNs can accurately and effectively solve the Grad--Shafranov equation with several different boundary conditions, making it more flexible than traditional solvers. This method is flexible as it does not require any mesh and basis choice, thereby streamlining the computational process. We also explore the parameter space by varying the size of the model, the learning rate, and boundary conditions to map various tradeoffs such as between reconstruction error and computational speed. Additionally, we introduce a parameterized PINN framework, expanding the input space to include variables such as pressure, aspect ratio, elongation, and triangularity in order to handle a broader range of plasma scenarios within a single network. Parameterized PINNs could be used in future work to solve inverse problems such as shape optimization.},
issn={1070-664X},
doi={10.1063/5.0188634},
url={https://doi.org/10.1063/5.0188634}
}

@inproceedings{
takamoto2023pdebench,
title={{PDEBENCH}: {AN} {EXTENSIVE} {BENCHMARK} {FOR} {SCI}- {ENTIFIC} {MACHINE} {LEARNING}},
author={Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pfl{\"u}ger and Mathias Niepert},
booktitle={ICLR 2023 Workshop on Physics for Machine Learning},
year={2023},
url={https://openreview.net/forum?id=b8SwOxZQ2kj}
}

@article{taylor_truncation,
author = {MacKinnon, Robert J. and Johnson, Richard W.},
title = {Differential-equation-based representation of truncation errors for accurate numerical simulation},
journal = {International Journal for Numerical Methods in Fluids},
volume = {13},
number = {6},
pages = {739-757},
keywords = {High-order finite difference method, Convection diffusion, Upwind differencing, Artificial diffusion, Navier-Stokes},
doi = {https://doi.org/10.1002/fld.1650130606},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/fld.1650130606},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/fld.1650130606},
abstract = {Abstract High-order compact finite difference schemes for two-dimensional convection-diffusion-type differential equations with constant and variable convection coefficients are derived. The governing equations are employed to represent leading truncation terms, including cross-derivatives, making the overall O(h4) schemes conform to a 3 × 3 stencil. We show that the two-dimensional constant coefficient scheme collapses to the optimal scheme for the one-dimensional case wherein the finite difference equation yields nodally exact results. The two-dimensional schemes are tested against standard model problems, including a Navier-Stokes application. Results show that the two schemes are generally more accurate, on comparable grids, than O(h2) centred differencing and commonly used O(h) and O(h3) upwinding schemes.},
year = {1991}
}

@article{optimal_tracking_tokamak,
author = {Li, Shunjie and Jiang, H. and Ren, Zhigang and Xu, C.},
year = {2014},
month = {06},
pages = {1-8},
title = {Optimal Tracking for a Divergent-Type Parabolic PDE System in Current Profile Control},
volume = {2014},
journal = {Abstract and Applied Analysis},
doi = {10.1155/2014/940965}
}


@article{Hendrick2024PlasmaBurn,
author = {Meyer, Hendrik and STEP Team},
title = {Plasma burn—mind the gap},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {382},
number = {2280},
pages = {20230406},
year = {2024},
doi = {10.1098/rsta.2023.0406},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2023.0406},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2023.0406}
,
    abstract = { The programme to design plasma scenarios for the Spherical Tokamak for Energy Production (STEP), a reactor concept aiming at net electricity production, seeks to exploit the inherent advantages of the spherical tokamak (ST) while making conservative assumptions about plasma performance. This approach is motivated by the large gap between present-day STs and future burning plasmas based on this concept. It is concluded that plasma exhaust in such a device is most likely to be manageable in a double null (DN) configuration, and that high core performance is favoured by positive triangularity (PT) plasmas with an elevated central safety factor. Based on a full technical and physics assessment of external heating and current drive (CD) systems, it was decided that the external CD is provided most effectively by microwaves. Operation with active resistive wall mode (RWM) stabilization as well as high elongation is needed for the most compact solution. The gap between existing devices and STEP is most pronounced in the area of core transport, owing to high normalized plasma pressure in the latter which changes qualitatively the nature of the turbulence controlling transport. Plugging this gap will require dedicated experiments, particularly on high-performance STs, and the development of reduced models that faithfully represent turbulent transport at high normalized pressure. Plasma scenarios in STEP will also need to be such that edge localized modes (ELMs) either do not occur or are small enough to be compatible with material lifetime limits. The high current needed for a power plant-relevant plasma leads to the unavoidable generation of high runaway electron beam current during a disruption, where novel mitigation techniques may be needed. This article is part of the theme issue ‘Delivering Fusion Energy – The Spherical Tokamak for Energy Production (STEP)’. }
}

@misc{gray2025guaranteedconfidencebandenclosurespde,
      title={Guaranteed confidence-band enclosures for PDE surrogates}, 
      author={Ander Gray and Vignesh Gopakumar and Sylvain Rousseau and Sébastien Destercke},
      year={2025},
      eprint={2501.18426},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.18426}, 
}


@ARTICLE{conv_correlation,
  author={Jordà, Marc and Valero-Lara, Pedro and Peña, Antonio J.},
  journal={IEEE Access}, 
  title={Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs}, 
  year={2019},
  volume={7},
  number={},
  pages={70461-70473},
  keywords={Convolution;Neural networks;Training;Graphics processing units;Two dimensional displays;Three-dimensional displays;Performance evaluation;Neural network;convolution;deep learning;cuDNN;GPU;volta},
  doi={10.1109/ACCESS.2019.2918851}}

@misc{OpenFOAM,
  author = {{OpenFOAM Foundation}},
  title = {OpenFOAM: The Open Source CFD Toolbox},
  year = {2025},
  url = {https://openfoam.org/},
  note = {Accessed: 2025-04-15}
}

@misc{chen2019learningimplicitfieldsgenerative,
      title={Learning Implicit Fields for Generative Shape Modeling}, 
      author={Zhiqin Chen and Hao Zhang},
      year={2019},
      eprint={1812.02822},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/1812.02822}, 
}

@inproceedings{Sitzmann2020,
 author = {Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7462--7473},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Neural Representations with Periodic Activation Functions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/53c04118df112c13a8c34b38343b9c10-Paper.pdf},
 volume = {33},
 year = {2020}
}


@Article{Cai2021,
author={Cai, Shengze
and Mao, Zhiping
and Wang, Zhicheng
and Yin, Minglang
and Karniadakis, George Em},
title={Physics-informed neural networks (PINNs) for fluid mechanics: a review},
journal={Acta Mechanica Sinica},
year={2021},
month={Dec},
day={01},
volume={37},
number={12},
pages={1727-1738},
abstract={Despite the significant progress over the last 50 years in simulating flow problems using numerical discretization of the Navier--Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse flow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review flow physics-informed learning, integrating seamlessly data and mathematical models, and implement them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake flows, supersonic flows, and biomedical flows.},
issn={1614-3116},
doi={10.1007/s10409-021-01148-1},
url={https://doi.org/10.1007/s10409-021-01148-1}
}


@Article{Cuomo2022,
author={Cuomo, Salvatore
and Di Cola, Vincenzo Schiano
and Giampaolo, Fabio
and Rozza, Gianluigi
and Raissi, Maziar
and Piccialli, Francesco},
title={Scientific Machine Learning Through Physics--Informed Neural Networks: Where we are and What's Next},
journal={Journal of Scientific Computing},
year={2022},
month={Jul},
day={26},
volume={92},
number={3},
pages={88},
abstract={Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
issn={1573-7691},
doi={10.1007/s10915-022-01939-z},
url={https://doi.org/10.1007/s10915-022-01939-z}
}

@misc{wang2020pinnsfailtrainneural,
      title={When and why PINNs fail to train: A neural tangent kernel perspective}, 
      author={Sifan Wang and Xinling Yu and Paris Perdikaris},
      year={2020},
      eprint={2007.14527},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2007.14527}, 
}

@misc{rathore2024challengestrainingpinnsloss,
      title={Challenges in Training PINNs: A Loss Landscape Perspective}, 
      author={Pratik Rathore and Weimu Lei and Zachary Frangella and Lu Lu and Madeleine Udell},
      year={2024},
      eprint={2402.01868},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01868}, 
}

@article{Sahli_Costabal_2024,
   title={$\Delta$-PINNs: Physics-informed neural networks on complex geometries},
   volume={127},
   ISSN={0952-1976},
   url={http://dx.doi.org/10.1016/j.engappai.2023.107324},
   DOI={10.1016/j.engappai.2023.107324},
   journal={Engineering Applications of Artificial Intelligence},
   publisher={Elsevier BV},
   author={Sahli Costabal, Francisco and Pezzuto, Simone and Perdikaris, Paris},
   year={2024},
   month=jan, pages={107324} }

@Article{Weinan2018,
author={E, Weinan
and Yu, Bing},
title={The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
journal={Communications in Mathematics and Statistics},
year={2018},
month={Mar},
day={01},
volume={6},
number={1},
pages={1-12},
abstract={We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
issn={2194-671X},
doi={10.1007/s40304-018-0127-z},
url={https://doi.org/10.1007/s40304-018-0127-z}
}

@article{Jagtap2020,
title = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {365},
pages = {113028},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2020.113028},
url = {https://www.sciencedirect.com/science/article/pii/S0045782520302127},
author = {Ameya D. Jagtap and Ehsan Kharazmi and George Em Karniadakis},
keywords = {cPINN, Mortar PINN, Domain decomposition, Machine learning, Conservation laws, Inverse problems},
abstract = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.}
}

@article{Sirignano_2018,
   title={DGM: A deep learning algorithm for solving partial differential equations},
   volume={375},
   ISSN={0021-9991},
   url={http://dx.doi.org/10.1016/j.jcp.2018.08.029},
   DOI={10.1016/j.jcp.2018.08.029},
   journal={Journal of Computational Physics},
   publisher={Elsevier BV},
   author={Sirignano, Justin and Spiliopoulos, Konstantinos},
   year={2018},
   month=dec, pages={1339–1364} }


@article{Zhu2019,
title = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
journal = {Journal of Computational Physics},
volume = {394},
pages = {56-81},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
author = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
keywords = {Physics-constrained, Normalizing flow, Conditional generative model, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.}
}

@article{Sun2020,
title = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {361},
pages = {112732},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.112732},
url = {https://www.sciencedirect.com/science/article/pii/S004578251930622X},
author = {Luning Sun and Han Gao and Shaowu Pan and Jian-Xun Wang},
keywords = {Physics-informed machine learning, Label-free, Neural networks, Uncertainty quantification, Cardiovascular flows, Navier-Stokes},
abstract = {Numerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics and sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time applications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty quantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL) has shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality. However, the off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and interpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient in most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical simulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural network (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential equations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments are conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties in fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.}
}

@misc{cohen2016groupequivariantconvolutionalnetworks,
      title={Group Equivariant Convolutional Networks}, 
      author={Taco S. Cohen and Max Welling},
      year={2016},
      eprint={1602.07576},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1602.07576}, 
}



@Article{Lagrave2022,
AUTHOR = {Lagrave, Pierre-Yves and Tron, Eliot},
TITLE = {Equivariant Neural Networks and Differential Invariants Theory for Solving Partial Differential Equations},
JOURNAL = {Physical Sciences Forum},
VOLUME = {5},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {13},
URL = {https://www.mdpi.com/2673-9984/5/1/13},
ISSN = {2673-9984},
ABSTRACT = {This paper discusses the use of Equivariant Neural Networks (ENN) for solving Partial Differential Equations by exploiting their underlying symmetry groups. We first show that Group-Convolutionnal Neural Networks can be used to generalize Physics-Informed Neural Networks and then consider the use of ENN to approximate differential invariants of a given symmetry group, hence allowing to build symmetry-preserving Finite Difference methods without the need to formally derivate corresponding numerical invariantizations. The benefit of our approach is illustrated on the 2D heat equation through the instantiation of an SE(2) symmetry-preserving discretization.},
DOI = {10.3390/psf2022005013}
}



@misc{Thomas2018tensorfieldnetworksrotation,
      title={Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds}, 
      author={Nathaniel Thomas and Tess Smidt and Steven Kearnes and Lusann Yang and Li Li and Kai Kohlhoff and Patrick Riley},
      year={2018},
      eprint={1802.08219},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1802.08219}, 
}

@misc{Weiler20183dsteerablecnnslearning,
      title={3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data}, 
      author={Maurice Weiler and Mario Geiger and Max Welling and Wouter Boomsma and Taco Cohen},
      year={2018},
      eprint={1807.02547},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.02547}, 
}

@misc{cohen2018sphericalcnns,
      title={Spherical CNNs}, 
      author={Taco S. Cohen and Mario Geiger and Jonas Koehler and Max Welling},
      year={2018},
      eprint={1801.10130},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.10130}, 
}

@article{Weyn2019,
author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
title = {Improving Data-Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {12},
number = {9},
pages = {e2020MS002109},
doi = {https://doi.org/10.1029/2020MS002109},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002109},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002109},
note = {e2020MS002109 10.1029/2020MS002109},
abstract = {Abstract We present a significantly improved data-driven global weather forecasting framework using a deep convolutional neural network (CNN) to forecast several basic atmospheric variables on a global grid. New developments in this framework include an off-line volume-conservative mapping to a cubed-sphere grid, improvements to the CNN architecture and the minimization of the loss function over multiple steps in a prediction sequence. The cubed-sphere remapping minimizes the distortion on the cube faces on which convolution operations are performed and provides natural boundary conditions for padding in the CNN. Our improved model produces weather forecasts that are indefinitely stable and produce realistic weather patterns at lead times of several weeks and longer. For short- to medium-range forecasting, our model significantly outperforms persistence, climatology, and a coarse-resolution dynamical numerical weather prediction (NWP) model. Unsurprisingly, our forecasts are worse than those from a high-resolution state-of-the-art operational NWP system. Our data-driven model is able to learn to forecast complex surface temperature patterns from few input atmospheric state variables. On annual time scales, our model produces a realistic seasonal cycle driven solely by the prescribed variation in top-of-atmosphere solar forcing. Although it currently does not compete with operational weather forecasting models, our data-driven CNN executes much faster than those models, suggesting that machine learning could prove to be a valuable tool for large-ensemble forecasting.},
year = {2020}
}

@misc{unke2021se3equivariantpredictionmolecularwavefunctions,
      title={SE(3)-equivariant prediction of molecular wavefunctions and electronic densities}, 
      author={Oliver T. Unke and Mihail Bogojeski and Michael Gastegger and Mario Geiger and Tess Smidt and Klaus-Robert Müller},
      year={2021},
      eprint={2106.02347},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      url={https://arxiv.org/abs/2106.02347}, 
}

@misc{brandstetter2022liepointsymmetrydata,
      title={Lie Point Symmetry Data Augmentation for Neural PDE Solvers}, 
      author={Johannes Brandstetter and Max Welling and Daniel E. Worrall},
      year={2022},
      eprint={2202.07643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.07643}, 
}

@misc{satorras2022enequivariantgraphneural,
      title={E(n) Equivariant Graph Neural Networks}, 
      author={Victor Garcia Satorras and Emiel Hoogeboom and Max Welling},
      year={2022},
      eprint={2102.09844},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.09844}, 
}

@misc{bronstein2021geometricdeeplearninggrids,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.13478}, 
}


@InProceedings{Finzi2020,
  title = 	 {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
  author =       {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {3165--3176},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/finzi20a/finzi20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/finzi20a.html},
  abstract = 	 {The translation equivariance of convolutional layers enables CNNs to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.}
}

@inproceedings{
knigge2024spacetime,
title={Space-Time Continuous {PDE} Forecasting using Equivariant Neural Fields},
author={David M Knigge and David Wessels and Riccardo Valperga and Samuele Papa and Jan-Jakob Sonke and Erik J Bekkers and Stratis Gavves},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=wN5AgP0DJ0}
}

@article{Mathews2021,
  title = {Uncovering turbulent plasma dynamics via deep learning from partial observations},
  author = {Mathews, A. and Francisquez, M. and Hughes, J. W. and Hatch, D. R. and Zhu, B. and Rogers, B. N.},
  journal = {Phys. Rev. E},
  volume = {104},
  issue = {2},
  pages = {025205},
  numpages = {11},
  year = {2021},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.104.025205},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.104.025205}
}

@article{NSFnets2021,
title = {NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations},
journal = {Journal of Computational Physics},
volume = {426},
pages = {109951},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109951},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120307257},
author = {Xiaowei Jin and Shengze Cai and Hui Li and George Em Karniadakis},
keywords = {PINNs, Turbulence, Velocity-pressure formulation, Vorticity-velocity formulation, Ill-posed problems, Transfer learning},
abstract = {In the last 50 years there has been a tremendous progress in solving numerically the Navier-Stokes equations using finite differences, finite elements, spectral, and even meshless methods. Yet, in many real cases, we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity applications the mesh generation is time consuming and still an art. Moreover, solving ill-posed problems (e.g., lacking boundary conditions) or inverse problems is often prohibitively expensive and requires different formulations and new computer codes. Here, we employ physics-informed neural networks (PINNs), encoding the governing equations directly into the deep neural network via automatic differentiation, to overcome some of the aforementioned limitations for simulating incompressible laminar and turbulent flows. We develop the Navier-Stokes flow nets (NSFnets) by considering two different mathematical formulations of the Navier-Stokes equations: the velocity-pressure (VP) formulation and the vorticity-velocity (VV) formulation. Since this is a new approach, we first select some standard benchmark problems to assess the accuracy, convergence rate, computational cost and flexibility of NSFnets; analytical solutions and direct numerical simulation (DNS) databases provide proper initial and boundary conditions for the NSFnet simulations. The spatial and temporal coordinates are the inputs of the NSFnets, while the instantaneous velocity and pressure fields are the outputs for the VP-NSFnet, and the instantaneous velocity and vorticity fields are the outputs for the VV-NSFnet. This is unsupervised learning and, hence, no labeled data are required beyond boundary and initial conditions and the fluid properties. The residuals of the VP or VV governing equations, together with the initial and boundary conditions, are embedded into the loss function of the NSFnets. No data is provided for the pressure to the VP-NSFnet, which is a hidden state and is obtained via the incompressibility constraint without extra computational cost. Unlike the traditional numerical methods, NSFnets inherit the properties of neural networks (NNs), hence the total error is composed of the approximation, the optimization, and the generalization errors. Here, we empirically attempt to quantify these errors by varying the sampling (“residual”) points, the iterative solvers, and the size of the NN architecture. For the laminar flow solutions, we show that both the VP and the VV formulations are comparable in accuracy but their best performance corresponds to different NN architectures. The initial convergence rate is fast but the error eventually saturates to a plateau due to the dominance of the optimization error. For the turbulent channel flow, we show that NSFnets can sustain turbulence at Reτ∼1,000, but due to expensive training we only consider part of the channel domain and enforce velocity boundary conditions on the subdomain boundaries provided by the DNS data base. We also perform a systematic study on the weights used in the loss function for balancing the data and physics components, and investigate a new way of computing the weights dynamically to accelerate training and enhance accuracy. In the last part, we demonstrate how NSFnets should be used in practice, namely for ill-posed problems with incomplete or noisy boundary conditions as well as for inverse problems. We obtain reasonably accurate solutions for such cases as well without the need to change the NSFnets and at the same computational cost as in the forward well-posed problems. We also present a simple example of transfer learning that will aid in accelerating the training of NSFnets for different parameter settings.}
}

@misc{cranmer2020lagrangianneuralnetworks,
      title={Lagrangian Neural Networks}, 
      author={Miles Cranmer and Sam Greydanus and Stephan Hoyer and Peter Battaglia and David Spergel and Shirley Ho},
      year={2020},
      eprint={2003.04630},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2003.04630}, 
}


@misc{greydanus2019hamiltonianneuralnetworks,
      title={Hamiltonian Neural Networks}, 
      author={Sam Greydanus and Misko Dzamba and Jason Yosinski},
      year={2019},
      eprint={1906.01563},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1906.01563}, 
}

@misc{lanthaler2024discretizationerrorfourierneural,
      title={Discretization Error of Fourier Neural Operators}, 
      author={Samuel Lanthaler and Andrew M. Stuart and Margaret Trautner},
      year={2024},
      eprint={2405.02221},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2405.02221}, 
}

@Article{Bedi2025,
author={Bedi, Sanchit
and Tiwari, Karn
and A. P., Prathosh
and Kota, Sri Harsha
and Krishnan, N. M. Anoop},
title={A neural operator for forecasting carbon monoxide evolution in cities},
journal={npj Clean Air},
year={2025},
month={Mar},
day={12},
volume={1},
number={1},
pages={2},
abstract={Real-time forecasting of carbon monoxide (CO) concentrations is essential for enabling timely interventions to improve urban air quality. Conventional air quality models often require extensive computational resources for accurate, multi-scale predictions, limiting their practicality for rapid, real-time application. To address this challenge, we introduce the Complex Neural Operator for Air Quality (CoNOAir), a machine learning model that forecast CO concentrations efficiently. CoNOAir demonstrates superior performance over state-of-the-art models, such as the Fourier Neural Operator (FNO), in both short-term (hourly) and extended (72-h) forecasts at a national scale. It excels in capturing extreme pollution events and performs consistently across multiple Indian cities, achieving an R2 above 0.95 for hourly CO predictions across all evaluated locations. CoNOAir equips authorities with an effective tool for issuing early warnings and designing targeted intervention strategies. This work marks a step forward in achieving dependable, real-time CO pollution predictions for densely populated urban centres.},
issn={3059-2240},
doi={10.1038/s44407-024-00002-5},
url={https://doi.org/10.1038/s44407-024-00002-5}
}




@inproceedings{
serrano2023operator,
title={Operator Learning with Neural Fields: Tackling {PDE}s on General Geometries},
author={Louis Serrano and Lise Le Boudec and Armand Kassa{\"\i} Koupa{\"\i} and Thomas X Wang and Yuan Yin and Jean-No{\"e}l Vittaut and patrick gallinari},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=4jEjq5nhg1}
}

@misc{herde2024poseidonefficientfoundationmodels,
      title={Poseidon: Efficient Foundation Models for PDEs}, 
      author={Maximilian Herde and Bogdan Raonić and Tobias Rohner and Roger Käppeli and Roberto Molinaro and Emmanuel de Bézenac and Siddhartha Mishra},
      year={2024},
      eprint={2405.19101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.19101}, 
}

@article{rahman2024pretraining,
  title={Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs},
  author={Rahman, Md Ashiqur and George, Robert Joseph and Elleithy, Mogab and Leibovici, Daniel and Li, Zongyi and Bonev, Boris and White, Colin and Berner, Julius and Yeh, Raymond A and Kossaifi, Jean and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  year={2024}
}

@inproceedings{
alkin2024universal,
title={Universal Physics Transformers: A Framework For Efficiently Scaling Neural Operators},
author={Benedikt Alkin and Andreas F{\"u}rst and Simon Lucas Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=oUXiNX5KRm}
}

@misc{kossaifi2023multigridtensorizedfourierneural,
      title={Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs}, 
      author={Jean Kossaifi and Nikola Kovachki and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2023},
      eprint={2310.00120},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.00120}, 
}

@misc{lee2023autoregressiverenaissanceneuralpde,
      title={Autoregressive Renaissance in Neural PDE Solvers}, 
      author={Yolanne Yi Ran Lee},
      year={2023},
      eprint={2310.19763},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.19763}, 
}

@misc{koehler2024apebenchbenchmarkautoregressiveneural,
      title={APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs}, 
      author={Felix Koehler and Simon Niedermayr and Rüdiger Westermann and Nils Thuerey},
      year={2024},
      eprint={2411.00180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00180}, 
}

@misc{zhou2024predictingchangestatesalternate,
      title={Predicting Change, Not States: An Alternate Framework for Neural PDE Surrogates}, 
      author={Anthony Zhou and Amir Barati Farimani},
      year={2024},
      eprint={2412.13074},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.13074}, 
}

@misc{li2020multipolegraphneuraloperator,
      title={Multipole Graph Neural Operator for Parametric Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2020},
      eprint={2006.09535},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.09535}, 
}

@article{Rosofsky_2023,
doi = {10.1088/2632-2153/acd168},
url = {https://dx.doi.org/10.1088/2632-2153/acd168},
year = {2023},
month = {may},
publisher = {IOP Publishing},
volume = {4},
number = {2},
pages = {025022},
author = {Rosofsky, Shawn G and Al Majed, Hani and Huerta, E A},
title = {Applications of physics informed neural operators},
journal = {Machine Learning: Science and Technology},
abstract = {We present a critical analysis of physics-informed neural operators (PINOs) to solve partial differential equations (PDEs) that are ubiquitous in the study and modeling of physics phenomena using carefully curated datasets. Further, we provide a benchmarking suite which can be used to evaluate PINOs in solving such problems. We first demonstrate that our methods reproduce the accuracy and performance of other neural operators published elsewhere in the literature to learn the 1D wave equation and the 1D Burgers equation. Thereafter, we apply our PINOs to learn new types of equations, including the 2D Burgers equation in the scalar, inviscid and vector types. Finally, we show that our approach is also applicable to learn the physics of the 2D linear and nonlinear shallow water equations, which involve three coupled PDEs. We release our artificial intelligence surrogates and scientific software to produce initial data and boundary conditions to study a broad range of physically motivated scenarios. We provide the source code, an interactive website to visualize the predictions of our PINOs, and a tutorial for their use at the Data and Learning Hub for Science.}
}


@article{Zhiwei2024BoundaryPINO,
    author = {Fang, Zhiwei and Wang, Sifan and Perdikaris, Paris},
    title = {Learning Only on Boundaries: A Physics-Informed Neural Operator for Solving Parametric Partial Differential Equations in Complex Geometries},
    journal = {Neural Computation},
    volume = {36},
    number = {3},
    pages = {475-498},
    year = {2024},
    month = {02},
    abstract = {Recently, deep learning surrogates and neural operators have shown promise in solving partial differential equations  (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parameterized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from O(Nd) to O(Nd-1), where d is the domain’s dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parameterized complex geometries and unbounded problems.},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01647},
    url = {https://doi.org/10.1162/neco\_a\_01647},
    eprint = {https://direct.mit.edu/neco/article-pdf/36/3/475/2335981/neco\_a\_01647.pdf},
}

@inproceedings{
white2023physicsinformed,
title={Physics-Informed Neural Operators with Exact Differentiation on Arbitrary Geometries},
author={Colin White and Julius Berner and Jean Kossaifi and Mogab Elleithy and David Pitt and Daniel Leibovici and Zongyi Li and Kamyar Azizzadenesheli and Anima Anandkumar},
booktitle={The Symbiosis of Deep Learning and Differential Equations III},
year={2023},
url={https://openreview.net/forum?id=qd7q9lB5hY}
}


@misc{goswami2022physicsinformeddeepneuraloperator,
      title={Physics-Informed Deep Neural Operator Networks}, 
      author={Somdatta Goswami and Aniruddha Bora and Yue Yu and George Em Karniadakis},
      year={2022},
      eprint={2207.05748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.05748}, 
}


@article{Li_2024FVGNN,
   title={Predicting unsteady incompressible fluid dynamics with finite volume informed neural network},
   volume={36},
   ISSN={1089-7666},
   url={http://dx.doi.org/10.1063/5.0197425},
   DOI={10.1063/5.0197425},
   number={4},
   journal={Physics of Fluids},
   publisher={AIP Publishing},
   author={Li, Tianyu and Zou, Shufan and Chang, Xinghua and Zhang, Laiping and Deng, Xiaogang},
   year={2024},
   month=apr }


@inproceedings{Rumelhart1986LearningIRRNN,
  title={Learning internal representations by error propagation},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  booktitle={Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations},
  year={1986},
  url={https://api.semanticscholar.org/CorpusID:62245742}
}

@article{Hochreiter1997LSTM,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}





@misc{cho2014learningphraserepresentationsusing,
      title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
      year={2014},
      eprint={1406.1078},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1406.1078}, 
}


@inproceedings{Krizhevsky2012CNN,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@misc{gopakumar2023fourierrnnsmodellingnoisyphysics,
      title={Fourier-RNNs for Modelling Noisy Physics Data}, 
      author={Vignesh Gopakumar and Stanislas Pamela and Lorenzo Zanisi},
      year={2023},
      eprint={2302.06534},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2302.06534}, 
}

@misc{hu2022neuralpdernnbasedneural,
      title={Neural-PDE: A RNN based neural network for solving time dependent PDEs}, 
      author={Yihao Hu and Tong Zhao and Shixin Xu and Zhiliang Xu and Lizhen Lin},
      year={2022},
      eprint={2009.03892},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2009.03892}, 
}


@misc{saha2021physicsincorporatedconvolutionalrecurrentneural,
      title={Physics-Incorporated Convolutional Recurrent Neural Networks for Source Identification and Forecasting of Dynamical Systems}, 
      author={Priyabrata Saha and Saurabh Dash and Saibal Mukhopadhyay},
      year={2021},
      eprint={2004.06243},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2004.06243}, 
}

@misc{jiang2024multigridgraphunetframework,
      title={A Multigrid Graph U-Net Framework for Simulating Multiphase Flow in Heterogeneous Porous Media}, 
      author={Jiamin Jiang and Jingrun Chen and Zhouwang Yang},
      year={2024},
      eprint={2412.12757},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/2412.12757}, 
}


@article{Le2021Multigrid,
title = {Surrogate modeling of fluid dynamics with a multigrid inspired neural network architecture},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100176},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100176},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000888},
author = {Quang Tuyen Le and Chinchun Ooi},
keywords = {Surrogate modeling, Machine learning, Convolutional Neural Network (CNN), Multigrid, Computational Fluid Dynamics (CFD)},
abstract = {Algebraic or geometric multigrid methods are commonly used in numerical solvers as they are a multi-resolution method able to handle problems with multiple scales. In this work, we propose a modification to the commonly-used U-Net neural network architecture that is inspired by the principles of multigrid methods, referred to here as U-Net-MG. We then demonstrate that this proposed U-Net-MG architecture can successfully reduce the test prediction errors relative to the conventional U-Net architecture when modeling a set of fluid dynamic problems. In total, we demonstrate an improvement in the prediction of velocity and pressure fields for the canonical fluid dynamics cases of flow past a stationary cylinder, flow past 2 cylinders in out-of-phase motion, and flow past an oscillating airfoil in both the propulsion and energy harvesting modes. In general, while both the U-Net and U-Net-MG models can model the systems well with test RMSEs of less than 1%, the use of the U-Net-MG architecture can further reduce RMSEs by between 20% and 70%.}
}

@misc{kurz2025harnessingequivariancemodelingturbulence,
      title={Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks}, 
      author={Marius Kurz and Andrea Beck and Benjamin Sanderse},
      year={2025},
      eprint={2504.07741},
      archivePrefix={arXiv},
      primaryClass={physics.flu-dyn},
      url={https://arxiv.org/abs/2504.07741}, 
}


@article{Mackay1992BNN,
    author = {MacKay, David J. C.},
    title = {A Practical Bayesian Framework for Backpropagation Networks},
    journal = {Neural Computation},
    volume = {4},
    number = {3},
    pages = {448-472},
    year = {1992},
    month = {05},
    abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
    issn = {0899-7667},
    doi = {10.1162/neco.1992.4.3.448},
    url = {https://doi.org/10.1162/neco.1992.4.3.448},
    eprint = {https://direct.mit.edu/neco/article-pdf/4/3/448/812348/neco.1992.4.3.448.pdf},
}


@Article{Breiman1996Bootsrapping,
author={Breiman, Leo},
title={Bagging predictors},
journal={Machine Learning},
year={1996},
month={Aug},
day={01},
volume={24},
number={2},
pages={123-140},
abstract={Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
issn={1573-0565},
doi={10.1007/BF00058655},
url={https://doi.org/10.1007/BF00058655}
}



@article{Hickey2001IntervalArithmetic,
author = {Hickey, T. and Ju, Q. and Van Emden, M. H.},
title = {Interval arithmetic: From principles to implementation},
year = {2001},
issue_date = {September 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/502102.502106},
doi = {10.1145/502102.502106},
abstract = {We start with a mathematical definition of a real interval as a closed, connected set of reals. Interval arithmetic operations (addition, subtraction, multiplication, and division) are likewise defined mathematically and we provide algorithms for computing these operations assuming exact real arithmetic. Next, we define interval arithmetic operations on intervals with IEEE 754 floating point endpoints to be sound and optimal approximations of the real interval operations and we show that the IEEE standard's specification of operations involving the signed infinities, signed zeros, and the exact/inexact flag are such as to make a correct and optimal implementation more efficient. From the resulting theorems, we derive data that are sufficiently detailed to convert directly to a program for efficiently implementing the interval operations. Finally, we extend these results to the case of general intervals, which are defined as connected sets of reals that are not necessarily closed.},
journal = {J. ACM},
month = sep,
pages = {1038–1068},
numpages = {31}
}

@inbook{Kochdumper_2023,
   title={Open- and Closed-Loop Neural Network Verification Using Polynomial Zonotopes},
   ISBN={9783031331701},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-031-33170-1_2},
   DOI={10.1007/978-3-031-33170-1_2},
   booktitle={NASA Formal Methods},
   publisher={Springer Nature Switzerland},
   author={Kochdumper, Niklas and Schilling, Christian and Althoff, Matthias and Bak, Stanley},
   year={2023},
   pages={16–36} }


@inproceedings{Makino2003TAYLORMA,
  title={TAYLOR MODELS AND OTHER VALIDATED FUNCTIONAL INCLUSION METHODS},
  author={Kyoko Makino and Martin Berz},
  booktitle={2003 International Journal of Pure and Applied Mathematics}, 
  year={2003},
  url={https://api.semanticscholar.org/CorpusID:53373973}
}

@INPROCEEDINGS{Gehr2018ZonotopeNN,
  author={Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)}, 
  title={AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation}, 
  year={2018},
  volume={},
  number={},
  pages={3-18},
  keywords={Robustness;Biological neural networks;Cats;Neurons;Safety;Perturbation methods;Reliable Machine Learning;Robustness;Neural Networks;Abstract Interpretation},
  doi={10.1109/SP.2018.00058}}

@misc{amini2020deepevidentialregression,
      title={Deep Evidential Regression}, 
      author={Alexander Amini and Wilko Schwarting and Ava Soleimany and Daniela Rus},
      year={2020},
      eprint={1910.02600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.02600}, 
}

@article{soleimany2021evidential,
  title={Evidential deep learning for guided molecular property prediction and discovery},
  author={Soleimany, Ava P and Amini, Alexander and Goldman, Samuel and Rus, Daniela and Bhatia, Sangeeta N and Coley, Connor W},
  journal={ACS central science},
  volume={7},
  number={8},
  pages={1356--1367},
  year={2021},
  publisher={ACS Publications}
}

@article{zhou2023trustworthy,
  title={Trustworthy fault diagnosis with uncertainty estimation through evidential convolutional neural networks},
  author={Zhou, Hanting and Chen, Wenhe and Cheng, Longsheng and Liu, Jing and Xia, Min},
  journal={IEEE Transactions on Industrial Informatics},
  volume={19},
  number={11},
  pages={10842--10852},
  year={2023},
  publisher={IEEE}
}

@misc{tan2025evidentialphysicsinformedneuralnetworks,
      title={Evidential Physics-Informed Neural Networks}, 
      author={Hai Siong Tan and Kuancheng Wang and Rafe McBeth},
      year={2025},
      eprint={2501.15908},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.15908}, 
}

@misc{liang2024conformalpredictionquantifyinguncertainty,
      title={Conformal Prediction on Quantifying Uncertainty of Dynamic Systems}, 
      author={Aoming Liang and Qi Liu and Lei Xu and Fahad Sohrab and Weicheng Cui and Changhui Song and Moncef Gabbouj},
      year={2024},
      eprint={2412.10459},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.10459}, 
}

@misc{podina2024conformalizedphysicsinformedneuralnetworks,
      title={Conformalized Physics-Informed Neural Networks}, 
      author={Lena Podina and Mahdi Torabi Rad and Mohammad Kohandel},
      year={2024},
      eprint={2405.08111},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.08111}, 
}

@article{Moya2025CPDON,
title = {Conformalized-DeepONet: A distribution-free framework for uncertainty quantification in deep operator networks},
journal = {Physica D: Nonlinear Phenomena},
volume = {471},
pages = {134418},
year = {2025},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2024.134418},
url = {https://www.sciencedirect.com/science/article/pii/S0167278924003683},
author = {Christian Moya and Amirhossein Mollaali and Zecheng Zhang and Lu Lu and Guang Lin},
keywords = {Operator learning, Uncertainty quantification, Conformal prediction, Quantile prediction, Dynamical systems, Time-dependent PDE},
abstract = {In this paper, we adopt conformal prediction, a distribution-free uncertainty quantification (UQ) framework, to obtain prediction intervals with coverage guarantees for Deep Operator Network (DeepONet) regression. Initially, we enhance the uncertainty quantification frameworks (B-DeepONet and Prob-DeepONet) previously proposed by the authors by using split conformal prediction. By combining conformal prediction with our Prob- and B-DeepONets, we effectively quantify uncertainty by generating rigorous prediction intervals for DeepONet prediction. Additionally, we design a novel Quantile-DeepONet that allows for a more natural use of split conformal prediction. We refer to this distribution-free effective uncertainty quantification framework as split conformal Quantile-DeepONet regression. Finally, we demonstrate the effectiveness of the proposed methods using various ordinary, partial differential equation numerical examples, and multi-fidelity learning.}
}


@Article{Chowell2021Bootstrap,
author={Chowell, Gerardo
and Luo, Ruiyan},
title={Ensemble bootstrap methodology for forecasting dynamic growth processes using differential equations: application to epidemic outbreaks},
journal={BMC Medical Research Methodology},
year={2021},
month={Feb},
day={14},
volume={21},
number={1},
pages={34},
abstract={Ensemble modeling aims to boost the forecasting performance by systematically integrating the predictive accuracy across individual models. Here we introduce a simple-yet-powerful ensemble methodology for forecasting the trajectory of dynamic growth processes that are defined by a system of non-linear differential equations with applications to infectious disease spread.},
issn={1471-2288},
doi={10.1186/s12874-021-01226-9},
url={https://doi.org/10.1186/s12874-021-01226-9}
}

@INPROCEEDINGS{Omer2019PVOutput,
  author={Omer, Zahi M. and Shareef, Hussain},
  booktitle={2019 29th Australasian Universities Power Engineering Conference (AUPEC)}, 
  title={Adaptive Boosting and Bootstrapped Aggregation based Ensemble Machine Learning Methods for Photovoltaic Systems Output Current Prediction}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Mathematical model;Random forests;Boosting;Temperature;Machine learning algorithms;Photovoltaic systems;ensemble machine learning;adaptive boosting;photovoltaics;single diode model;regression decision trees.},
  doi={10.1109/AUPEC48547.2019.211856}}



@Article{Plooy2021financialderivBootstrap,
AUTHOR = {du Plooy, Ryno and Venter, Pierre J.},
TITLE = {A Comparison of Artificial Neural Networks and Bootstrap Aggregating Ensembles in a Modern Financial Derivative Pricing Framework},
JOURNAL = {Journal of Risk and Financial Management},
VOLUME = {14},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {254},
URL = {https://www.mdpi.com/1911-8074/14/6/254},
ISSN = {1911-8074},
ABSTRACT = {In this paper, the pricing performances of two learning networks, namely an artificial neural network and a bootstrap aggregating ensemble network, were compared when pricing the Johannesburg Stock Exchange (JSE) Top 40 European call options in a modern option pricing framework using a constructed implied volatility surface. In addition to this, the numerical accuracy of the better performing network was compared to a Monte Carlo simulation in a separate numerical experiment. It was found that the bootstrap aggregating ensemble network outperformed the artificial neural network and produced price estimates within the error bounds of a Monte Carlo simulation when pricing derivatives in a multi-curve framework setting.},
DOI = {10.3390/jrfm14060254}
}





@article{Linka2022BPINN,
title = {Bayesian Physics Informed Neural Networks for real-world nonlinear dynamical systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {402},
pages = {115346},
year = {2022},
note = {A Special Issue in Honor of the Lifetime Achievements of J. Tinsley Oden},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115346},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522004327},
author = {Kevin Linka and Amelie Schäfer and Xuhui Meng and Zongren Zou and George Em Karniadakis and Ellen Kuhl},
keywords = {Dynamical systems, Machine learning, Neural Networks, Physics Informed Neural Networks, Bayesian Inference, Bayesian Neural Networks},
abstract = {Understanding real-world dynamical phenomena remains a challenging task. Across various scientific disciplines, machine learning has advanced as the go-to technology to analyze nonlinear dynamical systems, identify patterns in big data, and make decision around them. Neural networks are now consistently used as universal function approximators for data with underlying mechanisms that are incompletely understood or exceedingly complex. However, neural networks alone ignore the fundamental laws of physics and often fail to make plausible predictions. Here we integrate data, physics, and uncertainties by combining neural networks, physics informed modeling, and Bayesian inference to improve the predictive potential of traditional neural network models. We embed the physical model of a damped harmonic oscillator into a fully-connected feed-forward neural network to explore a simple and illustrative model system, the outbreak dynamics of COVID-19. Our Physics Informed Neural Networks seamlessly integrate data and physics, robustly solve forward and inverse problems, and perform well for both interpolation and extrapolation, even for a small amount of noisy and incomplete data. At only minor additional cost, they self-adaptively learn the weighting between data and physics. They can serve as priors in a Bayesian Inference, and provide credible intervals for uncertainty quantification. Our study reveals the inherent advantages and disadvantages of Neural Networks, Bayesian Inference, and a combination of both and provides valuable guidelines for model selection. While we have only demonstrated these different approaches for the simple model problem of a seasonal endemic infectious disease, we anticipate that the underlying concepts and trends generalize to more complex disease conditions and, more broadly, to a wide variety of nonlinear dynamical systems. Our source code and examples are available at https://github.com/LivingMatterLab/xPINNs.}
}

@article{Yang_2021,
   title={B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data},
   volume={425},
   ISSN={0021-9991},
   url={http://dx.doi.org/10.1016/j.jcp.2020.109913},
   DOI={10.1016/j.jcp.2020.109913},
   journal={Journal of Computational Physics},
   publisher={Elsevier BV},
   author={Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
   year={2021},
   month=jan, pages={109913} }


@misc{sam2024bayesianneuralnetworksdomain,
      title={Bayesian Neural Networks with Domain Knowledge Priors}, 
      author={Dylan Sam and Rattana Pukdee and Daniel P. Jeong and Yewon Byun and J. Zico Kolter},
      year={2024},
      eprint={2402.13410},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.13410}, 
}

@article{Bonneville2022,
title = {Bayesian deep learning for partial differential equation parameter discovery with sparse and noisy data},
journal = {Journal of Computational Physics: X},
volume = {16},
pages = {100115},
year = {2022},
issn = {2590-0552},
doi = {https://doi.org/10.1016/j.jcpx.2022.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2590055222000117},
author = {Christophe Bonneville and Christopher Earls},
keywords = {Scientific machine learning, SciML, Bayesian inference, Neural network, Partial differential equation, Inverse problems},
abstract = {Scientific machine learning has been successfully applied to inverse problems and PDE discovery in computational physics. One caveat concerning current methods is the need for large amounts of (“clean”) data, in order to characterize the full system response and discover underlying physical models. Bayesian methods may be particularly promising for overcoming these challenges, as they are naturally less sensitive to the negative effects of sparse and noisy data. In this paper, we propose to use Bayesian neural networks (BNN) in order to: 1) Recover the full system states from measurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian Monte-Carlo to sample the posterior distribution of a deep and dense BNN, and show that it is possible to accurately capture physics of varying complexity, without overfitting. 2) Recover the parameters instantiating the underlying partial differential equation (PDE) governing the physical system. Using the trained BNN, as a surrogate of the system response, we generate datasets of derivatives that are potentially comprising the latent PDE governing the observed system and then perform a sequential threshold Bayesian linear regression (STBLR), between the successive derivatives in space and time, to recover the original PDE parameters. We take advantage of the confidence intervals within the BNN outputs, and introduce the spatial derivatives cumulative variance into the STBLR likelihood, to mitigate the influence of highly uncertain derivative data points; thus allowing for more accurate parameter discovery. We demonstrate our approach on a handful of example, in applied physics and non-linear dynamics.}
}

@article{Meng2021MultiFidelityBNN,
title = {Multi-fidelity Bayesian neural networks: Algorithms and applications},
journal = {Journal of Computational Physics},
volume = {438},
pages = {110361},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110361},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121002564},
author = {Xuhui Meng and Hessam Babaee and George Em Karniadakis},
keywords = {Nonlinear correlation, Physics-informed neural networks, Hamiltonian Monte Carlo, Uncertainty quantification, Active learning, Satellite data},
abstract = {We propose a new class of Bayesian neural networks (BNNs) that can be trained using noisy data of variable fidelity, and we apply them to learn function approximations as well as to solve inverse problems based on partial differential equations (PDEs). These multi-fidelity BNNs consist of three neural networks: The first is a fully connected neural network, which is trained following the maximum a posteriori probability (MAP) method to fit the low-fidelity data; the second is a Bayesian neural network employed to capture the cross-correlation with uncertainty quantification between the low- and high-fidelity data; and the last one is the physics-informed neural network, which encodes the physical laws described by PDEs. For the training of the last two neural networks, we first employ the mean-field variational inference (VI) to maximize the evidence lower bound (ELBO) to obtain informative prior distributions for the hyperparameters in the BNNs, and subsequently we use the Hamiltonian Monte Carlo (HMC) method to estimate accurately the posterior distributions for the corresponding hyperparameters. We demonstrate the accuracy of the present method using synthetic data as well as real measurements. Specifically, we first approximate a one- and four-dimensional function, and then infer the reaction rates in one- and two-dimensional diffusion-reaction systems. Moreover, we infer the sea surface temperature (SST) in the Massachusetts and Cape Cod Bays using satellite images and in-situ measurements. Taken together, our results demonstrate that the present method can capture both linear and nonlinear correlation between the low- and high-fidelity data adaptively, identify unknown parameters in PDEs, and quantify uncertainties in predictions, given a few scattered noisy high-fidelity data. Finally, we demonstrate that we can effectively and efficiently reduce the uncertainties and hence enhance the prediction accuracy with an active learning approach, using as examples a specific one-dimensional function approximation and an inverse PDE problem.}
}

@Article{Aikawa2024,
author={Aikawa, Yuri
and Ueda, Naonori
and Tanaka, Toshiyuki},
title={Improving the Efficiency of Training Physics-Informed Neural Networks Using Active Learning},
journal={New Generation Computing},
year={2024},
month={Nov},
day={01},
volume={42},
number={4},
pages={739-760},
abstract={PINN, or physics-informed neural network, is a partial differential equation (PDE) solver realized as a neural network by incorporating the target PDE into the network as physical constraints. In this study, our focus lies in optimizing collocation point selection. We propose an active learning method to enhance the efficiency of PINN learning. The proposed method leverages variational inference based on dropout learning to assess the uncertainty inherent in the solution estimates provided by the PINN. Subsequently, it formulates an acquisition function for active learning grounded in this uncertainty assessment. By employing this acquisition function to probabilistically select collocation points, we can achieve a more expedited convergence to a reasonable solution, as opposed to relying on random sampling. The efficacy of our approach is empirically demonstrated using both Burgers' equation and the convection equation. We also show experimentally that the choice of the collocation points can affect the loss function, the fitting of initial and boundary conditions, and the sensible balance of PDE constraints.},
issn={1882-7055},
doi={10.1007/s00354-024-00253-6},
url={https://doi.org/10.1007/s00354-024-00253-6}
}


@Article{Abouzar2023MCDropout,
AUTHOR = {Choubineh, Abouzar and Chen, Jie and Coenen, Frans and Ma, Fei},
TITLE = {Applying Monte Carlo Dropout to Quantify the Uncertainty of Skip Connection-Based Convolutional Neural Networks Optimized by Big Data},
JOURNAL = {Electronics},
VOLUME = {12},
YEAR = {2023},
NUMBER = {6},
ARTICLE-NUMBER = {1453},
URL = {https://www.mdpi.com/2079-9292/12/6/1453},
ISSN = {2079-9292},
ABSTRACT = {Although Deep Learning (DL) models have been introduced in various fields as effective prediction tools, they often do not care about uncertainty. This can be a barrier to their adoption in real-world applications. The current paper aims to apply and evaluate Monte Carlo (MC) dropout, a computationally efficient approach, to investigate the reliability of several skip connection-based Convolutional Neural Network (CNN) models while keeping their high accuracy. To do so, a high-dimensional regression problem is considered in the context of subterranean fluid flow modeling using 376,250 generated samples. The results demonstrate the effectiveness of MC dropout in terms of reliability with a Standard Deviation (SD) of 0.012–0.174, and of accuracy with a coefficient of determination (R2) of 0.7881–0.9584 and Mean Squared Error (MSE) of 0.0113–0.0508, respectively. The findings of this study may contribute to the distribution of pressure in the development of oil/gas fields.},
DOI = {10.3390/electronics12061453}
}




@misc{folgoc2021mcdropoutbayesian,
      title={Is MC Dropout Bayesian?}, 
      author={Loic Le Folgoc and Vasileios Baltatzis and Sujal Desai and Anand Devaraj and Sam Ellis and Octavio E. Martinez Manzanera and Arjun Nair and Huaqi Qiu and Julia Schnabel and Ben Glocker},
      year={2021},
      eprint={2110.04286},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.04286}, 
}

@Article{Xinyue2025DynamicsUQ,
AUTHOR = {Xu, Xinyue and Wang, Julian},
TITLE = {Comparative Analysis of Physics-Guided Bayesian Neural Networks for Uncertainty Quantification in Dynamic Systems},
JOURNAL = {Forecasting},
VOLUME = {7},
YEAR = {2025},
NUMBER = {1},
ARTICLE-NUMBER = {9},
URL = {https://www.mdpi.com/2571-9394/7/1/9},
ISSN = {2571-9394},
ABSTRACT = {Uncertainty quantification (UQ) is critical for modeling complex dynamic systems, ensuring robustness and interpretability. This study extends Physics-Guided Bayesian Neural Networks (PG-BNNs) to enhance model robustness by integrating physical laws into Bayesian frameworks. Unlike Artificial Neural Networks (ANNs), which provide deterministic predictions, and Bayesian Neural Networks (BNNs), which handle uncertainty probabilistically but struggle with generalization under sparse and noisy data, PG-BNNs incorporate the laws of physics, such as governing equations and boundary conditions, to enforce physical consistency. This physics-guided approach improves generalization across different noise levels while reducing data dependency. The effectiveness of PG-BNNs is validated through a one-degree-of-freedom vibration system with multiple noise levels, serving as a representative case study to compare the performance of Monte Carlo (MC) dropout ANNs, BNNs, and PG-BNNs across interpolation and extrapolation domains. Model accuracy is assessed using Mean Squared Error (MSE), Mean Absolute Percentage Error (MAE), and Coefficient of Variation of Root Mean Square Error (CVRMSE), while UQ is evaluated through 95% Credible Intervals (CIs), Mean Prediction Interval Width (MPIW), the Quality of Confidence Intervals (QCI), and Coverage Width-based Criterion (CWC). Results demonstrate that PG-BNNs can achieve high accuracy and good adherence to physical laws simultaneously, compared to MC dropout ANNs and BNNs, which confirms the potential of PG-BNNs in engineering applications related to dynamic systems.},
DOI = {10.3390/forecast7010009}
}





@article{Halder_2024,
   title={Reduced-order modeling of unsteady fluid flow using neural network ensembles},
   volume={36},
   ISSN={1089-7666},
   url={http://dx.doi.org/10.1063/5.0207978},
   DOI={10.1063/5.0207978},
   number={7},
   journal={Physics of Fluids},
   publisher={AIP Publishing},
   author={Halder, Rakesh and Ataei, Mohammadmehdi and Salehipour, Hesam and Fidkowski, Krzysztof and Maki, Kevin},
   year={2024},
   month=jul }


@article{Pestourie_2023,
   title={Physics-enhanced deep surrogates for partial differential equations},
   volume={5},
   ISSN={2522-5839},
   url={http://dx.doi.org/10.1038/s42256-023-00761-y},
   DOI={10.1038/s42256-023-00761-y},
   number={12},
   journal={Nature Machine Intelligence},
   publisher={Springer Science and Business Media LLC},
   author={Pestourie, Raphaël and Mroueh, Youssef and Rackauckas, Chris and Das, Payel and Johnson, Steven G.},
   year={2023},
   month=dec, pages={1458–1465} }


@article{Zanisi_2024,
doi = {10.1088/1741-4326/ad240d},
url = {https://dx.doi.org/10.1088/1741-4326/ad240d},
year = {2024},
month = {feb},
publisher = {IOP Publishing},
volume = {64},
number = {3},
pages = {036022},
author = {Zanisi, L. and Ho, A. and Barr, J. and Madula, T. and Citrin, J. and Pamela, S. and Buchanan, J. and Casson, F.J. and Gopakumar, V. and JET Contributors},
title = {Efficient training sets for surrogate models of tokamak turbulence with Active Deep Ensembles},
journal = {Nuclear Fusion},
abstract = {Model-based plasma scenario development lies at the heart of the design and operation of future fusion powerplants. Including turbulent transport in integrated models is essential for delivering a successful roadmap towards operation of ITER and the design of DEMO-class devices. Given the highly iterative nature of integrated models, fast machine-learning-based surrogates of turbulent transport are fundamental to fulfil the pressing need for faster simulations opening up pulse design, optimization, and flight simulator applications. A significant bottleneck is the generation of suitably large training datasets covering a large volume in parameter space, which can be prohibitively expensive to obtain for higher fidelity codes. In this work, we propose ADEPT (Active Deep Ensembles for Plasma Turbulence), a physics-informed, two-stage Active Learning strategy to ease this challenge. Active Learning queries a given model by means of an acquisition function that identifies regions where additional data would improve the surrogate model. We provide a benchmark study using available data from the literature for the QuaLiKiz quasilinear transport model. We demonstrate quantitatively that the physics-informed nature of the proposed workflow reduces the need to perform simulations in stable regions of the parameter space, resulting in significantly improved data efficiency compared to non-physics informed approaches which consider a regression problem over the whole domain. We show an up to a factor of 20 reduction in training dataset size needed to achieve the same performance as random sampling. We then validate the surrogates on multichannel integrated modelling of ITG-dominated JET scenarios and demonstrate that they recover the performance of QuaLiKiz to better than 10%. This matches the performance obtained in previous work, but with two orders of magnitude fewer training data points.}
}


@Article{Pestourie2020,
author={Pestourie, Rapha{\"e}l
and Mroueh, Youssef
and Nguyen, Thanh V.
and Das, Payel
and Johnson, Steven G.},
title={Active learning of deep surrogates for PDEs: application to metasurface design},
journal={npj Computational Materials},
year={2020},
month={Oct},
day={29},
volume={6},
number={1},
pages={164},
abstract={Surrogate models for partial differential equations are widely used in the design of metamaterials to rapidly evaluate the behavior of composable components. However, the training cost of accurate surrogates by machine learning can rapidly increase with the number of variables. For photonic-device models, we find that this training becomes especially challenging as design regions grow larger than the optical wavelength. We present an active-learning algorithm that reduces the number of simulations required by more than an order of magnitude for an NN surrogate model of optical-surface components compared to uniform random samples. Results show that the surrogate evaluation is over two orders of magnitude faster than a direct solve, and we demonstrate how this can be exploited to accelerate large-scale engineering optimization.},
issn={2057-3960},
doi={10.1038/s41524-020-00431-2},
url={https://doi.org/10.1038/s41524-020-00431-2}
}



@misc{scher2021ensemblemethodsneuralnetworkbased,
      title={Ensemble methods for neural network-based weather forecasts}, 
      author={Sebastian Scher and Gabriele Messori},
      year={2021},
      eprint={2002.05398},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph},
      url={https://arxiv.org/abs/2002.05398}, 
}


@misc{huang2017snapshotensemblestrain1,
      title={Snapshot Ensembles: Train 1, get M for free}, 
      author={Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger},
      year={2017},
      eprint={1704.00109},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1704.00109}, 
}

@misc{wen2020batchensemblealternativeapproachefficient,
      title={BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning}, 
      author={Yeming Wen and Dustin Tran and Jimmy Ba},
      year={2020},
      eprint={2002.06715},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.06715}, 
}

@misc{gopakumar2025calibratedphysicsinformeduncertaintyquantification,
      title={Calibrated Physics-Informed Uncertainty Quantification}, 
      author={Vignesh Gopakumar and Ander Gray and Lorenzo Zanisi and Timothy Nunn and Stanislas Pamela and Daniel Giles and Matt J. Kusner and Marc Peter Deisenroth},
      year={2025},
      eprint={2502.04406},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.04406}, 
}


@misc{mirman2021fundamentallimitsintervalarithmetic,
      title={The Fundamental Limits of Interval Arithmetic for Neural Networks}, 
      author={Matthew Mirman and Maximilian Baader and Martin Vechev},
      year={2021},
      eprint={2112.05235},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.05235}, 
}

@article{Betancourt2021Interval,
title = {Interval deep learning for computational mechanics problems under input uncertainty},
journal = {Probabilistic Engineering Mechanics},
volume = {70},
pages = {103370},
year = {2022},
issn = {0266-8920},
doi = {https://doi.org/10.1016/j.probengmech.2022.103370},
url = {https://www.sciencedirect.com/science/article/pii/S0266892022001035},
author = {David Betancourt and Rafi L. Muhanna},
keywords = {Deep neural networks, Interval analysis, Machine learning, Interval uncertainty},
abstract = {Solving problems in complex engineering systems often involves employing a cascade of computational models which solve different sub-problems. In computational mechanics in particular, complex systems that use model cascading under uncertainty include important safety-critical applications (e.g., aerospace and nuclear reactor systems). There are two fundamental considerations to solve problems with cascading models. First, the collaborating models involved in solving the problem must share compatible data formats and modeling assumptions. Second, the models must be capable of processing and propagating the uncertainty on the raw measurements and the uncertainty on the predictions from other models. The first consideration is often well-understood and implemented in practice. On the other hand, the second consideration, despite its importance, is less understood and implemented. We argue that in such complex engineering systems it is necessary to employ models capable of processing data uncertainty to obtain reliable predictions for the problem. One main difficulty in uncertainty quantification is that there is often not enough information about the data to make probabilistic assumptions. In fact, sometimes the only information available to the analyst is in the form of upper and lower bounds of the data. In such cases, interval analysis (IA) offers an alternative to quantify the uncertainty. Another difficulty is that we generally do not have direct access to the value of an uncertain mechanical quantity but only to indirect measurements or models that are used to represent it [1]. Indeed, the data uncertainty that flows through the model cascade can arise from measurements or from predictions of other models. To solve these problems, we present a novel interval deep neural network (DINN) that is capable of providing reliable input data to a mechanics model from data containing uncertainty. A numerical experiment is conducted using a dataset of concrete mix measurements with interval uncertainty to predict concrete strength. In general, the DINN can be used to solve multiple problems under uncertainty in science and engineering.}
}

@ARTICLE{Ronay2015IntervalWind,
  author={Ak, Ronay and Vitelli, Valeria and Zio, Enrico},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={An Interval-Valued Neural Network Approach for Uncertainty Quantification in Short-Term Wind Speed Prediction}, 
  year={2015},
  volume={26},
  number={11},
  pages={2787-2800},
  keywords={Uncertainty;Artificial neural networks;Training;Wind speed;Sociology;Statistics;Predictive models;Interval-valued neural networks (NNs);multi-objective genetic algorithm (MOGA);prediction intervals (PIs);short-term wind speed forecasting;uncertainty.;Interval-valued neural networks (NNs);multi-objective genetic algorithm (MOGA);prediction intervals (PIs);short-term wind speed forecasting;uncertainty},
  doi={10.1109/TNNLS.2015.2396933}}


@article{Tretiak2023IntervalRegression,
title = {Neural network model for imprecise regression with interval dependent variables},
journal = {Neural Networks},
volume = {161},
pages = {550-564},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000680},
author = {Krasymyr Tretiak and Georg Schollmeyer and Scott Ferson},
keywords = {Imprecise regression, Interval data, Neural network, Uncertainty},
abstract = {This paper presents a computationally feasible method to compute rigorous bounds on the interval-generalization of regression analysis to account for epistemic uncertainty in the output variables. The new iterative method uses machine learning algorithms to fit an imprecise regression model to data that consist of intervals rather than point values. The method is based on a single-layer interval neural network which can be trained to produce an interval prediction. It seeks parameters for the optimal model that minimizes the mean squared error between the actual and predicted interval values of the dependent variable using a first-order gradient-based optimization and interval analysis computations to model the measurement imprecision of the data. An additional extension to a multi-layer neural network is also presented. We consider the explanatory variables to be precise point values, but the measured dependent values are characterized by interval bounds without any probabilistic information. The proposed iterative method estimates the lower and upper bounds of the expectation region, which is an envelope of all possible precise regression lines obtained by ordinary regression analysis based on any configuration of real-valued points from the respective y-intervals and their x-values.}
}










@article{Chetwynd2006IntervalRegression,
author = {Chetwynd, D  and Worden, K  and Manson, G },
title = {An application of interval-valued neural networks to a regression problem},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {462},
number = {2074},
pages = {3097-3114},
year = {2006},
doi = {10.1098/rspa.2006.1717},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.2006.1717},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.2006.1717}
,
    abstract = { This paper is concerned with exploiting uncertainty in order to develop a robust regression algorithm for a pre-sliding friction process based on a Nonlinear Auto-Regressive with eXogenous inputs neural network. Essentially, it is shown that using an interval-valued neural network allows a trade-off between the model error and the interval width of the network weights or a ‘degree of uncertainty’ parameter. The neural network weights are replaced by interval variables and cannot therefore be derived from a conventional optimization algorithm; in this case, the problem is solved by using differential evolution. The paper also shows how to implement the idea of ‘opportunity’ as used in Ben-Haim's information-gap theory. }
}


@article{Schilling_2022,
   title={Verification of Neural-Network Control Systems by Integrating Taylor Models and Zonotopes},
   volume={36},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v36i7.20790},
   DOI={10.1609/aaai.v36i7.20790},
   number={7},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Schilling, Christian and Forets, Marcelo and Guadalupe, Sebastián},
   year={2022},
   month=jun, pages={8169–8177} }

@misc{streeter2023automaticallyboundingtaylorremainder,
      title={Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications}, 
      author={Matthew Streeter and Joshua V. Dillon},
      year={2023},
      eprint={2212.11429},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.11429}, 
}

@misc{lemesle2024neuralnetworkverificationpyrat,
      title={Neural Network Verification with PyRAT}, 
      author={Augustin Lemesle and Julien Lehmann and Tristan Le Gall},
      year={2024},
      eprint={2410.23903},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.23903}, 
}

@misc{jordan2022zonotopedomainslagrangianneural,
      title={Zonotope Domains for Lagrangian Neural Network Verification}, 
      author={Matt Jordan and Jonathan Hayase and Alexandros G. Dimakis and Sewoong Oh},
      year={2022},
      eprint={2210.08069},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.08069}, 
}

@inproceedings{Ladner2024polynomialzonotope,
author = {Ladner, Tobias and Althoff, Matthias},
title = {Exponent relaxation of polynomial zonotopes and its applications in formal neural network verification},
year = {2024},
isbn = {978-1-57735-887-9},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v38i19.30125},
doi = {10.1609/aaai.v38i19.30125},
abstract = {Formal verification of neural networks is a challenging problem due to the complexity and nonlinearity of neural networks. It has been shown that polynomial zonotopes can tightly enclose the output set of a neural network. Unfortunately, the tight enclosure comes with additional complexity in the set representation, thus, rendering subsequent operations expensive to compute, such as computing interval bounds and intersection checking. To address this issue, we present a novel approach to restructure a polynomial zonotope to tightly enclose the original polynomial zonotope while drastically reducing its complexity. The restructuring is achieved by relaxing the exponents of the dependent factors of polynomial zonotopes and finding an appropriate approximation error. We demonstrate the applicability of our approach on output sets of neural networks, where we obtain tighter results in various subsequent operations, such as order reduction, zonotope enclosure, and range bounding.},
booktitle = {Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {2377},
numpages = {8},
series = {AAAI'24/IAAI'24/EAAI'24}
}

@ARTICLE{FFT,
  author={Brigham, E. O. and Morrow, R. E.},
  journal={IEEE Spectrum}, 
  title={The fast Fourier transform}, 
  year={1967},
  volume={4},
  number={12},
  pages={63-70},
  doi={10.1109/MSPEC.1967.5217220}}



@article{srivastava2015highway,
      title={Highway Networks}, 
      author={Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
      year={2015},
      eprint={1505.00387},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal = {arxiv preprint arxiv:1505.00387}
}


@article{dalal2019autoregressive,
      title={Autoregressive Models: What Are They Good For?}, 
      author={Murtaza Dalal and Alexander C. Li and Rohan Taori},
      year={2019},
      eprint={1910.07737},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal = {arxiv preprint arxiv:1910.07737}
}


@article{zhao2023incremental,
      title={Incremental Spectral Learning in Fourier Neural Operator}, 
      author={Jiawei Zhao and Robert Joseph George and Zongyi Li and Anima Anandkumar},
      year={2023},
      eprint={2211.15188},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      journal = {arxiv preprint arxiv:2211.15188}
}


@article{EASY_2016,
title = {Investigation of the effect of resistivity on scrape off layer filaments using three-dimensional simulations},
journal = {Physics of Plasmas},
volume = {23},
pages = {012512},
year = {2016},
doi = {https://doi.org/10.1063/1.4940330},
url = {https://pubs.aip.org/aip/pop/article-abstract/23/1/012512/108420},
author = {L.Easy and F.Militello and J.Omotani and N.R.Walkden and B.Dudson}
}


@article{MILITELLO_2017,
title = {On the interaction of scrape off layer filaments},
journal = {Physics of Plasmas},
volume = {59},
pages = {125013},
year = {2017},
doi = {https://doi.org/10.1088/1361-6587/aa9252},
url = {https://iopscience.iop.org/article/10.1088/1361-6587/aa9252},
author = {F.Militello and B.Dudson and L.Easy and A.Kirk and P.Naylor}
}



@misc{carey2024dataefficiencylongterm,
      title={Data efficiency and long term prediction capabilities for neural operator surrogate models of core and edge plasma codes}, 
      author={N. Carey and L. Zanisi and S. Pamela and V. Gopakumar and J. Omotani and J. Buchanan and J. Brandstetter},
      year={2024},
      eprint={2402.08561},
      archivePrefix={arXiv},
      primaryClass={physics.plasm-ph},
      url={https://arxiv.org/abs/2402.08561}, 
}


@ARTICLE{Walkden2016-ys,
  title     = "Dynamics of {3D} isolated thermal filaments",
  author    = "Walkden, N R and Easy, L and Militello, F and Omotani, J T",
  abstract  = "Simulations have been carried out to establish how electron
               thermal physics, introduced in the form of a dynamic electron
               temperature, affects isolated filament motion and dynamics in
               3D. It is found that thermal effects impact filament motion in
               two major ways when the pressure perturbation within the
               filament is supported primarily through a temperature increase
               as opposed to density: they lead to a strong increase in
               filament propagation in the bi-normal direction and a
               significant decrease in net radial propagation. Both effects
               arise from the temperature dependence of the sheath current
               which leads to a non-uniform floating potential, with the latter
               effect supplemented by faster pressure loss. The reduction in
               radial velocity can only occur when the filament cross-section
               loses angular symmetry. The behaviour is observed across
               different filament sizes and suggests that filaments with much
               larger temperature perturbations than density perturbations are
               more strongly confined to the near SOL region.",
  journal   = "Plasma Phys. Control. Fusion",
  publisher = "IOP Publishing",
  volume    =  58,
  number    =  11,
  pages     = "115010",
  month     =  nov,
  year      =  2016,
}


@article{Smith_2020,
	doi = {10.1088/1741-4326/ab826a},
	url = {https://doi.org/10.1088/1741-4326/ab826a},
	year = 2020,
	month = {may},
	publisher = {{IOP} Publishing},
	volume = {60},
	number = {6},
	pages = {066021},
	author = {S.F. Smith and S.J.P. Pamela and A. Fil and M. Hölzl and G.T.A. Huijsmans and A. Kirk and D. Moulton and O. Myatra and A.J. Thornton and H.R. Wilson and},
	title = {Simulations of edge localised mode instabilities in {MAST}-U Super-X tokamak plasmas},
	journal = {Nuclear Fusion},
	abstract = {The high heat fluxes to the divertor during edge localised mode (ELM) instabilities have to be reduced for a sustainable future tokamak reactor. A solution to reduce the heat fluxes could be the Super-X divertor, which will be tested on MAST-U. ELM simulations for MAST-U Super-X tokamak plasmas have been obtained, using JOREK. A factor 10 decrease in the peak heat flux to the outer target and almost a factor 8 decrease in the ELM energy fluence when comparing the Super-X to a conventional divertor configuration has been found. A detached MAST-U case, after the roll-over in the target parallel electron density flux, is used as a starting point for ELM burn-through simulations. The plasma burns through the neutrals front during the ELM causing the divertor plasma to re-attach. After the crash a transition back to detachment is indicated, where the recovery to pre-ELM divertor conditions occurs in a few milliseconds, when the neutral pressure is high in the divertor. Recovery times are shorter than the inter-ELM phase in previous MAST experiments. The peak ELM energy fluence obtained after the ELM burn-through is 0.82 kJ/m2, which is significantly lower than that predicted from the empirical scaling of the ELM energy fluence - indicating promising results for future MAST-U operations.}
}

@article{McArdle_MAST_2010,
	journal = {Review of Scientific Instruments},
	doi = {10.1063/1.3499219},
	url = {https://doi.org/10.1063/1.3499219},
	year = 2010,
	month = {may},
	publisher = {{IOP} Publishing},
	volume = {81},
	pages = {113504},
	author = {G. Hommen and M. de Baar and P. Nuij and G. McArdle and R. Akers and M. Steinbuch},
	title = {Optical boundary reconstruction of tokamak plasmas for feedback control of plasma position and shape},
}

@misc{pamela_2024_11099659,
  author       = {Pamela, Stanislas},
  title        = {Neural-Parareal dataset JOREK blob runs with
                   electrostatic model batch\_0001-0500
                  },
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.11099659},
  url          = {https://doi.org/10.5281/zenodo.11099659},
}

@misc{pamela_2024_11099671,
  author       = {Pamela, Stanislas},
  title        = {Neural-Parareal dataset JOREK blob runs with
                   electrostatic model batch\_0501-1000
                  },
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.11099671},
  url          = {https://doi.org/10.5281/zenodo.11099671},
}

@misc{pamela_2024_11099679,
  author       = {Pamela, Stanislas},
  title        = {Neural-Parareal dataset JOREK blob runs with
                   electrostatic model batch\_1001-1500
                  },
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.11099679},
  url          = {https://doi.org/10.5281/zenodo.11099679},
}

@misc{pamela_2024_11099685,
  author       = {Pamela, Stanislas},
  title        = {Neural-Parareal dataset JOREK blob runs with
                   electrostatic model batch\_1501-2000
                  },
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.11099685},
  url          = {https://doi.org/10.5281/zenodo.11099685},
}


@article{mdp,
 ISSN = {00959057, 19435274},
 URL = {http://www.jstor.org/stable/24900506},
 author = {Richard Bellman},
 journal = {Journal of Mathematics and Mechanics},
 number = {5},
 pages = {679--684},
 publisher = {Indiana University Mathematics Department},
 title = {A Markovian Decision Process},
 urldate = {2023-04-26},
 volume = {6},
 year = {1957}
}

@article{lions2001Parareal,
  TITLE = {{R{\'e}solution d'EDP par un sch{\'e}ma en temps < parar{\'e}el >}},
  AUTHOR = {Lions, Jacques-Louis and Maday, Yvon and Turinici, Gabriel},
  URL = {https://hal.science/hal-00798372},
  JOURNAL = {{Comptes rendus de l'Acad{\'e}mie des sciences. S{\'e}rie I, Math{\'e}matique}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {332},
  NUMBER = {7},
  PAGES = {661-668},
  YEAR = {2001},
  PDF = {https://hal.science/hal-00798372v1/file/CRAS_01_lions_maday_turinici.pdf},
  HAL_ID = {hal-00798372},
  HAL_VERSION = {v1},
}

@inproceedings{Oskarsson2024GraphFM,
 author = {Oskarsson, Joel and Landelius, Tomas and Deisenroth, Marc Peter and Lindsten, Fredrik},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {41577--41648},
 publisher = {Curran Associates, Inc.},
 title = {Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/492592890311679d7f71559148358973-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@misc{gopakumar2024validerrorbarsneural,
      title={Valid Error Bars for Neural Weather Models using Conformal Prediction}, 
      author={Vignesh Gopakumar and Joel Oskarrson and Ander Gray and Lorenzo Zanisi and Stanislas Pamela and Daniel Giles and Matt Kusner and Marc Deisenroth},
      year={2024},
      eprint={2406.14483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.14483}, 
}

@book{arfken1985methods,
  title={Mathematical Methods for Physicists},
  author={Arfken, George},
  year={1985},
  edition={3},
  publisher={Academic Press},
  address={Orlando, FL},
  pages={810--814},
  chapter={15},
  section={5},
  note={Convolution Theorem}
}


@misc{deangelis2022exactboundsamplitudephase,
      title={Exact bounds on the amplitude and phase of the interval discrete Fourier transform in polynomial time}, 
      author={Marco de Angelis},
      year={2022},
      eprint={2205.13978},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/2205.13978}, 
}

@article{Behrendt2022SetFFT,
title = {Projecting interval uncertainty through the discrete Fourier transform: An application to time signals with poor precision},
journal = {Mechanical Systems and Signal Processing},
volume = {172},
pages = {108920},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2022.108920},
url = {https://www.sciencedirect.com/science/article/pii/S0888327022001054},
author = {Marco Behrendt and Marco {de Angelis} and Liam Comerford and Yuanjin Zhang and Michael Beer},
keywords = {Discrete Fourier transform, Complex intervals, Dependency tracking, Interval arithmetic, Power spectral density estimation, Uncertainty quantification},
abstract = {The discrete Fourier transform (DFT) is often used to decompose a signal into a finite number of harmonic components. The efficient and rigorous propagation of the error present in a signal through the transform can be computationally challenging. Real data is always subject to imprecision because of measurement uncertainty. For example, such uncertainty may come from sensors whose precision is affected by degradation, or simply from digitisation. On many occasions, only error bounds on the signal may be known, thus it may be necessary to automatically propagate the error bounds without making additional artificial assumptions. This paper presents a method that can automatically propagate interval uncertainty through the DFT while yielding the exact bounds on the Fourier amplitude and on an estimation of the Power Spectral Density (PSD) function. The method allows technical analysts to project interval uncertainty – present in the time signals – to the Fourier amplitude and PSD function without making assumptions about the dependence and the distribution of the error over the time steps. Thus, it is possible to calculate and analyse system responses in the frequency domain without conducting extensive Monte Carlo simulations nor running expensive optimisations in the time domain. The applicability of this method in practice is demonstrated by a technical application. It is also shown that conventional Monte Carlo methods severely underestimate the uncertainty.}
}

@misc{chen2019neuralordinarydifferentialequations,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1806.07366}, 
}

@book{kreyszig1972advanced,
  title={Advanced Engineering Mathematics},
  author={Kreyszig, Erwin},
  edition={3},
  year={1972},
  publisher={Wiley},
  address={New York},
  isbn={0-471-50728-8}
}

@book{smith1985numerical,
  title={Numerical Solution of Partial Differential Equations: Finite Difference Methods},
  author={Smith, G. D.},
  edition={3},
  year={1985},
  publisher={Oxford University Press},
  address={Oxford}
}


@book{versteeg1995introduction,
  title={An Introduction to Computational Fluid Dynamics: The Finite Volume Method},
  author={Versteeg, H. K. and Malalasekera, W.},
  year={1995},
  publisher={Addison-Wesley},
  address={Reading, MA}
}


@book{reddy2006introduction,
  title={Introduction to the Finite Element Method},
  author={Reddy, J. N.},
  edition={3},
  year={2006},
  publisher={McGraw-Hill Education},
  address={New York},
  url={https://www.accessengineeringlibrary.com/content/book/9780072466850},
  isbn={9780072466850}
}


@book{fornberg1996practical,
  title={A Practical Guide to Pseudospectral Methods},
  author={Fornberg, Bengt},
  year={1996},
  publisher={Cambridge University Press},
  address={Cambridge, UK}
}

@misc{bodnar2024foundationmodelearth,
      title={A Foundation Model for the Earth System}, 
      author={Cristian Bodnar and Wessel P. Bruinsma and Ana Lucic and Megan Stanley and Anna Vaughan and Johannes Brandstetter and Patrick Garvan and Maik Riechert and Jonathan A. Weyn and Haiyu Dong and Jayesh K. Gupta and Kit Thambiratnam and Alexander T. Archibald and Chun-Chieh Wu and Elizabeth Heider and Max Welling and Richard E. Turner and Paris Perdikaris},
      year={2024},
      eprint={2405.13063},
      archivePrefix={arXiv},
      primaryClass={physics.ao-ph},
      url={https://arxiv.org/abs/2405.13063}, 
}