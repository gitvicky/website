[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Gopakumar",
    "section": "",
    "text": "I am a senior research scientist at the UK Atomic Energy Authority (UKAEA), where I lead a team developing “actionable” surrogate models for exascale simulations and data-driven models for the Fusion industry. My research focusses on enhancing machine learning models’ performance, robustness and interpretability through physics-based approaches.\nI am also a visiting researcher with the SciML group at STFC’s Rutherford Appleton Laboratory.\nConcurrently, I’m pursuing a PhD in Machine Learning under Marc Deisenroth in the Sustainability and Machine Learning Group at University College London.\n\n\nCurrent Research:\n\nPhysics-Informed Machine Learning\nContinuous Dynamics Modelling\nUncertainty Quantification\nDesign of Experiments\n\nBuilding simvue.io : AI-driven, open-source simulation management and tracking dashboard for streamlining engineering workflows. Developed with public funding from the UK Government. Currently in private \\(\\beta\\). \n\n“If there is a God, it must be a differential equation” - Bertrand Russell"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Calibrated Physics-Informed Uncertainty Quantification - Workshop on Calibrating Uncertainties, Isaac Newton Institute for Mathematical Sciences (Cambridge 2025) [Video] [Slides]\nOn Surrogate Modelling for Fusion - PhysicsX (London 2024) [Slides]\nFNO for Plasma Modelling - IAEA Workshop on AI for Accelerating Fusion and Plasma Science (Vienna 2023) [Slides]\nFNO for Plasma Modelling - IAEA Fusion Energy Conference (London, 2023) [Slides]\nFNO for Plasma Modelling - AI for sustainability workshop @ UCL (London, 2023)\nFourier RNNs for modelling noisy physics data - IEEE ICMLA (Bahamas, 2022) [Slides]\nInformed Sampling of the Plasma Hyperspace for Digital Twinning - IAEA Fusion Data Processing, Validation, Analysis (Chengdu, 2021) [Slides]\nOptimising Physics Informed Neural Networks - PyTorch Ecosystem Day (Virtual, 2021)[Poster]\nFluid Surrogates using Neural PDEs - SciML at RAL, STFC (Oxford, 2020) [Slides]\nSolving Fluid Dynamics with Neural Networks - FusionEP (Virtual, 2020) [Video] [Slides]\nData Driven Modelling of Plasma in Tokamaks - IOP Physics in the spotlight (London, 2019) [Slides]\nData Driven Modelling and Control of Plasma in Fusion Reactors - O’Reilly AI London (London, 2019) [Video]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Neural_ODE.html",
    "href": "Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#markdown",
    "href": "Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#code-cell",
    "href": "Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "Neural_ODE.html#equation",
    "href": "Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html",
    "href": "blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019)\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "blogposts/Neural_ODE.html#markdown",
    "href": "blogposts/Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#code-cell",
    "href": "blogposts/Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#equation",
    "href": "blogposts/Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#original-paper",
    "href": "blogposts/Neural_ODE.html#original-paper",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Blogposts/Neural_ODE.html",
    "href": "Blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019),\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_ODE.html",
    "href": "Blog/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_Operators.html",
    "href": "Blog/Neural_Operators.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nCalibrated Physics-Informed Uncertainty Quantification\nVignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timorthy Nunn,\nDaniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth\nICML, 2025\nTLDR: Calibrated uncertainty quantification of neural PDE solvers using physics residual errors as non-conformity scores for data-free conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nUncertainty Quantification of Surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi,\nDaniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Blog/geometry.html",
    "href": "Blog/geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. (Li et al. 2023), (Li et al. 2020)\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF.\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions.\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that klatent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nThese seems to be adjacent to the kind of structure that we are looking at with a division of power and responsibilities, with specific networks and models learning differnet parts of the task.\n\n\n\n\nReferences\n\nLi, Zongyi, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, et al. 2023. “Geometry-Informed Neural Operator for Large-Scale 3D PDEs.” In Thirty-Seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=86dXbqT5Ua.\n\n\nLi, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. “Multipole Graph Neural Operator for Parametric Partial Differential Equations.” In Advances in Neural Information Processing Systems, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6755–66. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
  },
  {
    "objectID": "Blog/Geometry.html",
    "href": "Blog/Geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. [1–4]\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF [5–7].\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions. [8]\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture as shown below.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; B\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that latent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nUsing a structured NO such as the FNO, that has inductive bias, quick to evaluate has found to be rather beneficial, but the open questions still lie within the choice of the encoder-decoder. Papers such as [1, 3] utilise graph neural networks as the encoder-decoder, but however they have significant computational and memory requirements.\nI am currently exploring the idea of maybe using coordinate-based MLPs for the encoder and a GP as the decoder, with a neural operator deployed as a neural ODE with operator-splitting as the processor (neural-UDE):\n\n\n\n\n\nflowchart LR\n  A(NeRF) --&gt; B(Neural-UDE)\n  B --&gt; B\n  B --&gt; C(GP)\n\n\n\n\n\n\nThe challenge with the NeRF is that though allow for continuous space representations, they are terrible at enforcing strict boundaries within the domain and have soft gradients across them. These leads to losing strutcures with significant gradients being lost in the initial condition and geometry. The advantages could be that the IC might not have sharp enough gradients so representative capacity might not be that much of a concern. The other advantage is that they are small, light models, based on simple MLPs.\nAs for the GP, the challenges remain the same as always, whether they can be scaled to handle the dimension and size as we might need. Might have to train multiple GPs or Mixture of GPs? The advantage is that we can get UQ built into the models.\nThis approach brings in a division of labour, having models learn a specific task and then connected together in an approach similar to integrated modelling. Is this a kind of Mixture of Experts ? \n\nExploring Kernel Methods\n\nConsverative Remapping\n\n\nNon-Uniform FFT\nFFT over non-uniform grids (nuFFT) as demonstrated within this pytorch repository, where the grids are structured using a Kaisser-Bessel window functions as interpolation kernels. Essentially we are using a weighted kernel approach to move from unstructured grids to structured grids.\n\n\n\n\n\n\nReferences\n\n1. Li Z, Kovachki NB, Choy C, et al (2023) Geometry-informed neural operator for large-scale 3D PDEs. In: Thirty-seventh conference on neural information processing systems\n\n\n2. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Multipole graph neural operator for parametric partial differential equations. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 6755–6766\n\n\n3. Brandstetter J, Worrall DE, Welling M (2022) Message passing neural PDE solvers. In: International conference on learning representations\n\n\n4. Li T, Zou S, Chang X, Zhang L, Deng X (2024) Predicting unsteady incompressible fluid dynamics with finite volume informed neural network. Physics of Fluids 36(4). https://doi.org/10.1063/5.0197425\n\n\n5. Sitzmann V, Martel J, Bergman A, Lindell D, Wetzstein G (2020) Implicit neural representations with periodic activation functions. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 7462–7473\n\n\n6. Serrano L, Boudec LL, Koupaı̈ AK, et al (2023) Operator learning with neural fields: Tackling PDEs on general geometries. In: Thirty-seventh conference on neural information processing systems\n\n\n7. Yin Y, Kirchmeyer M, Franceschi J-Y, Rakotomamonjy A, gallinari patrick (2023) Continuous PDE dynamics forecasting with implicit neural representations. In: The eleventh international conference on learning representations\n\n\n8. Chen Y, Hosseini B, Owhadi H, Stuart AM (2021) Solving and learning nonlinear PDEs with gaussian processes. Journal of Computational Physics 447:110668. https://doi.org/https://doi.org/10.1016/j.jcp.2021.110668"
  },
  {
    "objectID": "Personal/personal.html",
    "href": "Personal/personal.html",
    "title": "Personal Blog",
    "section": "",
    "text": "The best and the worst"
  },
  {
    "objectID": "Personal/best_and_worst.html",
    "href": "Personal/best_and_worst.html",
    "title": "The Best and the Worst",
    "section": "",
    "text": "There’s a big conundrum where I am torn between feeling that the best is yet to come and that it’s already over."
  },
  {
    "objectID": "Blog/UPE.html",
    "href": "Blog/UPE.html",
    "title": "Universal Physics Engine",
    "section": "",
    "text": "Can AI serve as a Universal Physics Engine ?\n\n\nIn Stephen Wolfram’s (one of the legends in the field of computational physics and mathematics) latest writing, he explores ideas on the fields and methods with which he thinks AI will impact scientific disciplines. He starts the blog with:\n“To the ultimate question of whether AI can solve Science, we’re going to see that the answer is inevitably and firmly no.”\nSo probably there is not much of a reason for me to explore the idea of building a universal physics engine using AI, but hey I love attempting the impossible (why do you think I ended up doing Fusion !!!).\nScience can be broadly defined to fall under three categories:\n\n\n\n\n\nflowchart LR\n  A(Prediction)\n  B(Discovery)\n  C(Explanation)\n\n\n\n\n\n\nNeed to a background on the different kind of approaches being taken:\n\nSurrogate Models of all kinds.\nFoundation Models for Physics\nGenerative Enginering - PhysicsX, Zoo.dev and the varioud likes of those. Generative models for statistical physics.\n\nMath proofing approaches\nText-Code is all you need with the right software - Bring in the Karpathy tweet.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nCurrent Limitations\nPotential AI Capabilities\nNovel Solutions\nChallenges\n\n\n\n\nMathematical Framework\nLimited to specific classes of PDEs; Separate frameworks for different physics domains\nUnified mathematical framework spanning quantum to classical physics; Automatic detection of symmetries and conservation laws; Dynamic generation of problem-specific basis functions\nDevelopment of new mathematical structures beyond tensors and operators; Creation of hybrid symbolic-numerical methods; Discovery of new transformations between problem domains\nEnsuring mathematical consistency across scales; Proving convergence for new methods; Handling mathematical singularities\n\n\nGeometric Processing\nPre-defined mesh types; Manual domain decomposition; Limited handling of complex boundaries\nAutomated optimal mesh generation for arbitrary geometries; Intelligent boundary condition handling; Adaptive multi-resolution techniques\nSelf-designing coordinate systems; Topology-aware discretization; Geometry-informed basis functions\nDealing with moving boundaries; Handling topological changes; Ensuring mesh quality\n\n\nMulti-physics Coupling\nManual coupling between different physics models; Limited cross-scale interactions\nAutomated detection of relevant physics; Seamless coupling across scales; Self-adaptive model selection\nCreation of unified multi-physics formulations; Development of scale-bridging operators; Automatic derivation of reduced-order models\nMaintaining conservation properties; Handling disparate time scales; Managing computational complexity\n\n\nError Control & Stability\nFixed error estimators; Predefined stability criteria; Manual parameter tuning\nReal-time error prediction; Adaptive stability preservation; Automated parameter optimization\nDevelopment of new error metrics; Creation of self-stabilizing schemes; Learning-based error estimation\nGuaranteeing global stability; Balancing accuracy vs. efficiency; Handling chaos and sensitivity\n\n\nComputational Methods\nFixed numerical schemes; Limited parallelization; Domain-specific optimizations\nDynamic algorithm selection; Automated parallelization strategies; Problem-specific method synthesis\nCreation of new numerical algorithms; Development of quantum-inspired methods; Adaptive hybrid schemes\nScaling to large problems; Managing memory hierarchy; Ensuring reproducibility\n\n\nUser Interaction\nLimited feedback on solution quality; Fixed visualization options; Preset parameter ranges\nInteractive problem refinement; Adaptive visualization; Automated parameter exploration\nDevelopment of intuitive interfaces; Creation of explanation systems; Generation of physical insights\nCommunicating complex concepts; Handling ambiguous specifications; Providing meaningful feedback\n\n\nPhysical Consistency\nManual enforcement of conservation laws; Fixed constitutive relations; Predefined material models\nAutomatic constraint preservation; Learning-based constitutive relations; Adaptive material modeling\nDiscovery of new conservation principles; Creation of physics-informed neural operators; Development of universal material models\nEnsuring physical realizability; Handling unknown physics; Maintaining causality\n\n\nData Integration\nLimited use of experimental data; Fixed model parameters; Separate calibration steps\nReal-time data assimilation; Automated model calibration; Dynamic parameter updating\nDevelopment of physics-data hybrid methods; Creation of adaptive measurement operators; Automated experiment design\nHandling noisy data; Dealing with sparse measurements; Ensuring model validity"
  },
  {
    "objectID": "Slides/slides.html",
    "href": "Slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Plasma Surrogate Modelling using FNO - IAEA Workshop on AI in Fusion (December 2023)"
  },
  {
    "objectID": "Personal/visual_art_portfoilo.html",
    "href": "Personal/visual_art_portfoilo.html",
    "title": "Visual Art",
    "section": "",
    "text": "Kovalam December 2024"
  },
  {
    "objectID": "Blog/blog.html",
    "href": "Blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Iterative Solvers for Linear Systems\nConvolution as a form of Integration\nGeometry for Neural PDEs\nUniversal Physics Engine\nA review on UQ and Surrogate Modelling for Neural PDE solvers\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "href": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "title": "Integration by way of Convolution",
    "section": "The Mathematical Foundation of Regularized Inversion",
    "text": "The Mathematical Foundation of Regularized Inversion\nWhen we have a signal \\(y\\) that results from the convolution of an unknown signal \\(f\\) with a known kernel \\(g\\):\n\\[y = f * g\\]\nIn the frequency domain (using the Fourier transform), this becomes:\n\\[Y(\\omega) = F(\\omega) \\cdot G(\\omega)\\]\nWhere \\(Y\\), \\(F\\), and \\(G\\) are the Fourier transforms of \\(y\\), \\(f\\), and \\(g\\) respectively.\nIdeally, we could recover \\(f\\) by:\n\\[F(\\omega) = \\frac{Y(\\omega)}{G(\\omega)}\\]\nAnd then applying the inverse Fourier transform to get \\(f\\) in the time domain.\n\nThe Problem of Ill-Conditioning\nThe difficulty arises when \\(G(\\omega)\\) approaches zero at certain frequencies. This occurs with many important kernels, including our [1, -2, 1] second derivative kernel.\nFor the second derivative kernel, the frequency response is approximately:\n\\[G(\\omega) \\approx -\\omega^2\\]\nThis means \\(G(\\omega)\\) is very small near \\(\\omega = 0\\) (the DC component and low frequencies). Division by these small values causes numerical instability, amplifying noise and errors.\n\n\nTikhonov Regularization\nWhat we’re doing with epsilon is a form of Tikhonov regularization, which can be mathematically represented as:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{G(\\omega) + \\epsilon}\\]\nThis is equivalent to finding the solution to the minimization problem:\n\\[\\min_f \\|g * f - y\\|^2 + \\epsilon \\|f\\|^2\\]\nWhere the first term measures how well our recovered signal explains the observed data, and the second term penalizes large values in the solution, providing stability.\n\n\nMathematical Properties of the Regularization\nTo understand what epsilon does mathematically, let’s analyze its effect at different frequencies:\n\nWhere \\(|G(\\omega)| \\gg \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{G(\\omega)} \\approx F(\\omega)\\] The recovery is accurate at frequencies where the kernel has significant response.\nWhere \\(|G(\\omega)| \\ll \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{\\epsilon} \\approx 0\\] The recovery suppresses components at frequencies where the kernel has near-zero response.\nWhere \\(|G(\\omega)| \\approx \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{2G(\\omega)} \\approx \\frac{F(\\omega)}{2}\\] The recovery partially retrieves information, with some attenuation.\n\nThis creates a smooth transition between fully recovered frequencies and suppressed frequencies, avoiding the sharp discontinuities that would cause ringing artifacts."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "href": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "title": "Integration by way of Convolution",
    "section": "Alternative Approaches to Signal Recovery",
    "text": "Alternative Approaches to Signal Recovery\nThere are several alternative approaches for recovering a signal after convolution, especially for the case of integration following differentiation:\n\n1. Direct Integration (for Differential Kernels)\nSince our [1, -2, 1] kernel approximates the second derivative, integration is a natural inverse operation. We can recover an approximation to the original signal by integrating twice:\n\\[\\hat{f}(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nWhere \\(C_1\\) and \\(C_2\\) are integration constants that need to be determined from boundary conditions or additional information.\nFor discrete signals, this becomes cumulative summation:\ndef double_integrate(signal):\n    # First integration (cumulative sum)\n    first_integral = np.cumsum(signal)\n    # Second integration\n    second_integral = np.cumsum(first_integral)\n    return second_integral\nThe challenge is determining the correct integration constants, which represent the linear and constant components lost during differentiation.\n\n\n2. Wiener Deconvolution\nWiener deconvolution incorporates knowledge about the signal-to-noise ratio (SNR):\n\\[F_{\\text{Wiener}}(\\omega) = \\frac{G^*(\\omega)}{|G(\\omega)|^2 + \\frac{1}{\\text{SNR}(\\omega)}} \\cdot Y(\\omega)\\]\nWhere \\(G^*(\\omega)\\) is the complex conjugate of \\(G(\\omega)\\) and \\(\\text{SNR}(\\omega)\\) is the signal-to-noise ratio at each frequency.\nThis approach is more adaptive than simple regularization, as it adjusts the regularization based on the expected noise level at each frequency.\n\n\n3. Iterative Methods\nFor very ill-conditioned problems, iterative methods like conjugate gradient or LSMR can be more stable:\n\\[f_{k+1} = f_k + \\alpha_k(g^* * (y - g * f_k))\\]\nWhere \\(g^*\\) is the adjoint (time-reversed) kernel and \\(\\alpha_k\\) is a step size.\nThese methods gradually refine the solution, avoiding direct division in the frequency domain.\n\n\n4. Wavelet-Based Deconvolution\nWavelets provide localization in both time and frequency, making them well-suited for deconvolution problems:\n\nTransform the signal to the wavelet domain\nApply regularized inversion in the wavelet domain\nTransform back to the time domain\n\nThis approach can better handle signals with localized features and non-stationary properties."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "href": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "title": "Integration by way of Convolution",
    "section": "Specific Case: Integration After Differentiation",
    "text": "Specific Case: Integration After Differentiation\nFor your specific interest in performing integration after a differential kernel has been applied, let me explain the mathematical connection more explicitly.\nIf we have applied the second derivative kernel [1, -2, 1] to a signal \\(f\\), obtaining \\(y\\):\n\\[y[n] = f[n+1] - 2f[n] + f[n-1] \\approx \\frac{d^2f}{dt^2}\\]\nThen to recover \\(f\\), we need to integrate \\(y\\) twice. In the continuous domain, this would be:\n\\[f(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nIn the frequency domain, integration corresponds to division by \\(j\\omega\\). So double integration is division by \\((j\\omega)^2 = -\\omega^2\\). The frequency response of our [1, -2, 1] kernel is approximately \\(-\\omega^2\\), which means the ideal recovery filter would be \\(\\frac{1}{-\\omega^2} = -\\frac{1}{\\omega^2}\\).\nThis perfectly matches our regularized inverse:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{-\\omega^2 + \\epsilon}\\]\nThe regularization term \\(\\epsilon\\) prevents division by zero at \\(\\omega = 0\\), which corresponds to the integration constants we would need to determine in the time domain approach."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "href": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "title": "Integration by way of Convolution",
    "section": "Practical Implementation for Integration Recovery",
    "text": "Practical Implementation for Integration Recovery\nLet me outline a robust approach for your integration task:\n\nSpectral Domain Method with Regularization\ndef recover_by_integration_spectral(signal, kernel, epsilon=1e-6):\n    n_fft = len(signal) + len(kernel) - 1\n    padded_signal = F.pad(signal, (0, n_fft - len(signal)))\n    padded_kernel = F.pad(kernel, (0, n_fft - len(kernel)))\n\n    # FFT\n    signal_fft = torch.fft.rfft(padded_signal)\n    kernel_fft = torch.fft.rfft(padded_kernel)\n\n    # Inverse filtering with regularization\n    recovered_fft = signal_fft / (kernel_fft + epsilon)\n\n    # IFFT\n    recovered = torch.fft.irfft(recovered_fft)\n\n    return recovered[:len(signal)]\nTime Domain Integration with Boundary Correction\ndef recover_by_double_integration(signal, boundary_values=None):\n    # First integration\n    first_integral = torch.cumsum(signal, dim=0)\n\n    # Correct for linear drift (first integration constant)\n    if boundary_values and 'start_slope' in boundary_values:\n        first_integral = first_integral + boundary_values['start_slope'] * torch.arange(len(signal))\n\n    # Second integration\n    second_integral = torch.cumsum(first_integral, dim=0)\n\n    # Correct for constant offset (second integration constant)\n    if boundary_values and 'start_value' in boundary_values:\n        second_integral = second_integral + boundary_values['start_value']\n\n    return second_integral\nHybrid Approach\nFor the most robust recovery, we can combine spectral and time domain methods:\ndef hybrid_recovery(signal, kernel, epsilon=1e-6):\n    # Spectral recovery for high frequencies\n    spectral_recovery = recover_by_integration_spectral(signal, kernel, epsilon)\n\n    # Time domain integration for low frequency components\n    time_recovery = recover_by_double_integration(signal)\n\n    # High-pass filter for spectral recovery\n    high_freq = high_pass_filter(spectral_recovery)\n\n    # Low-pass filter for time domain recovery\n    low_freq = low_pass_filter(time_recovery)\n\n    # Combine the two\n    return high_freq + low_freq"
  },
  {
    "objectID": "Blog/Inverse_kernel.html#conclusion",
    "href": "Blog/Inverse_kernel.html#conclusion",
    "title": "Integration by way of Convolution",
    "section": "Conclusion",
    "text": "Conclusion\nThe epsilon regularization parameter provides a mathematically sound approximation to the inverse problem, creating a balance between recovery fidelity and numerical stability. It’s essentially solving a regularized least-squares problem that finds the solution with the best trade-off between data fidelity and solution stability.\nFor the specific case of recovering a signal after applying a differential kernel, direct integration methods can be combined with spectral approaches for robust results. The key challenge is determining the integration constants (or equivalently, the low-frequency components) that were lost during differentiation.\nThe most effective approach often combines multiple methods, using the strengths of each to compensate for the weaknesses of others. This hybrid approach can provide more reliable signal recovery across a wide range of practical scenarios."
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-i-the-landscape-of-linear-system-solvers",
    "href": "Blog/iterative_solvers.html#part-i-the-landscape-of-linear-system-solvers",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part I: The Landscape of Linear System Solvers",
    "text": "Part I: The Landscape of Linear System Solvers\n\nSection 1: Foundational Concepts: Direct vs. Iterative Solvers\nIn computational mathematics, one of the most fundamental and ubiquitous tasks is the solution of a system of linear equations, which can be expressed in the matrix form:\n\\[Ax = b\\]\nHere, \\(A\\) is a known \\(n \\times n\\) coefficient matrix, \\(b\\) is a known \\(n\\)-dimensional vector, and \\(x\\) is the \\(n\\)-dimensional solution vector to be found. The methods developed to solve this problem fall into two primary categories: direct methods and iterative methods.\nThe choice between these two families of algorithms is a critical decision in the design of numerical simulations, dictated by the size, structure, and origin of the linear system.\n\nDirect Methods\nDirect methods are algorithms that, in the absence of round-off error, compute the exact solution \\(x\\) in a finite and predetermined number of arithmetic operations. The most well-known of these is Gaussian elimination, which systematically transforms the original system into an equivalent upper triangular system that can be easily solved via back substitution. In modern numerical linear algebra, direct methods are almost always implemented through a matrix factorization, such as the LU, Cholesky, or QR decompositions.\nThe principal advantages of direct methods are their robustness and generality. They are applicable to a wide range of problems, and their behavior is highly predictable. For a non-singular matrix, a direct solver is guaranteed to produce a solution. This reliability has led to the development of mature, highly optimized, and robust software libraries like LAPACK, which are a cornerstone of scientific computing.\nHowever, the primary challenge and ultimate limitation of direct methods is their scaling behavior with respect to problem size, \\(n\\). When the matrix \\(A\\) is dense (i.e., has few zero entries), forming its LU factorization requires approximately \\(2n^3/3\\) arithmetic operations, with storage requirements of \\(O(n^2)\\). While formidable, this is often acceptable for problems of moderate size.\nThe true difficulty arises when dealing with the large, sparse matrices that are characteristic of many scientific and engineering applications, particularly those originating from the discretization of partial differential equations (PDEs). The factorization process for a sparse matrix introduces new non-zero elements in positions that were originally zero in the factors \\(L\\) and \\(U\\). This phenomenon, known as fill-in, can be catastrophic. For a sparse matrix arising from a 2D PDE discretization, where the number of non-zeros is proportional to \\(N\\), the storage required for the LU factors can grow to \\(O(N^{3/2})\\) and the computational work to \\(O(N^2)\\). For 3D problems, the situation is even more dire, with storage scaling as \\(O(N^{5/3})\\) and work as \\(O(N^{7/3})\\). For the million-variable systems common in modern simulations, storing the dense factors would require terabytes of memory, rendering direct methods completely impractical.\n\n\nIterative Methods\nIn stark contrast to direct methods, iterative methods do not compute the exact solution in a fixed number of steps. Instead, they begin with an initial guess for the solution, denoted \\(x^{(0)}\\) (often a vector of zeros), and progressively generate a sequence of improved approximations, \\(x^{(1)}, x^{(2)}, x^{(3)}, \\ldots\\), that ideally converge to the true solution \\(x\\). At each step \\(k\\), the quality of the approximation is typically measured by the norm of the residual vector, \\(r^{(k)} = b - Ax^{(k)}\\). The iteration continues until this residual is smaller than a user-specified tolerance, or a maximum number of iterations is reached.\nThe primary advantages of iterative methods directly address the shortcomings of direct solvers for large-scale problems:\nMemory Efficiency: Iterative methods typically only require the storage of the non-zero elements of the matrix \\(A\\) along with a handful of auxiliary vectors. For a sparse matrix with an average of \\(k\\) non-zeros per row, the memory footprint is \\(O(Nk)\\), which is vastly more efficient than the memory required for the dense factors of a direct method.\nComputational Efficiency: The core computational kernel of most modern iterative methods is the matrix-vector product \\((A \\cdot v)\\). For a sparse matrix, this operation is very cheap, requiring only \\(O(Nk)\\) floating-point operations. If the method converges to a satisfactory solution in \\(m\\) iterations, and if \\(m \\ll N\\), the total computational work, roughly \\(O(Nkm)\\), can be orders of magnitude lower than that of a direct solve. For certain classes of problems, particularly those from elliptic PDEs, advanced iterative methods can converge in a number of iterations that is nearly independent of the problem size \\(N\\), achieving so-called linear cost.\nParallelism: The fundamental operations of iterative methods—matrix-vector products, inner products, and vector updates (AXPY operations)—are composed of many independent calculations. This structure makes them far easier to implement efficiently on parallel computing architectures compared to the complex data dependencies and communication patterns inherent in parallel matrix factorizations.\nTunable Precision: Iterative methods provide the flexibility to trade off computational effort for solution accuracy. In many application contexts, such as the inner loops of a nonlinear solver or a time-dependent simulation, a highly precise solution to the linear system at each step is unnecessary. An approximate solution is often sufficient, and an iterative method can be stopped early, saving significant computation time.\nThe main drawback of iterative methods is that their performance is not as predictable as that of direct methods. Convergence is not guaranteed for all linear systems, and the rate of convergence can vary dramatically depending on the properties of the matrix \\(A\\), most notably its condition number. For many challenging problems, a “naive” iterative method will converge too slowly to be practical, or it may fail to converge at all. Consequently, the successful application of iterative methods often requires careful selection of the algorithm and the use of sophisticated preconditioning techniques, which are designed to transform the problem into one that is more amenable to iterative solution.\n\n\nEconomic Perspective\nThe choice between direct and iterative solvers can be viewed through an economic lens, balancing predictable but potentially prohibitive costs against lower per-unit costs with uncertain total effort. A direct solver is akin to purchasing a custom manufacturing machine: the upfront cost (factorization) is high and determined by the problem size, but once paid, it can produce solutions (for different right-hand sides, \\(b\\)) relatively cheaply and in a known amount of time. An iterative solver is more like hiring a worker on an hourly basis: the rate per hour (cost per iteration) is low and predictable, but the total time required to complete the job (number of iterations) is not known in advance and depends on the difficulty of the task (the condition of the matrix). For small, simple jobs (small, dense matrices), the certainty of the machine is preferable. For massive, complex projects (large, sparse matrices), the lower hourly rate is the only feasible option, and the focus shifts to managing the project efficiently to minimize the total hours worked—a role perfectly analogous to preconditioning in the world of iterative solvers.\n\n\n\n\n\n\n\n\nFeature\nDirect Methods\nIterative Methods\n\n\n\n\nSolution Accuracy\nExact (in absence of round-off error)\nApproximate, up to a specified tolerance\n\n\nComputational Cost (Dense N×N)\nHigh, typically \\(O(N^3)\\)\nVery high, generally not used\n\n\nComputational Cost (Sparse N×N)\nHigh due to fill-in, e.g., \\(O(N^2)\\) for 2D PDEs\nLow if convergence is fast, \\(O(Nkm)\\) where \\(m \\ll N\\)\n\n\nMemory Usage (Sparse)\nHigh due to fill-in, e.g., \\(O(N^{3/2})\\) for 2D PDEs\nLow, typically \\(O(Nk)\\)\n\n\nApplicability\nGeneral-purpose for any non-singular matrix\nMethod choice depends on matrix properties (e.g., symmetry)\n\n\nRobustness\nVery high; predictable behavior\nConvergence is not guaranteed; can be slow or fail\n\n\nParallelism\nDifficult to parallelize efficiently due to data dependencies\nCore operations are highly parallelizable\n\n\nKey Challenge\nManaging fill-in and the associated high memory/computational cost\nEnsuring and accelerating convergence\n\n\n\nTable 1: Comparison of Direct vs. Iterative Solvers"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-ii-a-deep-dive-into-modern-krylov-subspace-methods",
    "href": "Blog/iterative_solvers.html#part-ii-a-deep-dive-into-modern-krylov-subspace-methods",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part II: A Deep Dive into Modern Krylov Subspace Methods",
    "text": "Part II: A Deep Dive into Modern Krylov Subspace Methods\nThe most powerful and widely used iterative techniques today belong to the family of Krylov subspace methods. These methods work by generating an optimal approximate solution from a special subspace—the Krylov subspace—which is built using successive applications of the matrix \\(A\\) to an initial vector, typically the initial residual \\(r_0\\). The Krylov subspace of order \\(m\\) is defined as:\n\\[\\mathcal{K}_m(A, r_0) = \\text{span}\\{r_0, Ar_0, A^2r_0, \\ldots, A^{m-1}r_0\\}\\]\nBy searching for a solution within this subspace, these methods implicitly construct a polynomial in the matrix \\(A\\) that approximates \\(A^{-1}\\). This section delves into the two most important Krylov subspace methods: the Conjugate Gradient method for symmetric positive-definite systems and the Generalized Minimal Residual method for general non-symmetric systems.\n\nSection 2: The Conjugate Gradient (CG) Method: The Workhorse for Symmetric Positive-Definite Systems\nThe Conjugate Gradient (CG) method, developed by Magnus Hestenes and Eduard Stiefel in 1952, is arguably the most important iterative method ever devised. It is the algorithm of choice for solving large, sparse linear systems where the coefficient matrix \\(A\\) is symmetric and positive-definite (SPD). An SPD matrix is a symmetric matrix for which \\(x^T Ax &gt; 0\\) for any non-zero vector \\(x\\), or equivalently, all its eigenvalues are positive.\n\nMathematical Foundation: An Optimization Perspective\nThe power and elegance of the CG method stem from its deep connection to optimization. For an SPD matrix \\(A\\), solving the linear system \\(Ax = b\\) is mathematically equivalent to finding the unique vector \\(x\\) that minimizes the quadratic function (often called a quadratic form or energy functional):\n\\[\\phi(x) = \\frac{1}{2}x^T Ax - b^T x\\]\nThis equivalence is established by examining the gradient of \\(\\phi(x)\\). The gradient is given by \\(\\nabla \\phi(x) = Ax - b\\). The minimum of the convex function \\(\\phi(x)\\) occurs where its gradient is zero, which means \\(Ax - b = 0\\), or \\(Ax = b\\). Thus, the linear system solution is the minimizer of the quadratic form.\nThis reframing allows us to approach the linear algebra problem using optimization techniques. A simple starting point is the method of steepest descent, where one iteratively takes steps in the direction of the negative gradient, which is the direction of the fastest local decrease of \\(\\phi(x)\\). The search direction at step \\(k\\) is simply \\(p_k = r_k = b - Ax_k\\). While intuitive, steepest descent often converges very slowly, as the search directions can become nearly orthogonal in successive steps, leading to a characteristic zig-zagging path toward the minimum.\nThe breakthrough of the CG method lies in its choice of search directions. Instead of using the steepest descent directions, it constructs a set of search directions \\(\\{p_0, p_1, \\ldots, p_{n-1}\\}\\) that are A-conjugate (or A-orthogonal). This property is defined as:\n\\[p_i^T A p_j = 0 \\quad \\text{for all } i \\neq j\\]\nThis condition is a generalization of standard orthogonality; if \\(A = I\\), it reduces to the familiar dot product being zero. The set of \\(n\\) A-conjugate vectors forms a basis for \\(\\mathbb{R}^n\\). The profound consequence of A-conjugacy is that when we perform a line search to minimize \\(\\phi(x)\\) along a new direction \\(p_k\\), this minimization does not interfere with the minimization already achieved in the previous directions \\(\\{p_0, \\ldots, p_{k-1}\\}\\). This allows the method to converge to the exact solution in at most \\(n\\) iterations (in exact arithmetic), since it effectively performs \\(n\\) independent one-dimensional minimizations along the basis directions.\n\n\nThe Conjugate Gradient Algorithm\nA remarkable feature of the CG method is that these A-conjugate directions can be generated “on the fly” using a simple and efficient three-term recurrence relation. Each new direction \\(p_k\\) is constructed from the current residual \\(r_k\\) and the previous search direction \\(p_{k-1}\\), without needing to store all previous vectors. This makes the algorithm computationally inexpensive and require minimal storage (only a few vectors need to be stored at any time).\nThe algorithm proceeds as follows:\nInitialization: 1. Choose an initial guess \\(x_0\\) (e.g., \\(x_0 = 0\\)). 2. Compute the initial residual: \\(r_0 = b - Ax_0\\). 3. Set the first search direction to be the residual: \\(p_0 = r_0\\). 4. Compute \\(\\text{rsold} = r_0^T r_0\\).\nIteration: For \\(k = 0, 1, 2, \\ldots\\) until convergence: 1. Compute the matrix-vector product: \\(v_k = Ap_k\\). 2. Compute the optimal step size to minimize \\(\\phi\\) along \\(p_k\\): \\(\\alpha_k = \\frac{\\text{rsold}}{p_k^T v_k}\\). 3. Update the solution: \\(x_{k+1} = x_k + \\alpha_k p_k\\). 4. Update the residual: \\(r_{k+1} = r_k - \\alpha_k v_k\\). 5. Check for convergence: if \\(||r_{k+1}||_2\\) is below a tolerance, stop. 6. Compute \\(\\text{rsnew} = r_{k+1}^T r_{k+1}\\). 7. Update the search direction to be A-conjugate to previous directions: \\(p_{k+1} = r_{k+1} + \\frac{\\text{rsnew}}{\\text{rsold}} p_k\\). 8. Prepare for the next iteration: \\(\\text{rsold} = \\text{rsnew}\\).\nThis algorithm requires only one matrix-vector product per iteration, along with a few inner products and vector updates, making it extremely efficient.\n\n\nPython Implementation\nThe following Python code provides a basic, self-contained implementation of the Conjugate Gradient algorithm, illustrating its structure:\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=1000):\n    \"\"\"\n    Solves the system Ax=b for an SPD matrix A using the Conjugate Gradient method.\n\n    Args:\n        A (np.ndarray): The symmetric positive-definite coefficient matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray, optional): Initial guess. Defaults to a zero vector.\n        tol (float, optional): The tolerance for convergence. Defaults to 1e-6.\n        max_iter (int, optional): Maximum number of iterations. Defaults to 1000.\n\n    Returns:\n        tuple: A tuple containing the solution vector x and the number of iterations.\n    \"\"\"\n    n = len(b)\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n\n    if np.sqrt(rs_old) &lt; tol:\n        return x, 0\n\n    for i in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = r @ r\n        \n        if np.sqrt(rs_new) &lt; tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x, i + 1\n\n\nThe Dual Nature of Conjugate Gradient\nThe CG method occupies a unique position, blurring the line between direct and iterative solvers. Its theoretical foundation guarantees that for an \\(N \\times N\\) system, it will find the exact solution in at most \\(N\\) steps, a property characteristic of a direct method. This finite termination property is a direct consequence of constructing \\(N\\) A-orthogonal search directions that span the entire solution space \\(\\mathbb{R}^N\\).\nHowever, the true power of CG lies not in this finite termination property, but in its performance as an iterative method. For the very large systems where CG is applied (with \\(N\\) in the millions), performing \\(N\\) iterations is computationally infeasible and would be slower than a direct solve. The practical utility of CG comes from its optimality property: at each iteration \\(k\\), the CG algorithm finds the approximation \\(x_k\\) in the Krylov subspace \\(\\mathcal{K}_k(A, r_0)\\) that minimizes the A-norm of the error, \\(||x - x_k||_A = \\sqrt{(x - x_k)^T A (x - x_k)}\\).\nThis optimality ensures that CG makes the best possible progress toward the solution at every step, given the information available in the Krylov subspace. As a result, it often produces an excellent approximation to the solution in a number of iterations \\(k\\) that is much smaller than the matrix dimension \\(N\\), i.e., \\(k \\ll N\\). The rate of this convergence is closely tied to the distribution of the eigenvalues of \\(A\\). If the eigenvalues are clustered together, or if the condition number \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\) is small, convergence is very rapid. Therefore, while its theoretical underpinnings classify it as a direct method, its practical application and value are entirely as a fast iterative method. This dual identity resolves the apparent contradiction in its common descriptions and highlights its exceptional nature among numerical algorithms.\n\n\n\nSection 3: The Generalized Minimal Residual (GMRES) Method: Tackling Non-Symmetric Systems\nWhen the coefficient matrix \\(A\\) is not symmetric, or not positive-definite, the optimization framework of the CG method is no longer applicable. For these more general cases, the Generalized Minimal Residual (GMRES) method is the standard and most robust Krylov subspace technique. Developed by Youcef Saad and Martin Schultz in 1986, GMRES is designed to solve any non-singular linear system \\(Ax = b\\).\n\nMathematical Foundation: A Least-Squares Perspective\nThe core principle of GMRES is fundamentally different from that of CG. Instead of minimizing an energy functional, GMRES directly tackles the residual. At each iteration \\(m\\), GMRES finds the vector \\(x_m\\) in the affine Krylov subspace \\(x_0 + \\mathcal{K}_m(A, r_0)\\) that minimizes the 2-norm (Euclidean norm) of the corresponding residual vector. That is, it solves the minimization problem:\n\\[x_m = \\arg\\min_{z \\in x_0 + \\mathcal{K}_m(A, r_0)} ||b - Az||_2\\]\nTo solve this problem efficiently and in a numerically stable manner, GMRES employs the Arnoldi iteration. The Arnoldi process is an algorithm that constructs an orthonormal basis \\(\\{v_1, v_2, \\ldots, v_m\\}\\) for the Krylov subspace \\(\\mathcal{K}_m(A, r_0)\\). After \\(m\\) steps, the Arnoldi process yields two crucial outputs:\n\nA matrix \\(V_{m+1} = [v_1, v_2, \\ldots, v_{m+1}]\\) whose columns form an orthonormal basis for \\(\\mathcal{K}_{m+1}(A, r_0)\\).\nAn \\((m+1) \\times m\\) upper-Hessenberg matrix \\(\\tilde{H}_m\\) (a matrix with zeros below the first subdiagonal).\n\nThese matrices are related by the fundamental Arnoldi relation: \\(AV_m = V_{m+1}\\tilde{H}_m\\). This relation is the key to making GMRES practical. Any vector \\(z\\) in the search space can be written as \\(z = x_0 + V_m y\\) for some vector \\(y \\in \\mathbb{R}^m\\). Substituting this into the residual minimization problem gives:\n\\[\\min_{y \\in \\mathbb{R}^m} ||b - A(x_0 + V_m y)||_2 = \\min_{y \\in \\mathbb{R}^m} ||r_0 - AV_m y||_2\\]\nUsing the Arnoldi relation and the fact that \\(v_1 = r_0/||r_0||_2\\), this becomes:\n\\[\\min_{y \\in \\mathbb{R}^m} ||r_0||_2 v_1 - V_{m+1}\\tilde{H}_m y||_2\\]\nSince the columns of \\(V_{m+1}\\) are orthonormal, multiplying by \\(V_{m+1}^T\\) does not change the 2-norm. This transforms the original large, \\(N\\)-dimensional minimization problem into a small, \\((m+1) \\times m\\) linear least-squares problem that is cheap to solve:\n\\[\\min_{y \\in \\mathbb{R}^m} ||||r_0||_2 e_1 - \\tilde{H}_m y||_2\\]\nwhere \\(e_1\\) is the first standard basis vector. This small least-squares problem is typically solved using a QR factorization of \\(\\tilde{H}_m\\), which can be updated efficiently at each step using Givens rotations.\n\n\nThe GMRES Algorithm and its Practical Costs\nThe full GMRES algorithm involves an outer loop over the iteration count \\(m\\). Inside the loop, one step of the Arnoldi process is performed to generate the new basis vector \\(v_{m+1}\\) and the \\(m\\)-th column of the Hessenberg matrix \\(\\tilde{H}_m\\). Then, the small least-squares problem is solved to find the coefficients \\(y\\), and the approximate solution \\(x_m\\) is formed.\nA critical drawback of this process becomes apparent when compared to CG. The Arnoldi iteration is a “long-term” recurrence. To ensure the new vector \\(v_{m+1}\\) is orthogonal to all previous basis vectors, it must be explicitly orthogonalized against every one of them \\((v_1, \\ldots, v_m)\\) using a Gram-Schmidt process. This means that both the computational work and the storage required per iteration grow linearly with the iteration count \\(m\\). The storage cost is \\(O(Nm)\\) and the work per iteration is \\(O(Nm)\\) for the orthogonalization, plus the cost of the matrix-vector product. For problems that converge slowly, requiring a large \\(m\\), this becomes prohibitively expensive.\nThe standard solution to this practical limitation is restarted GMRES, denoted GMRES(k). In this variant, the algorithm is run for a fixed number of \\(k\\) iterations (where \\(k\\) is the restart parameter, typically a small number like 20 or 50). After \\(k\\) steps, the accumulated Krylov basis is discarded, an updated solution \\(x_k\\) is computed, and the entire process is restarted using \\(x_k\\) as the new initial guess. This strategy keeps the memory and computational costs bounded and manageable. However, this practicality comes at a price: by discarding the Krylov subspace, the algorithm loses its optimality and monotonic convergence properties. The residual norm is no longer guaranteed to decrease at every outer iteration, and the method can stagnate, especially for difficult problems.\n\n\nPython Implementation\nImplementing GMRES from scratch is considerably more involved than CG due to the Arnoldi process and the least-squares solve. Therefore, it is almost always used via a library function like scipy.sparse.linalg.gmres:\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import gmres\n\n# Define a non-symmetric matrix A and a right-hand side b\nA = csc_matrix([[3, 1, 0], [1, -1, 0], [0, 1, 2]], dtype=float)\nb = np.array([2, 4, -1], dtype=float)\n\n# Set an initial guess\nx0 = np.zeros(A.shape[0])\n\n# Solve the system using GMRES with a restart parameter of 20\n# and a tolerance of 1e-8.\nx, exit_code = gmres(A, b, x0=x0, restart=20, tol=1e-8)\n\nif exit_code == 0:\n    print(\"GMRES converged to a solution.\")\n    print(f\"Solution x: {x}\")\n    print(f\"Residual norm: {np.linalg.norm(b - A @ x)}\")\nelse:\n    print(f\"GMRES did not converge. Exit code: {exit_code}\")\n\n\nThe Optimality vs. Practicality Dilemma\nThe design of GMRES and its common restarted variant perfectly encapsulates a central trade-off in numerical algorithm design: the tension between theoretical optimality and practical feasibility. Full GMRES is “optimal” in the sense that it finds the approximation with the smallest possible residual norm within the ever-expanding Krylov subspace at each step. This guarantees that the residual norm decreases monotonically, a very desirable property.\nThis optimality, however, is built on the long-term recurrence of the Arnoldi process. To maintain the orthonormal basis, each new vector must be compared against all previous ones, leading to work and storage costs that grow with each iteration. The reason CG avoids this fate is the profound consequence of symmetry. For an SPD matrix, the Arnoldi process simplifies to the Lanczos process, which has a short three-term recurrence. This allows CG to generate its A-orthogonal basis with constant work and storage per iteration. This is a luxury not afforded to general non-symmetric matrices.\nGMRES(k) is the pragmatic compromise. It sacrifices the powerful optimality and guaranteed monotonic convergence of the full method to gain an algorithm with fixed, manageable memory and computational costs per cycle of \\(k\\) iterations. The choice of the restart parameter \\(k\\) is a heuristic balancing act: if \\(k\\) is too small, the algorithm may lose too much information from the Krylov subspace at each restart and converge very slowly or stagnate; if \\(k\\) is too large, the cost of the inner iterations becomes excessive. This illustrates that for the general class of non-symmetric problems, we must often accept less elegant and more heuristic solutions than those available for the highly structured SPD case.\n\n\n\n\n\n\n\n\nFeature\nConjugate Gradient (CG)\nGeneralized Minimal Residual (GMRES)\n\n\n\n\nApplicable Matrix Type\nSymmetric Positive-Definite (SPD)\nGeneral, Non-singular\n\n\nUnderlying Principle\nMinimization of a quadratic form \\(\\phi(x)\\)\nMinimization of the residual norm \\(||r||_2\\)\n\n\nOptimality Criterion\nMinimizes \\(||x - x_k||_A\\) in \\(\\mathcal{K}_k\\)\nMinimizes \\(||r_k||_2\\) in \\(x_0 + \\mathcal{K}_k\\)\n\n\nRecurrence Length\nShort (3-term recurrence)\nLong (depends on all previous vectors)\n\n\nWork per Iteration\nConstant, \\(O(Nk)\\)\nGrows with iteration \\(m\\), \\(O(Nm)\\)\n\n\nStorage per Iteration\nConstant (a few vectors)\nGrows with iteration \\(m\\), \\(O(Nm)\\)\n\n\nConvergence Guarantee\nGuaranteed for SPD matrices\nMonotonic residual reduction (full GMRES)\n\n\nCommon Variant\nPreconditioned CG (PCG)\nRestarted GMRES (GMRES(k))\n\n\n\nTable 2: Summary of Key Krylov Subspace Methods (CG and GMRES)"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-iii-the-primary-application-solving-partial-differential-equations",
    "href": "Blog/iterative_solvers.html#part-iii-the-primary-application-solving-partial-differential-equations",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part III: The Primary Application: Solving Partial Differential Equations",
    "text": "Part III: The Primary Application: Solving Partial Differential Equations\nWhile the study of iterative solvers is a rich subfield of numerical linear algebra, their development is not an abstract exercise. The primary motivation for creating and refining these algorithms is their application to solving problems from science and engineering. Overwhelmingly, this means solving the massive linear systems that arise from the numerical approximation of Partial Differential Equations (PDEs). This part of the report will bridge the gap between the algebraic methods described previously and their principal domain of application.\n\nSection 4: How PDEs Generate Large Linear Systems\nPDEs are mathematical equations that describe physical phenomena involving rates of change with respect to multiple independent variables, such as space and time. They are the language of physics, modeling everything from heat conduction and fluid dynamics to structural mechanics and electromagnetism. Except in very simple cases, these equations cannot be solved analytically. Instead, we must turn to computational methods, which requires transforming the continuous problem into a discrete one that a computer can handle. This process is known as discretization.\n\nDiscretization: From Continuous to Discrete\nThe core idea of discretization is to replace the continuous domain of the PDE with a finite set of points or volumes and to approximate the derivatives in the PDE with algebraic expressions involving the solution values at these discrete locations. Two of the most common discretization techniques are the Finite Difference Method and the Finite Element Method.\nFinite Difference Method (FDM): This is the most direct approach to discretization. The problem domain is overlaid with a regular grid of points. At each grid point, the partial derivatives in the PDE are replaced by finite difference approximations, which are derived from Taylor series expansions. For example, the second partial derivative of a function \\(u(x,y)\\) with respect to \\(x\\) at a grid point \\((i,j)\\) can be approximated by a centered difference:\n\\[\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2}\\]\nwhere \\(h\\) is the spacing between grid points and \\(u_{i,j}\\) is the approximate solution at \\((x_i, y_j)\\). By substituting these algebraic approximations for all derivatives in the PDE at every interior grid point, the differential equation is transformed into a large system of coupled linear equations. The unknowns in this system are the values of the solution at each grid point.\nFinite Element Method (FEM): FEM is a more versatile and powerful technique, particularly for problems with complex geometries or varying material properties. In this method, the continuous domain is partitioned into a mesh of smaller, simpler geometric shapes called “finite elements” (e.g., triangles in 2D, tetrahedra in 3D). Within each element, the unknown solution is approximated by a simple function, typically a polynomial, defined in terms of its values at the element’s nodes. The PDE is then reformulated into an equivalent integral or “weak” form. By requiring this integral form to hold over the collection of finite elements, a system of linear algebraic equations is generated, where the unknowns are the solution values at the nodes of the mesh.\n\n\nProperties of the Resulting Matrix A\nRegardless of the specific method used (FDM or FEM), the discretization of a PDE results in a linear system \\(Ax = b\\) with several defining characteristics:\nLarge-Scale: The number of equations and unknowns, \\(N\\), is equal to the number of degrees of freedom in the discrete model (e.g., the number of grid points in FDM). To achieve high accuracy, especially in three dimensions, the mesh must be very fine. It is common for \\(N\\) to be in the millions or even billions, making the system enormous.\nSparsity: A crucial feature of these systems is that they are sparse. The equation corresponding to a particular node or element only involves its immediate neighbors in the mesh. For instance, a standard 5-point finite difference stencil for the 2D Laplacian results in an equation at grid point \\((i,j)\\) that only involves values at \\((i,j)\\), \\((i±1,j)\\), and \\((i,j±1)\\). Consequently, the corresponding row in the matrix \\(A\\) will have at most five non-zero entries, regardless of how large \\(N\\) is. This inherent sparsity is what makes the use of iterative methods both possible and necessary.\nStructure: The matrices are not only sparse but often highly structured. For problems on regular grids, the non-zero entries form distinct patterns, such as bands along the main diagonal (e.g., a tridiagonal or block-tridiagonal structure). This structure can sometimes be exploited by specialized solvers.\n\n\n\nSection 5: Matching Solvers to PDE Types\nThe mathematical classification of a second-order PDE is not merely an abstract label; it reflects the fundamental physics of the problem it models. This classification, in turn, directly determines the mathematical properties of the matrix \\(A\\) that arises from its discretization, and this is the critical link that guides the selection of an appropriate iterative solver.\n\nElliptic PDEs\nPrototype and Physics: The canonical elliptic PDE is the Laplace equation (\\(\\nabla^2 u = 0\\)) or the Poisson equation (\\(\\nabla^2 u = f\\)). These equations model steady-state phenomena where the system has reached equilibrium and there is no time evolution. Physical examples include steady-state heat conduction, electrostatics, potential fluid flow, and stress analysis in solid mechanics.\nResulting Matrix Properties: When a self-adjoint elliptic operator like the Laplacian is discretized using standard methods such as centered finite differences or the Galerkin finite element method, the resulting matrix \\(A\\) is almost always Symmetric and Positive-Definite (SPD). The symmetry of \\(A\\) is a direct reflection of the self-adjoint property of the continuous operator. The positive-definiteness is linked to the dissipative or energy-minimizing nature of the underlying physics; for example, in heat transfer, the system seeks to minimize thermal energy.\nRecommended Solver: The SPD nature of the matrix makes the Conjugate Gradient (CG) method the ideal and most efficient iterative solver for these systems. Its convergence is guaranteed, and its performance is optimal for this class of matrices.\n\n\nParabolic PDEs\nPrototype and Physics: The classic parabolic PDE is the heat equation, \\(\\frac{\\partial u}{\\partial t} = \\alpha \\nabla^2 u\\). These equations model time-dependent diffusion processes, where a quantity like heat or a chemical concentration spreads and smooths out over time.\nResulting Matrix Properties: The properties of the matrix for a parabolic problem depend on how the time derivative is discretized. Using the method of lines, one discretizes in space first, yielding a system of ordinary differential equations (ODEs): \\(\\frac{du}{dt} = -A_s u + f\\), where \\(A_s\\) is the spatial discretization matrix. Applying a time-stepping scheme to solve this ODE system leads to a linear system at each time step.\nFor an implicit time-stepping scheme like Backward Euler, the system to be solved at each step is of the form \\((I + \\Delta t \\alpha A_s)u^{n+1} = u^n\\). If the spatial operator \\(A_s\\) (from the elliptic part \\(\\nabla^2 u\\)) is SPD, then the full system matrix \\(A = (I + \\Delta t \\alpha A_s)\\) is also SPD.\nHowever, if the problem includes a convection (or advection) term, such as in the convection-diffusion equation (\\(\\frac{\\partial u}{\\partial t} + v \\cdot \\nabla u = \\alpha \\nabla^2 u\\)), the first-order spatial derivative \\(\\nabla u\\) introduces a non-symmetric component into the spatial operator. The resulting system matrix \\(A\\) will be non-symmetric.\nRecommended Solver: For pure diffusion problems solved with implicit schemes, the resulting SPD system is well-suited for CG. When a convection term is present and significant, the matrix becomes non-symmetric, and GMRES becomes the necessary choice.\n\n\nHyperbolic PDEs\nPrototype and Physics: The quintessential hyperbolic PDE is the wave equation, \\(\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\nabla^2 u\\). These equations describe transport and wave propagation phenomena, such as acoustics, electromagnetics, and fluid dynamics with shocks.\nResulting Matrix Properties: The discretization of hyperbolic equations, especially when written as a first-order system or when dominated by advection terms, almost invariably leads to non-symmetric matrices. These matrices can also be indefinite (having both positive and negative eigenvalues) and are often more ill-conditioned than those from elliptic problems.\nRecommended Solver: Due to the non-symmetric and often indefinite nature of the system matrix, GMRES is the standard iterative solver for discretized hyperbolic PDEs. CG is fundamentally inapplicable in this context.\nWhile this mapping from PDE class to solver choice is a powerful and generally reliable guide, it is essential to recognize that the properties of the matrix \\(A\\) are a function of both the continuous PDE operator and the specific numerical discretization scheme chosen: \\(A = \\text{Discretize}(\\text{Operator}_{\\text{PDE}})\\). For example, while the Laplacian is an elliptic operator, discretizing it with certain non-standard finite difference stencils or mixed finite element methods can lead to non-symmetric or indefinite saddle-point systems. An expert practitioner must therefore consider the details of the numerical method when selecting a solver, not just the broad classification of the PDE. This understanding underscores why robust numerical libraries often query the properties of the matrix itself (e.g., by testing for symmetry) rather than relying solely on user-provided metadata about the problem’s physical origin.\n\n\n\n\n\n\n\n\n\n\nPDE Class\nPhysical Example\nTypical Matrix Properties from Standard Discretization\nRecommended Iterative Solver\nCommon Preconditioners\n\n\n\n\nElliptic\nSteady-State Heat Conduction, Electrostatics\nSymmetric Positive-Definite (SPD)\nConjugate Gradient (CG)\nIncomplete Cholesky (IC), Multigrid\n\n\nParabolic (Diffusion)\nTransient Heat Transfer\nSymmetric Positive-Definite (SPD) (with implicit schemes)\nConjugate Gradient (CG)\nIncomplete Cholesky (IC), SSOR\n\n\nParabolic (Convection)\nPollutant Transport\nNon-symmetric\nGMRES\nIncomplete LU (ILU)\n\n\nHyperbolic\nAcoustics, Wave Propagation\nNon-symmetric, often indefinite and ill-conditioned\nGMRES\nIncomplete LU (ILU), Domain Decomposition\n\n\n\nTable 3: PDE Characteristics and Recommended Solver/Preconditioner Pairings"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-iv-the-crucial-enhancement-preconditioning",
    "href": "Blog/iterative_solvers.html#part-iv-the-crucial-enhancement-preconditioning",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part IV: The Crucial Enhancement: Preconditioning",
    "text": "Part IV: The Crucial Enhancement: Preconditioning\nThe successful application of iterative methods to the large, complex linear systems arising from PDEs is rarely a simple matter of choosing a solver and running it. More often than not, the raw performance of a method like CG or GMRES is insufficient for practical use. The convergence can be painfully slow, or it may fail altogether. This section addresses this critical challenge and introduces preconditioning, the single most important family of techniques for making iterative solvers robust, efficient, and truly powerful.\n\nSection 6: The “Why” of Preconditioning: Battling the Condition Number\nThe convergence rate of Krylov subspace methods is intimately linked to the spectral properties of the coefficient matrix \\(A\\). For a problem to be “easy” for an iterative solver, the matrix must be “well-conditioned.”\n\nThe Problem: Ill-Conditioning and the Condition Number\nThe metric that quantifies the “difficulty” of a linear system is the condition number, denoted \\(\\kappa(A)\\). Formally, it is defined as \\(\\kappa(A) = ||A|| \\cdot ||A^{-1}||\\), where \\(||\\cdot||\\) is some matrix norm. For the SPD matrices relevant to CG, the 2-norm condition number has a simple and intuitive interpretation: it is the ratio of the largest eigenvalue to the smallest eigenvalue of the matrix, \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\).\nA small condition number (close to 1) indicates a well-conditioned problem. A very large condition number signifies an ill-conditioned problem, meaning the matrix is close to being singular (non-invertible). The effect of the condition number on iterative solver performance is dramatic:\n\nFor the Conjugate Gradient method, the number of iterations required to reach a certain tolerance is approximately proportional to the square root of the condition number, i.e., iterations \\(\\propto \\sqrt{\\kappa(A)}\\).\nFor GMRES, the relationship is more complex and depends on the full distribution of eigenvalues in the complex plane, but its convergence is also severely hampered by a large condition number.\n\nA major source of ill-conditioned systems is the discretization of PDEs. As the discretization mesh is refined to achieve higher physical accuracy (i.e., the mesh spacing \\(h\\) goes to zero), the condition number of the resulting matrix \\(A\\) typically blows up. For a second-order elliptic problem, \\(\\kappa(A)\\) grows like \\(O(h^{-2})\\). This creates a vicious cycle: the very act of improving the physical model’s accuracy makes the resulting algebraic problem exponentially harder to solve.\n\n\nThe Solution: Preconditioning\nPreconditioning is a strategy to combat this ill-conditioning. The core idea is not to solve the original system \\(Ax = b\\), but to solve a mathematically equivalent system that has more favorable spectral properties. This is achieved by introducing a preconditioner, which is a matrix \\(M\\) that is, in some sense, a cheap and simple approximation of \\(A\\). The preconditioned system can be formed in several ways:\nLeft Preconditioning: The system is transformed to \\((M^{-1}A)x = M^{-1}b\\). The iterative solver is then applied to the matrix \\(M^{-1}A\\) and the right-hand side \\(M^{-1}b\\).\nRight Preconditioning: The system is transformed to \\((AM^{-1})y = b\\), where the original solution is recovered via \\(x = M^{-1}y\\). Here, the solver is applied to the matrix \\(AM^{-1}\\). A key advantage of this approach is that the original residual \\(r = b - Ax\\) can still be monitored for convergence, which is often desirable.\nSymmetric Preconditioning: When \\(A\\) is SPD and we wish to use CG, it is crucial that the preconditioned matrix also be SPD. This is achieved with a preconditioner of the form \\(M = CC^T\\), where \\(C\\) is non-singular. The system becomes \\((C^{-1}AC^{-T})y = C^{-1}b\\), with \\(x = C^{-T}y\\). The matrix \\(C^{-1}AC^{-T}\\) is guaranteed to be SPD.\nThe ideal preconditioner \\(M\\) must satisfy two competing objectives:\nEffectiveness: \\(M\\) must be a good approximation of \\(A\\), such that the preconditioned matrix (\\(M^{-1}A\\) or \\(AM^{-1}\\)) is close to the identity matrix. This ensures that its condition number is close to 1, leading to rapid convergence.\nEfficiency: The action of the preconditioner, which involves solving a system of the form \\(Mz = r\\) for \\(z\\), must be computationally very cheap to perform at every iteration of the solver.\nThis duality defines the central challenge of preconditioning. The perfect preconditioner is \\(M = A\\), which makes \\(\\kappa(M^{-1}A) = \\kappa(I) = 1\\) and leads to convergence in one iteration. However, solving \\(Mz = r\\) is then equivalent to solving the original hard problem, making it useless. Conversely, the cheapest preconditioner is \\(M = I\\). Solving \\(Iz = r\\) is trivial, but it does nothing to improve the condition number. All practical preconditioners are therefore sophisticated compromises that lie on a spectrum between these two extremes. They are designed to capture the essential features of \\(A\\) that cause ill-conditioning while remaining simple enough to be inverted efficiently.\n\n\n\n\n\n\n\n\n\n\nPreconditioner Family\nCore Idea\nCost to Apply (per iteration)\nQuality/Effectiveness\nTypical Use Case\n\n\n\n\nSimple/Stationary (Jacobi, GS, SOR)\nUse a simple part of \\(A\\) (e.g., diagonal) as the preconditioner.\nVery Low, \\(O(N)\\)\nLow to Moderate. Effective for diagonally dominant matrices.\nGeneral-purpose first attempt; smoothers in Multigrid.\n\n\nFactorization-based (ILU/IC)\nCompute a sparse, approximate factorization \\(A \\approx \\tilde{L}\\tilde{U}\\).\nLow, cost of sparse triangular solves.\nModerate to High. A powerful “black-box” technique.\nGeneral-purpose preconditioning for sparse systems from PDEs.\n\n\nProblem-Specific (Multigrid, DD)\nExploit the underlying geometry and physics of the PDE.\nLow to Moderate, but with higher setup cost.\nVery High, often “optimal” (\\(O(N)\\) solvers).\nLarge-scale PDE problems, especially elliptic, on serial or parallel machines.\n\n\n\nTable 4: A Comparative Overview of Preconditioning Techniques\n\n\n\nSection 7: A Catalogue of Preconditioning Techniques\nThis section provides a detailed examination of common “black-box” or general-purpose preconditioning techniques. These are often the first methods to try when the specific structure of a problem is either unknown or not easily exploitable. They form the foundation of many preconditioning strategies.\n\n7.1 Classical Stationary Methods as Preconditioners\nThe classical iterative methods—Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR)—can be repurposed as simple and computationally inexpensive preconditioners. Their structure is derived from a splitting of the matrix \\(A\\) into its diagonal (\\(D\\)), strictly lower triangular (\\(-L\\)), and strictly upper triangular (\\(-U\\)) parts, such that \\(A = D - L - U\\).\nJacobi Preconditioner:\nMathematical Structure: The Jacobi preconditioner is the simplest possible choice: it uses only the diagonal of the matrix \\(A\\). The preconditioner is defined as \\(M_J = D\\).\nRationale: The action of this preconditioner, solving \\(Mz = r\\), reduces to a simple, perfectly parallelizable vector scaling operation: \\(z = D^{-1}r\\), where \\(D^{-1}\\) is a diagonal matrix whose entries are the reciprocals of the diagonal entries of \\(A\\). This is extremely cheap to compute. The Jacobi preconditioner is effective only when the matrix \\(A\\) is strongly diagonally dominant, meaning the magnitude of each diagonal entry is large compared to the sum of the magnitudes of the off-diagonal entries in its row.\nPython Snippet (for GMRES):\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import gmres, LinearOperator\nfrom scipy.sparse import csc_matrix\n\n# Assume A (a sparse matrix) and b are defined\n# A = csc_matrix(...)\n# b = np.array(...)\n\n# Create the Jacobi preconditioner M_inv = D^-1\ndiagonal_of_A = A.diagonal()\nif np.any(diagonal_of_A == 0):\n    raise ValueError(\"Matrix has zero on diagonal, Jacobi is not applicable.\")\n\nM_inv = diags(1.0 / diagonal_of_A)\n\n# The preconditioner M is defined by its action M_inv * r\ndef apply_jacobi_precond(r):\n    return M_inv @ r\n\nM = LinearOperator(A.shape, matvec=apply_jacobi_precond)\n\n# Solve the preconditioned system\nx, exit_code = gmres(A, b, M=M, tol=1e-8)\nGauss-Seidel (GS) and Successive Over-Relaxation (SOR) Preconditioners:\nMathematical Structure: The Gauss-Seidel preconditioner uses the lower triangular part of \\(A\\), including the diagonal: \\(M_{GS} = D - L\\). Applying this preconditioner requires solving the lower triangular system \\((D - L)z = r\\), which is done efficiently via forward substitution. The SOR preconditioner is a generalization, defined as \\(M_{SOR} = \\frac{1}{\\omega}(D - \\omega L)\\), where \\(\\omega \\in (0, 2)\\) is a relaxation parameter that can accelerate convergence.\nRationale: By incorporating the lower triangular part of \\(A\\), GS and SOR preconditioners capture more information about the matrix than the simple Jacobi diagonal, often leading to better convergence. However, the forward substitution process is inherently sequential, making these preconditioners more difficult to parallelize than Jacobi. In the context of advanced methods, these stationary methods are rarely used as standalone preconditioners for Krylov solvers. Instead, their true value lies in their role as smoothers within Multigrid cycles. They are exceptionally good at damping high-frequency (oscillatory) error components, which is precisely the task of the smoother.\n\n\n7.2 Incomplete Factorization Preconditioners (ILU/IC)\nIncomplete factorization preconditioners are among the most powerful and popular general-purpose techniques for matrices arising from PDEs. They are a direct attempt to address the “perfect but useless” nature of a full LU factorization as a preconditioner.\nMathematical Structure: The core idea is to compute an approximate LU factorization, \\(A \\approx \\tilde{L}\\tilde{U}\\), where \\(\\tilde{L}\\) and \\(\\tilde{U}\\) are sparse lower and upper triangular matrices. The preconditioner is then \\(M = \\tilde{L}\\tilde{U}\\). Applying the preconditioner involves solving \\(Mz = r\\), which is done via a two-step forward and backward substitution: solve \\(\\tilde{L}y = r\\) for \\(y\\), then solve \\(\\tilde{U}z = y\\) for \\(z\\). This is efficient as long as \\(\\tilde{L}\\) and \\(\\tilde{U}\\) remain sparse. For SPD matrices, the symmetric analogue, Incomplete Cholesky (IC) factorization (\\(A \\approx \\tilde{L}\\tilde{L}^T\\)), is used.\nRationale - Controlling Fill-in: The key to these methods is to perform a standard factorization process but to strategically discard entries to prevent excessive fill-in and maintain sparsity. There are several strategies for this:\nILU(0) / IC(0): This is the simplest variant, where the sparsity pattern of the incomplete factors \\(\\tilde{L}\\) and \\(\\tilde{U}\\) is constrained to be exactly the same as the sparsity pattern of the original matrix \\(A\\). No new non-zero entries are allowed. This is computationally cheap and requires minimal extra storage, but its effectiveness can be limited.\nILUT (Incomplete LU with Thresholding): This is a more robust and flexible approach. During the factorization, fill-in is permitted, but any newly created entry whose magnitude is below a specified drop tolerance (droptol) is discarded. This allows the user to tune the trade-off between the accuracy of the preconditioner and its storage/computational cost. A smaller tolerance results in a denser, more accurate preconditioner that accelerates convergence more but is more expensive to compute and apply.\nPython Snippet (ILU Preconditioner with SciPy):\nimport numpy as np\nfrom scipy.sparse.linalg import spilu, gmres, LinearOperator\nfrom scipy.sparse import csc_matrix\n\n# Assume A (a sparse matrix) and b are defined\n# A = csc_matrix(...)\n# b = np.array(...)\n\n# Create the ILUT preconditioner object\n# drop_tol controls dropping of small terms, fill_factor controls memory usage\nilu = spilu(A, drop_tol=1e-5, fill_factor=20)\n\n# Define the action of the preconditioner M^-1\ndef apply_ilu_precond(r):\n    return ilu.solve(r)\n\nM = LinearOperator(A.shape, matvec=apply_ilu_precond)\n\n# Solve the preconditioned system using GMRES\nx, exit_code = gmres(A, b, M=M, tol=1e-8)\nThe preconditioners discussed in this section represent a spectrum of generality. Jacobi is the most basic and broadly applicable, but often the weakest. ILU is significantly more powerful and serves as a robust black-box choice for a wide array of problems arising from PDEs. However, its construction can be complex, and it may still fail or perform poorly for extremely ill-conditioned or indefinite systems. The failure of these general algebraic methods to adequately solve a problem is often a strong signal that the problem possesses a special structure that is not being exploited. This realization motivates the development of the problem-aware, physics-based preconditioners discussed in the next section, which represent the state of the art for high-performance scientific computing.\n\n\n\nSection 8: Advanced, Problem-Aware Preconditioners for PDEs\nWhen general-purpose algebraic preconditioners are insufficient, the next step is to employ methods that are explicitly designed to exploit the underlying structure of the PDE problem. These advanced techniques, such as Multigrid and Domain Decomposition methods, are not just algebraic manipulations; they are numerical algorithms that incorporate knowledge of the problem’s physics and geometry to construct highly effective, often optimal, preconditioners.\n\n8.1 Multigrid Methods\nMultigrid is widely regarded as one of the most efficient solution methods for the linear systems arising from elliptic PDEs, often achieving optimal complexity, meaning the computational cost to solve the system is proportional to the number of unknowns, \\(O(N)\\).\nMathematical Structure and Rationale: The power of multigrid stems from a fundamental insight into the nature of error in iterative methods. Simple iterative methods like Jacobi or Gauss-Seidel, when applied to PDE problems, are very effective at reducing high-frequency (or oscillatory) components of the error but are extremely inefficient at reducing low-frequency (or smooth) error components. Smooth error components change very little between adjacent grid points, so local relaxation operations have little effect on them.\nThe genius of multigrid is to recognize that an error component that is smooth on a fine grid will appear more oscillatory on a coarser grid. The method leverages a hierarchy of grids, from the fine grid where the solution is desired down to a very coarse grid. The core idea is to use a simple iterative method to handle the high-frequency error on the fine grid and then transfer the remaining smooth error to a coarser grid, where it can be eliminated efficiently.\nA single multigrid cycle consists of the following steps:\n\nPre-Smoothing: On the current (fine) grid, apply a few iterations of a simple relaxation method (e.g., Gauss-Seidel). This step effectively damps the high-frequency error components.\nResidual Computation: Calculate the residual of the smoothed approximation: \\(r_f = b_f - A_f x_f\\).\nRestriction: Transfer the fine-grid residual to the next coarser grid: \\(r_c = R(r_f)\\). The restriction operator \\(R\\) performs a weighted averaging of fine-grid values to produce coarse-grid values.\nCoarse-Grid Solve: On the coarse grid, solve the residual equation \\(A_c e_c = r_c\\) to find the error correction \\(e_c\\). This step is the recursive part of the algorithm: the coarse-grid system is itself solved using a multigrid cycle. This continues until a grid is reached that is so coarse it can be solved cheaply with a direct method.\nProlongation (Interpolation): Transfer the computed error correction from the coarse grid back to the fine grid: \\(e_f = P(e_c)\\). The prolongation operator \\(P\\) interpolates the coarse-grid values to produce fine-grid values.\nCorrection: Update the fine-grid solution with the interpolated correction: \\(x_f \\leftarrow x_f + e_f\\).\nPost-Smoothing: Apply a few more relaxation sweeps to smooth out any high-frequency errors introduced by the interpolation process.\n\nThe pattern of recursion defines the type of cycle. The V-cycle is the simplest, involving one recursive call. The W-cycle makes two recursive calls at each level, making it more robust but also more expensive per cycle. The F-cycle is an intermediate compromise. When used as a preconditioner for a Krylov method like CG, one multigrid cycle serves as the application of the preconditioner inverse, \\(M^{-1}\\).\nPython Snippet (using PyAMG): Implementing a multigrid solver from scratch is a significant undertaking. Libraries like PyAMG (Algebraic Multigrid) provide powerful implementations. Algebraic Multigrid (AMG) is a variant that automatically constructs the grid hierarchy and operators based only on the matrix \\(A\\), without needing explicit geometric information.\nimport pyamg\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import cg\n\n# Assume A is a sparse matrix from a discretized elliptic PDE\n# and b is the right-hand side vector.\n# A = csr_matrix(...)\n# b = np.array(...)\n\n# 1. Create the multigrid hierarchy (this is the setup phase)\n#    'smoothed_aggregation_solver' is a common AMG method.\nml = pyamg.smoothed_aggregation_solver(A)\n\n# 2. Use the multigrid hierarchy as a preconditioner for CG.\n#    The aspreconditioner() method returns a LinearOperator that\n#    applies one V-cycle.\nM = ml.aspreconditioner(cycle='V')\n\n# 3. Solve the preconditioned system\nx, info = cg(A, b, M=M, tol=1e-8)\n\nif info == 0:\n    print(\"CG with AMG preconditioner converged.\")\nelse:\n    print(f\"CG with AMG did not converge in {info} iterations.\")\n\n\n8.2 Domain Decomposition Methods\nDomain Decomposition (DD) methods are a family of techniques fundamentally designed to enable parallel computing for PDE problems. The guiding principle is “divide and conquer.” The large, global physical domain is partitioned into a set of smaller, simpler subdomains. The original PDE problem is then solved on these subdomains concurrently, with each subdomain typically assigned to a different processor.\nMathematical Structure and Rationale: The main challenge in DD methods is to correctly enforce the solution’s continuity and the PDE’s governing laws across the artificial interfaces created between subdomains. The independent subdomain solves are coordinated through an iterative process that exchanges information across these interfaces. Like multigrid, DD methods are most often used as preconditioners for Krylov solvers. The action of the preconditioner, \\(M^{-1}r\\), involves solving the independent problems on all subdomains in parallel, followed by a communication step to update the interface values.\nThere are two main classes of DD methods:\nOverlapping Schwarz Methods: In these methods, the subdomains are constructed to have a small region of overlap with their neighbors. The iterative process is simple and intuitive: solve the PDE on subdomain \\(\\Omega_i\\), use the resulting solution values in the overlap region as boundary conditions for the solve on the neighboring subdomain \\(\\Omega_j\\), and repeat this process until the solution across all interfaces converges.\nNon-overlapping Methods (Iterative Substructuring): Here, the subdomains intersect only at their boundaries (interfaces and corners). These methods are algebraically more complex. They reformulate the global problem into one that explicitly solves for the unknowns on the interfaces, coupled with independent solves in the interior of each subdomain. Prominent examples include the FETI (Finite Element Tearing and Interconnecting) methods, which use Lagrange multipliers to enforce continuity at the interfaces, and primal methods like BDDC (Balancing Domain Decomposition by Constraints).\nPython Snippet (Conceptual): A practical implementation of a DD method requires a parallel computing framework (like MPI) and is highly complex. The following conceptual code illustrates the logic of a two-domain additive Schwarz preconditioner.\n# Conceptual snippet for a two-domain Additive Schwarz preconditioner\n# A_i: matrix for interior of subdomain i\n# B_i: matrix coupling interior of i to interface\n# S: Schur complement matrix for the interface problem\n\ndef apply_additive_schwarz(r):\n    # r is the global residual vector\n\n    # 1. Restrict residual to each subdomain\n    r1 = R1 @ r  # R1 is a restriction operator\n    r2 = R2 @ r  # R2 is a restriction operator\n\n    # 2. Solve local problems on subdomains (in parallel)\n    # This is the core of the parallel computation\n    z1 = solve_subdomain_1(A1, r1)\n    z2 = solve_subdomain_2(A2, r2)\n\n    # 3. Solve a coarse/interface problem to get global correction\n    # This step requires communication\n    r_interface = R_interface @ r\n    z_interface = solve_interface(S, r_interface)\n\n    # 4. Prolongate local solutions back to global vector\n    z = P1 @ z1 + P2 @ z2 + P_interface @ z_interface\n    return z\nThe most powerful preconditioners, like Multigrid and Domain Decomposition, are not merely algebraic constructs. Their success derives from the fact that they create a simplified but physically meaningful approximation of the original problem. An ill-conditioned matrix from a PDE reflects strong coupling across different scales or spatial regions. Multigrid directly addresses the multi-scale nature of elliptic problems by explicitly representing the problem on a hierarchy of grids, handling local physics with the smoother and global physics with the coarse-grid correction. Similarly, Domain Decomposition respects the spatial locality of physical laws by solving the problem exactly within subdomains and then iteratively correcting for the coupling between them. This reveals a deep principle in modern computational science: the most effective algorithms are often those that are “problem-aware.” To achieve true scalability and efficiency, the solver must incorporate knowledge of the physical and geometric problem it is designed to solve."
  },
  {
    "objectID": "Blog/iterative_solvers.html#conclusion",
    "href": "Blog/iterative_solvers.html#conclusion",
    "title": "Iterative solvers for linear system of equations",
    "section": "Conclusion",
    "text": "Conclusion\nIterative methods represent a cornerstone of modern scientific computing, providing the only feasible path for solving the vast linear systems that arise from the discretization of partial differential equations. The journey from fundamental concepts to state-of-the-art application reveals a landscape of increasing sophistication, driven by the need to balance computational cost, memory usage, and robustness.\nAt the heart of modern techniques lie the Krylov subspace methods, with the Conjugate Gradient (CG) method providing an elegant and optimal solution for symmetric positive-definite systems, and the Generalized Minimal Residual (GMRES) method offering a robust, if more costly, alternative for general non-symmetric problems. The choice between them is dictated by the mathematical properties of the system matrix, which are, in turn, a direct reflection of the underlying PDE’s physical character—elliptic problems typically yield SPD matrices suited for CG, while parabolic and hyperbolic problems often lead to non-symmetric systems requiring GMRES.\nHowever, the practical power of these solvers is only fully unlocked through preconditioning. The challenge of ill-conditioning, where the difficulty of the algebraic problem increases dramatically with the physical model’s fidelity, necessitates the transformation of the original system into one with more favorable spectral properties. The spectrum of preconditioners—from simple algebraic techniques like Jacobi and Incomplete LU factorization to advanced, problem-aware strategies like Multigrid and Domain Decomposition—highlights a crucial theme: the most powerful numerical methods are those that are not “black boxes” but are intelligently designed to exploit the physical and geometric structure of the problem at hand. Multigrid’s hierarchical approach to error smoothing and Domain Decomposition’s parallel “divide and conquer” strategy are prime examples of this principle.\nFor the practitioner, this leads to a clear workflow: identify the PDE class, choose the appropriate Krylov solver (CG or GMRES), and, most critically, select a preconditioner that matches the problem’s complexity and the available computational resources. For the researcher, the field remains vibrant, with ongoing work in developing more robust preconditioners for challenging multi-physics problems, adapting algorithms for emerging computer architectures, and further blurring the lines between algebraic solvers and physical models. Ultimately, the continued advancement in iterative methods is a key enabler for pushing the boundaries of scientific discovery and engineering innovation."
  },
  {
    "objectID": "Blog/upgrade_viva.html#traditional-numerical-methods-and-their-limitations",
    "href": "Blog/upgrade_viva.html#traditional-numerical-methods-and-their-limitations",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Traditional Numerical Methods and their Limitations",
    "text": "Traditional Numerical Methods and their Limitations\nTraditional numerical methods such as finite difference [6], finite volume [7], finite element [8], and spectral methods [9], while mathematically rigorous, often require supercomputing resources and can take hours, days, or even weeks to deliver solutions [10], severely limiting their utility for iterative design exploration and real-time applications.\nConsider modelling magnetohydrodynamics (MHD) in fusion reactors: classical methods for multi-physics and multi-scale simulation are only accurate when flow features are smooth, requiring meshes to resolve the smallest features. For complex systems like fusion reactors or climate models, this makes direct numerical simulation computationally prohibitive. The traditional approach involves using simplified versions of the governing equations to allow coarser meshes, sacrificing accuracy for feasibility. This resolution-efficiency tradeoff presents a significant bottleneck to scientific progress."
  },
  {
    "objectID": "Blog/upgrade_viva.html#the-need-for-surrogates",
    "href": "Blog/upgrade_viva.html#the-need-for-surrogates",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "The Need for Surrogates",
    "text": "The Need for Surrogates\nThe quest to accelerate the solution of these equations has become increasingly urgent, particularly in domains like nuclear fusion, where rapid advancements are essential to meet ambitious energy production timelines. With insufficient time or funding to deliver commercial fusion through conventional ‘test-based’ development, our simulation capability must be transformed into an ‘actionable’ or predictive capability built upon state-of-the-art data+model hybrid simulation. This transformation necessitates a paradigm shift from traditional numerical solvers to more efficient computational approaches.\nNeural surrogate models have emerged as a promising alternative, offering the potential to reduce computational costs by several orders of magnitude [11, 12]. These models, trained on data generated from high-fidelity simulations or experimental observations, learn to emulate the PDE solutions across various initial conditions and parameter configurations. Recent advancements in neural operators, graph neural networks, and other deep learning architectures have demonstrated remarkable capabilities in capturing complex PDE dynamics while maintaining computational efficiency. For instance, neural PDE solvers in fusion applications have shown speedups of six orders of magnitude over traditional numerical solvers while maintaining acceptable accuracy levels [13].\n\n\n\n\n\n\nFigure 1: Neural PDE solvers use data from traditional numerical solvers to quickly approximate PDEs across various conditions (shown by black arrows). To ensure reliability, these models incorporate uncertainty quantification (UQ). If the predicted error exceeds a threshold \\(\\epsilon\\), the numerical solver is used and added to the training data; otherwise, predictions are used as output (shown by red arrows)."
  },
  {
    "objectID": "Blog/upgrade_viva.html#actionable-surrogate-models",
    "href": "Blog/upgrade_viva.html#actionable-surrogate-models",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Actionable Surrogate Models",
    "text": "Actionable Surrogate Models\nHowever, the transition from traditional numerical methods to neural surrogate models introduces its own set of challenges, particularly regarding the reliability and trustworthiness of these models. Unlike their numerical counterparts, which are built on well-established mathematical principles with known error bounds, neural surrogates may produce confident predictions without adequate uncertainty quantification (UQ), potentially leading to catastrophic consequences in safety-critical applications.\nAn actionable surrogate is one that can be confidently deployed in scientific and engineering workflows, with well-characterised uncertainties, physical consistency guarantees, and sufficient interpretability to enable scientific discovery and engineering design. Such models would allow “rapid screening and idea testing and real-time prediction-based experimental control and optimisation,” enabling scientists and engineers to explore more hypotheses and reach more robust conclusions. See Figure 2 for an overview.\nFor surrogate models to be truly actionable and adopted in industry and scientific research, several critical challenges must be addressed:\nPhysical Consistency: Machine learned models are often ignorant of fundamental laws of physics and can result in ill-posed problems or non-physical solutions. Physics-informed machine learning approaches, which integrate mathematical physics models with data-driven learning, are essential for ensuring physically consistent predictions.\nUncertainty Quantification: When replacing established physics-based models with surrogate models, quantifying the uncertainty in predictions is crucial. Modelling complex systems requires the inclusion and characterisation of uncertainties and errors that enter at various stages of the computational workflow.\n\n\n\n\n\n\nFigure 2: Four tenets of an Actionable Surrogate Model.\n\n\n\nInterpretability: Black-box surrogate models may achieve impressive performance but lack the interpretability required for scientific understanding and engineering decision-making.\nGeneralisability: Surrogate models trained on specific scenarios may not generalise well to out-of-distribution scenarios, limiting their broader applicability. Methods that enable surrogate models to extrapolate beyond their training distribution are crucial for real-world deployment. For a neural PDE, generalisability extends to different physics, geometries and boundary conditions."
  },
  {
    "objectID": "Blog/upgrade_viva.html#the-need-for-uncertainty-quantification",
    "href": "Blog/upgrade_viva.html#the-need-for-uncertainty-quantification",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "The Need for Uncertainty Quantification",
    "text": "The Need for Uncertainty Quantification\nThe challenge of uncertainty quantification (UQ) for neural differential equations is multifaceted. Traditional Bayesian approaches [14], while theoretically sound, often struggle with the high dimensionality and computational demands of complex PDEs. Alternative methods like dropout-based approaches [15] or ensemble techniques [16] may provide uncertainty estimates but lack statistical guarantees. Recent work on conformal prediction offers a promising avenue, providing distribution-free prediction intervals with guaranteed coverage properties [17]. However, extending these methods to high-dimensional spatio-temporal domains presents its own challenges, requiring careful consideration of exchangeability assumptions and computational efficiency."
  },
  {
    "objectID": "Blog/upgrade_viva.html#combining-experimental-and-simulation-data-for-scientific-discovery",
    "href": "Blog/upgrade_viva.html#combining-experimental-and-simulation-data-for-scientific-discovery",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Combining Experimental and Simulation Data for Scientific Discovery",
    "text": "Combining Experimental and Simulation Data for Scientific Discovery\nA persistent challenge in computational science and engineering is the “sim2real gap”—the discrepancy between simulation predictions and real-world observations. Traditional physics-based simulations, while mathematically rigorous, often rely on simplifying assumptions and idealised conditions that fail to capture the full complexity of physical systems. Actionable surrogate models offer a promising approach to bridge this gap by synergistically combining physics-based knowledge encoded in PDEs with experimental measurements.\nBy integrating the structured inductive biases of physical equations with the flexibility of data-driven learning, these models can account for phenomena that are difficult to model from first principles alone, such as material imperfections, boundary interactions, and emergent behaviours. In fusion applications, for example, a surrogate model could integrate theoretical plasma physics with experimental diagnostics to better predict anomalous transport phenomena that pure simulation struggles to capture. Similarly, in fluid dynamics, surrogate models can incorporate both Navier-Stokes principles and real-world measurements to account for complex turbulence effects across diverse flow regimes.\nThis hybrid approach not only improves prediction accuracy but also enhances model generalisability, allowing the surrogate to perform reliably in previously unseen conditions. Moreover, when equipped with proper uncertainty quantification, these models can identify where the sim2real gap is largest, guiding targeted experiments and model refinements in an iterative, scientifically principled manner. The ability to seamlessly assimilate both theoretical knowledge and experimental evidence positions actionable surrogate models as powerful tools for scientific discovery and engineering innovation, enabling more reliable extrapolation from limited data and accelerating the pathway from simulation to real-world application."
  },
  {
    "objectID": "Blog/upgrade_viva.html#building-digital-twins-using-surrogate-models",
    "href": "Blog/upgrade_viva.html#building-digital-twins-using-surrogate-models",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Building Digital Twins using Surrogate Models",
    "text": "Building Digital Twins using Surrogate Models\nThe development of actionable surrogate models is particularly critical for the creation of digital twins—virtual replicas of physical systems that can reliably simulate their behaviour in real-time. In fusion energy research, digital twins would enable researchers to rapidly explore plasma scenarios, optimise reactor designs, and predict maintenance needs without the enormous costs and delays of physical testing. For instance, a digital twin of a tokamak could predict plasma instabilities before they occur, allowing for preventive control measures that extend operation time and improve performance.\nIn weather and climate modelling, actionable surrogates can facilitate high-resolution simulations of coastal flooding, extreme weather events, and climate tipping points at speeds compatible with early warning systems and adaptive policy planning [18].\nEngineering applications such as aerospace design, materials development, and civil infrastructure benefit similarly—digital twins built with reliable surrogate models can predict structural fatigue, material degradation, and system performance throughout lifecycle phases, allowing for predictive maintenance and design optimisation with quantified confidence levels.\nIn all these domains, the transformative potential of digital twins can only be realised if the underlying surrogate models not only execute swiftly but also provide physically consistent predictions with well-characterised uncertainty bounds, enabling decision-makers to take appropriate actions based on simulation outcomes they can trust."
  },
  {
    "objectID": "Blog/upgrade_viva.html#summary",
    "href": "Blog/upgrade_viva.html#summary",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Summary",
    "text": "Summary\nAchieving this vision requires a concerted effort across multiple disciplines, bridging the gap between computational sciences, physics, and engineering. It necessitates not only technical advancements in neural network architectures and uncertainty quantification methods but also careful validation against experimental data and integration with domain knowledge [19]. The path forward involves developing surrogate models that are not just fast approximations but trusted partners in the scientific and engineering design process, capable of providing insights with quantified confidence levels suitable for high-consequence decision-making.\nThis report addresses the current state of the art and the pressing challenges in surrogate modelling and uncertainty quantification for neural differential equations, focusing on developing methods that maintain physical consistency while providing reliable uncertainty estimates. By advancing the state-of-the-art in this field, we aim to contribute to the broader goal of making computational simulations more accessible, reliable, and actionable across scientific and engineering disciplines, particularly in the journey toward commercial fusion energy, weather modelling and other grand challenges of our time."
  },
  {
    "objectID": "Blog/upgrade_viva.html#sec-objective",
    "href": "Blog/upgrade_viva.html#sec-objective",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "The Objective",
    "text": "The Objective\n\nPDE modelling\nConsider the arbitrary partial differential equation (PDE) modelling the spatio-temporal evolution of \\(n\\) field variables \\(u\\in \\mathbb{R}^n\\):\n\\[\n\\begin{align}\nD = D_t(u) + D_X(u) &= 0, \\quad X\\in\\Omega,\\; t\\in[0,T], \\label{eqn:pde} \\\\\nu(X,t) &= g(X,t), \\quad X\\in\\partial\\Omega \\; t\\in[0,T], \\label{eqn:bc}\\\\\nu(X,0) &= a(\\lambda, X) \\label{eqn:ic}.\n\\end{align}\n\\]\nHere, \\(X\\) defines the spatial domain bounded by \\(\\Omega\\) and defined with geometry \\(\\omega\\). The PDE is defined over a continuous temporal domain \\([0,T]\\) and \\(D_X\\) and \\(D_t\\), the composite operators of the associated spatial and temporal derivatives, including the PDE coefficients. The PDE is further defined by the boundary condition \\(g(X,t)\\) and the initial condition \\(a(\\lambda, X)\\), parameterised by \\(\\lambda\\). Our objective resembles the forward problem of solving a PDE setup as an initial-boundary value problem (ibvp). Mathematically, we need to perform the mapping defined below:\n\\[\n\\mathcal{U} : \\mathcal{P} \\rightarrow \\mathcal{U}. \\label{eq:sm}\n\\]\nHere, \\(\\mathcal{P}\\) is the space of all possible PDE problems, characterised by the operators \\(D_X, D_t\\), domain geometry \\(\\omega\\), boundary conditions \\(g\\), and initial conditions \\(a\\) and \\(\\mathcal{U}\\) is the space of all PDE solutions.\n\n\nNeural PDE Modelling\nWithin the context of neural PDE modelling, we aim to achieve our objective by devising a parameterised neural network \\(G(\\theta)\\) to serve as a surrogate model that can approximate the mapping defined within Equation ?@eq-sm:\n\\[\n\\mathcal{N}_\\theta \\approx \\mathcal{U}. \\label{eq:sm_no}\n\\]\nFor a given PDE problem \\(p \\in \\mathcal{P}\\), the surrogate model generates a prediction of the PDE solution \\(u\\):\n\\[\n\\mathcal{N}_\\theta(p)(X,t) \\approx u(X, t), \\label{eq:sm_no_p}\n\\]\nwhich approximates the true solution as defined in Equations ([eqn:pde?]), ([eqn:bc?]), ([eqn:ic?]).\n\n\nError Metrics and Constraints\nTo evaluate the performance of our surrogate model, we define error metrics that quantify both the accuracy of the approximation and its adherence to the underlying physical principles. The point-wise approximation error can be defined as:\n\\[\n\\epsilon(X,t) = \\|\\mathcal{U}(p)(X,t) - \\mathcal{N}_\\theta(p)(X,t)\\|.\\label{eq:point_error}\n\\]\nThe overall solution accuracy can be measured using integral norms:\n\\[\n\\mathcal{E}_{\\text{sol}} = \\left(\\int_{\\Omega}\\int_{0}^{T} \\|\\mathcal{U}(p)(X,t) - \\mathcal{N}_\\theta(p)(X,t)\\|^2 \\, dt \\, dX\\right)^{1/2}. \\label{eq:sol_error}\n\\]\nBeyond point-wise direct solution accuracy, we can define physics-informed error metrics that measure how well the surrogate model satisfies the governing equations and obeys the conservation laws:\n\\[\n\\mathcal{E}_{\\text{PDE}} = \\left(\\int_{\\Omega}\\int_{0}^{T} \\|D(\\mathcal{N}_\\theta(p)(X,t))\\|^2 \\, dt \\, dX\\right)^{1/2}. \\label{eq:pde_error}\n\\]\nHere, \\(D\\) represents the residual operator of the PDE (obtained by expressing ([eqn:pde?]) in it’s canonical form). This measures the deviation of the surrogate model prediction from the true physics when deployed to solve for a specific PDE solution. By formulating the PDE in its conservative form, ?@eq-pde_error becomes a measure of the deviation from the conservation laws for the approximate prediction of the neural PDE.\nTo ensure the surrogate model remains physically meaningful and sufficiently accurate, we impose the following constraints:\n\\[\n\\begin{align}\n\\mathcal{E}_{\\text{sol}} &\\leq \\epsilon_{\\text{tol}} \\label{eq:sol_tol}, \\\\\n\\mathcal{E}_{\\text{PDE}} &\\leq \\delta_{\\text{tol}} \\label{eq:pde_tol},\n\\end{align}\n\\]\nwhere \\(\\epsilon_{\\text{tol}}\\) and \\(\\delta_{\\text{tol}}\\) are user-defined, problem-specific tolerance thresholds for solution accuracy and PDE residual, respectively.\n\n\nUncertainty Quantification as a Fallback\nFor neural PDE based surrogate models to become truly actionable in high-stakes scientific and engineering applications, they must navigate a critical balance between error constraints and uncertainty quantification. Ideally, these models should simultaneously satisfy both solution accuracy \\((\\mathcal{E}_{\\text{sol}} \\leq \\epsilon_{\\text{tol}})\\) and physics-informed \\((\\mathcal{E}_{\\text{PDE}} \\leq \\delta_{\\text{tol}})\\) error constraints, ensuring predictions that are not only accurate compared to ground truth but also respect underlying physical laws; however, real-world complexity makes strict adherence to it challenging. This is more pronounced, particularly in regions with sparse training data, regime extrapolation, or complex multi-physics interactions. In those situations an actionable surrogate must transition from confident prediction to informed uncertainty quantification, providing calibrated estimates for both solution uncertainty and potential physics violations through spatiotemporal uncertainty maps. This allows us to precisely identify where the model might operate outside its error constraints, thereby enabling selective model refinement, adaptive sampling strategies, and hybrid approaches that blend surrogate predictions with traditional solvers in high-uncertainty regions, ultimately transforming these surrogates from mere computational accelerators to decision-support tools that engineers and scientists can confidently use to guide design exploration, determine when additional high-fidelity simulations are needed. This nuanced approach to error constraints and uncertainty quantification moves beyond the binary notion of either trusting or distrusting surrogate models. Helping make informed judgments for high-consequence decisions across applications from fusion energy research to climate modelling, creating a continuum of reliability that adapts to the specific domain, available data, and physical complexity while maintaining the scientific rigour necessary for addressing our era’s grand challenges."
  },
  {
    "objectID": "Blog/upgrade_viva.html#report-structure",
    "href": "Blog/upgrade_viva.html#report-structure",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Report Structure",
    "text": "Report Structure\nThis report is structured into 5 chapters: introduction, background, review of current work, future work and the research plan. For the background (Section 2), review and future work, we structure it into two sections: surrogate modelling and uncertainty quantification. For the background, we provide a literature review of the methods deployed to model PDEs and to provide error bounds. Within the review section, we provide an overview of our finished work in both modelling and validating surrogate models. While in the future work chapter, we outline the projects that we are hypothesising to be completed over the course of the PhD. The research plan provides a Gantt chart outlining the project timescales."
  },
  {
    "objectID": "Blog/upgrade_viva.html#surrogate-modelling",
    "href": "Blog/upgrade_viva.html#surrogate-modelling",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Surrogate Modelling",
    "text": "Surrogate Modelling\nAs defined in Section 1.8, surrogate modelling of PDEs involves learning to map the spatio-temporal evolution of physical variables within a defined domain. The data under consideration inherits certain implicit biases, such as causality, sequentiality and, depending on the physics under consideration, a range of invariances and equivariances. The surrogate model to serve as the neural PDE needs to be built considering this implicit bias within the data to solve an initial-boundary value problem as defined in Equations ([eqn:pde?]), ([eqn:bc?]), ([eqn:ic?]).\n\n\n\n\n\n\nFigure 3: Neural PDE landscape: Most surrogate models deployed within SciML frameworks exist along the given Physics-Data axis. On the left end, we have models driven explicitly by physical constraints with limited dependence on data. As we move further right along this axis, the model’s explicit knowledge of the physics fades, increasing the dependence on training data from which the model hopes to distill the physics dynamics under consideration.\n\n\n\nAn ideal surrogate model defined in (?@eq-sm_no_p), should be able to model any PDE(s), explore multiple boundary conditions, provide predictions that are continuous in space and time, and can adapt to any domain geometry all the while being within the tolerance limits as defined in (?@eq-sol_tol) and (?@eq-pde_tol). Though the current research landscape is yet to devise a method for constructing an ideal surrogate model, there have been significant developments in providing some of the above-mentioned features to the model [20].\nWe explore the existing literature under the guise of the physics-data tradeoff that each of these methods espouses. In Figure 3, we outline this physics-data axis along which different kinds of surrogate models lie. Along the left end of the axis, the models are physics-driven, relying on less data and more information about the PDE under consideration, in the form of residuals or conservation equations. The further right we move on the axis, the model becomes more data-driven, implicitly learning the physics of the PDE from simulation/experimental data. Though the primary focus will be on the capability to model the physics well, for each type of neural PDE, we will also be looking at its capability to model and handle multi-physics, predict across the space-time continuum for desired geometries while adapting to different boundary conditions.\n\nPhysics-Informed Neural Networks\nPhysics-Informed Neural Networks (PINNs) are a class of machine learning models that incorporate physical laws and domain knowledge directly into the neural network training process through the use of PDE residuals as soft constraints [21].\nGiven an arbitrary partial differential equation (PDE) system as given in ([eqn:pde?]):\n\\[D = D_t(u) + D_X(u) = 0, \\quad X\\in\\Omega,\\; t\\in[0,T],\\]\nwith boundary conditions \\(u(X,t) = g(X,t)\\) on \\(X\\in\\partial\\Omega\\) and initial conditions \\(u(X,0) = a(\\lambda, X)\\), a PINN represents the solution \\(u(X,t)\\) using a neural network \\(\\mathcal{N}_\\theta(X,t)\\) with parameters \\(\\theta\\). The key innovation of PINNs is the construction of a composite loss function that penalises:\n\nThe PDE residual: \\(\\mathcal{L}_{\\text{PDE}} = \\|D_t(\\mathcal{N}_\\theta) + D_X(\\mathcal{N}_\\theta)\\|^2_{\\Omega \\times [0,T]}\\)\nThe boundary condition mismatch: \\(\\mathcal{L}_{\\text{BC}} = \\|\\mathcal{N}_\\theta - g\\|^2_{\\partial\\Omega \\times [0,T]}\\)\nThe initial condition mismatch: \\(\\mathcal{L}_{\\text{IC}} = \\|\\mathcal{N}_\\theta(\\cdot,0) - a(\\lambda,\\cdot)\\|^2_{\\Omega}\\)\n\nThe total loss is a weighted sum of the PDE residual, the error with the initial and boundary conditions:\n\\[\n\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}},\n\\]\nleading to an unsupervised training objective:\n\\[\n\\theta^* = \\arg\\min_{\\theta} \\mathcal{L}(\\theta).\n\\]\nThis approach effectively embeds the physics described by the PDE system directly into the neural network training process. Traditionally, PINNs are devised as coordinate-based MLPs (implicit neural representations) that learn to map the space-time coordinates to physical field variables [22, 23]. PINNs serve as surrogates that approximate the mapping \\(\\mathcal{U}\\) from the PDE problem space to the solution space while inherently respecting the conservation laws encoded in the PDE formulation. Unlike traditional numerical methods, PINNs leverage automatic differentiation to evaluate the PDE residuals, eliminating the need for mesh generation and making them particularly advantageous for high-dimensional problems or complex geometries. PINNs have found application in a range of modelling tasks, ranging from fluid mechanics [24], molecular and material deformation [25] to magnetic control of fusion devices [26]. There exist various variants to PINNs that rely on modelling using a physics-driven approach. Deep Galerkin methods minimise a volumetric residual applied over a test function [27], Deep Ritz method where the loss is defined as the energy of the problem’s solution and minimised [28]. PINNs are also structured to solve the conservative form of the PDE [29], allowing for better representations of sources and sinks.\nLimitations: Though PINNs mathematically guarantee convergence to the solution, in practice, they often fail to meet this criterion. They suffer from the errors arising from the approximation, generalisation and optimisation found within neural networks [30]. Faced with a multi-objective optimisation task as given in the composite loss function, they fail to satisfy all objectives, imperative for a well-defined PDE [31]. They are often ill-conditioned, leading to a poorly defined loss landscape unable for gradient descent-based algorithms to traverse and identify a suitable minima [32].\nGeometry: Being inherently mesh-free in design, PINNs are capable of adapting to different geometries within the same spatio-temporal problem domain. However, due to the lack of an explicit definition of the domain geometry, they fail to model the boundary conditions imposed by complex geometries and need additional geometry information as inputs to aid the modelling [33].\nSpace-time Continuum: The mesh-free nature allows PINNs to be continuous in space and time, generating predictions that are smooth, infinitely differentiable solutions across the interested domain.\nMulti-Physics: PINNs generally struggle with complex multiscale and multiphysics settings as it adds more objectives to the loss function, increasing the complexity of the optimisation task in hand.\nBoundary Conditions: PINNs either encode the boundary condition as a soft-constraint in the loss or as a hard-constraint within the architecture [34, 35], however they remain fixed after training and fail to adapt to other boundary conditions when deployed.\n\nSparse/Coarse PINNs\nConsidering the challenge of optimisation for PINNs, when trained in an unsupervised setting, various works have proposed the utilisation of limited simulation/experimental data to help augment the loss landscape [30, 36]. In these works, the loss is modified to have a data element as well:\n\\[\n\\mathcal{L}_{\\text{total}} = \\lambda_{\\text{PDE}}\\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{BC}}\\mathcal{L}_{\\text{BC}} + \\lambda_{\\text{IC}}\\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{Data}}\\mathcal{L}_{\\text{Data}},\n\\]\nwhere \\(\\mathcal{L}_{\\text{Data}}\\) is the L2 norm taken across the model predictions and the target data. The target data could be obtained from sparse experimental data or simulation data generated on a coarser grid. Supplying data to the training objective transforms the optimisation from an unsupervised learning scenario to a hybrid supervised-unsupervised model, allowing the model to approximate the solution better. However, the optimal choice of the coefficients \\(\\lambda\\) within the loss functions still remains an open problem [37].\n\n\nLagrangian and Hamiltonian Neural Networks\nLagrangian and Hamiltonian Neural Networks are specialised machine learning architectures that incorporate fundamental principles from classical mechanics to model physical systems with improved accuracy and physical consistency.\nLagrangian Neural Networks (LNNs) learn to approximate the Lagrangian function \\((L = T - V\\), where \\(T\\) is kinetic energy and \\(V\\) is potential energy) of a physical system [38]. By learning this scalar function and applying the Euler-Lagrange equations through automatic differentiation, LNNs can predict system dynamics while naturally preserving conservation laws and symmetries. This approach is particularly effective for systems where conservation of energy and momentum are essential.\nLNNs learn the Lagrangian function and use the Euler-Lagrange equations:\n\\[\n\\mathcal{L}_{\\text{LNN}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\left\\| \\ddot{q}_i - \\frac{d}{dt}\\left(\\frac{\\partial L_\\theta}{\\partial \\dot{q}}\\right)_i + \\left(\\frac{\\partial L_\\theta}{\\partial q}\\right)_i \\right\\|^2\\right),\n\\]\nwhere \\(L_\\theta(q, \\dot{q})\\) is the neural network approximation of the Lagrangian with parameters \\(\\theta\\), \\(q\\) represents generalized coordinates and \\(\\dot{q}\\) their time derivatives. The Euler-Lagrange equation \\(\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0\\) governs the dynamics.\nHamiltonian Neural Networks (HNNs) instead learn the Hamiltonian function \\((H = T + V)\\) of a system. By learning this energy function and implementing Hamilton’s equations through automatic differentiation, HNNs explicitly preserve the symplectic structure of physical systems. This makes them exceptionally well-suited for modelling conservative dynamical systems and providing stable long-term predictions [39]. Both approaches offer significant advantages over conventional neural networks by embedding physical inductive biases directly into their architecture, resulting in models that require less training data and produce physically consistent predictions that respect fundamental conservation laws.\nHNNs learn the Hamiltonian function and use Hamilton’s equations:\n\\[\n\\mathcal{L}_{\\text{HNN}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\left\\| \\dot{q}_i - \\left(\\frac{\\partial H_\\theta}{\\partial p}\\right)_i \\right\\|^2 + \\left\\| \\dot{p}_i + \\left(\\frac{\\partial H_\\theta}{\\partial q}\\right)_i \\right\\|^2 \\right)\n\\]\nWhere: \\(H_\\theta(q, p)\\) is the neural network approximation of the Hamiltonian with parameters \\(\\theta\\), \\(q\\) represents generalized coordinates and \\(p\\) their conjugate momenta. Hamilton’s equations \\(\\dot{q} = \\frac{\\partial H}{\\partial p}\\) and \\(\\dot{p} = -\\frac{\\partial H}{\\partial q}\\) govern the dynamics.\nBoth LNNs and HNNs can incorporate additional regularization terms or constraints for specific physical properties as needed.\n\n\n\nEquivariant Neural Networks\nEquivariant Neural Networks (EqNNs) are a class of neural architectures designed to respect the underlying symmetries and invariances present in physical systems. By embedding these symmetries directly into the network architecture, EqNNs can provide more physically consistent solutions with improved sample efficiency compared to standard neural networks [40–43].\nLet \\(G\\) be a symmetry group acting on the input space \\(\\mathcal{X}\\) and output space \\(\\mathcal{Y}\\) via group actions \\(\\rho_{\\mathcal{X}}\\) and \\(\\rho_{\\mathcal{Y}}\\), respectively. A function \\(f: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) is said to be equivariant with respect to \\(G\\) if for all \\(g \\in G\\) and \\(x \\in \\mathcal{X}\\):\n\\[\nf(\\rho_{\\mathcal{X}}(g)x) = \\rho_{\\mathcal{Y}}(g)f(x)\n\\]\nIn the context of PDEs as described in ([eqn:pde?]), the equivariance property ensures that if the input data (e.g., initial conditions, boundary conditions) undergoes a transformation from group \\(G\\), the predicted solution transforms in a corresponding manner. For instance, if we rotate the boundary conditions of a fluid dynamics problem, an equivariant neural network would produce a solution that is exactly the rotated version of the solution for the original boundary conditions.\nEquivariant neural networks employ specialised architectural components that preserve group symmetries throughout the forward pass. Two main approaches have emerged for integrating symmetries into neural architectures [42]:\n\nGroup Convolutional Networks: These generalize standard convolutions to operate over a symmetry group \\(G\\), where the convolution operation naturally preserves equivariance [41]. Group convolution is defined for a compact group \\(G\\) with kernel \\(K: G \\rightarrow \\mathcal{L}(V_1, V_2)\\), feature function \\(f: G \\rightarrow V_1\\), and Haar measure \\(\\mu\\) on \\(G\\) as:\n\\[\n(K * f)(s) = \\int_G K(r^{-1}s)f(r)d\\mu(r)\n\\]\nThis operation can be extended to representative group convolutions that support non-regular representations, providing greater flexibility in modelling various symmetries [42].\nSteerable Convolutional Networks: These leverage steerable filters that transform predictably under group actions [44]. Tensor Field Networks [45] extend this approach to operate on geometric tensor fields to maintain SE(3) equivariance.\n\nFor solving PDEs, the loss function for an equivariant neural network \\(\\mathcal{E}_\\theta\\) could combine the standard PDE loss terms with equivariance constraints:\n$ \\[\\begin{align}\n\\mathcal{L}_{\\text{total}} &= \\mathcal{L}_{\\text{PDE}} + \\mathcal{L}_{\\text{BC}} + \\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{eq}}\\mathcal{L}_{\\text{eq}} \\\\\n\\mathcal{L}_{\\text{eq}} &= \\mathbb{E}_{g \\in G, x \\in \\mathcal{X}} \\left\\| \\mathcal{E}_\\theta(\\rho_{\\mathcal{X}}(g)x) - \\rho_{\\mathcal{Y}}(g)\\mathcal{E}_\\theta(x) \\right\\|^2\n\\end{align}\\] $\nwhere \\(\\lambda_{\\text{eq}}\\) balances the equivariance constraint against other loss terms. However, in practice, when the architecture is perfectly equivariant by construction, \\(\\mathcal{L}_{\\text{eq}}\\) becomes unnecessary, as equivariance is guaranteed for any parameter values \\(\\theta\\) [40, 42].\nA particularly powerful approach for solving PDEs with equivariant networks involves leveraging the theory of differential invariants [42]. For a PDE system with symmetry group \\(G\\), the differential operators can be expressed in terms of differential invariants:\n$ {i=1}^{k_t} a_i t^i u = F(^{G,1}{u,n}, , ^{G,k}{u,n}) $\nwhere \\(\\partial\\phi^{G,j}_{u,n}\\) are the differential invariants of the group \\(G\\). Equivariant neural networks can be trained to approximate these invariants, enabling the construction of symmetry-preserving discretisation schemes without the need to formally derive numerical invariantizations [42].\nEquivariant neural networks have demonstrated success in modelling various physical systems governed by PDEs. They have been used for turbulence modelling with symmetry preservation [46]. E(3)-equivariant mesh-based networks have shown superior performance in predicting turbulent flows by preserving rotational and translational symmetries [40]. Group-convolutional approaches have been successfully applied to 2D heat equations through SE(2) symmetry-preserving discretisations [42]. Spherical CNN architectures that respect the symmetries of the sphere have been applied to global weather prediction tasks [47, 48]. SE(3)-equivariant networks have been used to predict the molecular wave functions of atomistic systems, guaranteeing rotational and translational invariance of energy predictions [49]. Exploiting Lie point symmetries within the training data allows for further data augmentation, reducing sample complexity by an order of magnitude while maintaining physical consistency [50]. One of the most successful approaches is E(n)-Equivariant Graph Neural Networks [51], which combine the expressivity of graph neural networks with equivariance to Euclidean transformations, proving particularly effective for modelling particle systems and continuum mechanics.\nLimitations: Despite their theoretical advantages, equivariant neural networks face several challenges. They often require specialised implementations that can be computationally intensive, particularly for higher-dimensional groups or complex symmetries [52]. The formal derivation of numerical invariantization becomes increasingly challenging as the number of variables increases, limiting the applicability of these methods in high-dimensional settings [42]. The strict enforcement of symmetries may also limit the network’s expressivity in cases where the underlying system exhibits near-symmetries or broken symmetries [53].\nGeometry: Equivariant neural networks excel in handling complex geometries as they can naturally incorporate geometric transformations. For instance, SE(3)-equivariant networks can model rotationally invariant phenomena regardless of the orientation of the domain [45]. However, they may struggle with domains that have symmetry-breaking boundary features unless specifically designed to accommodate them [40]. For PDEs defined on non-Euclidean manifolds, specialised equivariant architectures have been developed to respect the underlying geometric structure [42].\nSpace-time Continuum: Depending on the architecture being deployed, equivariant networks can operate on continuous space-time domains or on fixed discrete domains. They often leverage positional encodings that respect the underlying symmetries of the space-time manifold [23].\nMulti-Physics: Equivariant neural networks are particularly well-suited for multi-physics problems where different physical phenomena share underlying symmetries. By enforcing these symmetries in the architecture, they can more efficiently learn the complex interactions between different physical processes [43]. An advantage is that once an equivariant architecture is trained to approximate the differential invariants of a specific symmetry group, it can be applied to solve any other PDE with the same symmetry group without retraining [42].\nBoundary Conditions: Enforcing boundary conditions while maintaining equivariance can be challenging, particularly for Dirichlet or Neumann conditions that may break certain symmetries. Various approaches have been proposed, including symmetry-preserving boundary constraint functions and augmented Lagrangian methods that incorporate boundary conditions as soft constraints while preserving the equivariant structure of the network [53]. For symmetry-preserving finite difference schemes, the formulation of boundary conditions in terms of differential invariants allows for the construction of discretisation schemes that respect both the PDEs and their boundary conditions [42].\n\n\nNeural Operators\nNeural Operators are a class of machine learning models that learns to approximate the operator that maps between function spaces from a finite collection of observed input-output pairs [54]. Given input functions from a Banach space \\(\\mathcal{A}\\) and output solutions in a Banach space \\(\\mathcal{U}\\), a neural operator learns the mapping:\n$ NO_{}: $\nThe core notion behind a neural operator is that it depends on discretisation-invariant functions rather than on discretised vectors. This is achieved by constructing the models to implement a method of kernel integration that takes the shape:\n$ ((a;)_t)(x) = _D (x, y, a(x), a(y) ; )_t(y)dy, $\nThe neural operator represents the non-local transformation of hidden representation \\(\\nu_t\\) to produce values at position \\(x\\). Here, \\(\\kappa\\) is a learnable kernel parameterised by \\(\\phi\\), allowing the model to capture relationships between input function values \\(a(x)\\) and \\(a(y)\\) across the entire domain \\(D\\). This kernel integration appears within the iterative layer update:\n$ _{t+1}(x) = (W_t_t(x) + ((a;)_t)(x) + b_t(x)), $\nwhere \\(\\sigma\\) is a nonlinear activation function, \\(W_t\\) represents a local linear transformation capturing point-wise features, and \\(b_t(x)\\) is a bias term. The complete neural operator architecture consists of an initial lifting operator \\(P\\) mapping input function \\(a(x) \\mapsto \\nu_0(x)\\), followed by \\(T\\) layers of these kernel integrations with nonlinearities, and concluding with a projection operator \\(Q\\) mapping \\(\\nu_T(x) \\mapsto u(x)\\) to produce the output function.\nThere exist different implementations of neural operators based on the method of kernel integration, each with its own set of advantages and disadvantages as outlined in Table 1.\n\n\n\nTable 1: Comparison of different neural operator architectures based on the kernel integration method, exploring the pros and cons of each.\n\n\n\n\n\n\n\n\n\n\n\nNeural Operator\nKernel Integration Method\nAdvantages\nDisadvantages\n\n\n\n\nGraph Neural Operator (GNO) [55]\nNyström approximation with domain truncation\n• Handles unstructured grids• Flexible domain geometry• Captures local structure efficiently\n• Performance depends on sampling quality• Truncation may lose long-range dependencies\n\n\nLow-rank Neural Operator (LNO) [54]\nTensor product decomposition\n• Linear complexity• Efficient for near-linear operators• Simple implementation\n• Limited expressivity• Struggles with highly non-linear operators• Rank must be chosen carefully\n\n\nMultipole Graph Neural Operator (MGNO) [56]\nMulti-scale decomposition with hierarchical matrices\n• Linear complexity• Captures multi-scale interactions• Works well on complex geometries\n• Complex implementation• Hierarchical structure design is problem-dependent• Higher overhead than other methods\n\n\nFourier Neural Operator (FNO) [11]\nSpectral convolution using FFT\n• Fast computation• Excellent for smooth functions• State-of-the-art performance on many PDEs\n• Limited to structured grids• Struggles with discontinuities• Periodic boundary conditions assumed\n\n\nDeepONet [57]\nBranch-trunk architecture with pointwise evaluation\n• Universal approximation guarantees• Can query at arbitrary output points• Theoretically well-founded\n• Not discretisation-invariant in basic form• Input function must be on fixed grid points• Linear approximation scheme\n\n\nImplicit Neural Representation (INR) [23]\nPoint-wise MLP with periodic activations\n• Continuous representation• Memory efficient• Resolution-independent\n• Not an operator (represents single function)• Slow training convergence• Struggles with high-frequency details\n\n\nConvolutional Neural Operator (CNO) [58]\nBandlimited convolution and continuous-discrete equivalence\n• Resolution invariant• Robust multi-scale processing• Excellent generalisation capability• Representation equivalence guarantees\n• Requires bandlimited function spaces• Higher implementation complexity• More expensive sinc filtering operations\n\n\nLaplace Neural Operator (LNO) [59]\nPole-residue Laplace domain transformation\n• Captures transient responses• Handles non-periodic signals• Physically interpretable parameters• Exponential convergence capabilities\n• Limited to specific applications• Complex parameter interpretation• More suitable for systems with poles and residues\n\n\nWavelet Neural Operator (WNO) [60]\nWavelet transform and kernel integration\n• Superior time-frequency localisation• Handles discontinuities effectively• Captures spatial patterns well• Works for complex boundaries\n• Choice of wavelet basis affects performance• Computational overhead for wavelet transform• More complex implementation\n\n\n\n\n\n\nNeural Operators have found application in modelling complex PDE across a range of domains from weather modelling [61–63], carbon monitoring [64, 65], fluid modelling [66, 67] and nuclear fusion [13, 68–70]. Neural Operators have been devised to formulate foundation physics models trained on a large PDE dataset with the aim to generalise to unseen physics [71–73].\nLimitations: Neural operators are only discretisation invariant in theory. In practice, because they are trained using discretised instances of the input functions and output solutions, they are rather discretisation-convergent [74, 75]. The extent of this convergence depends on the method of kernel integration deployed and on the aliasing errors that arise from the discretisation of the training data. Multigrid methods have demonstrated potential in addressing these aliasing errors [76]. Depending on the choice of kernel, these models can be relatively complex in operation and have a large memory footprint as well.\nSpace-time Continuum: As described above, neural operators allow for continuous predictions in the spatial domain, but are discrete in the temporal dimension as they are handled autoregressively [77, 78]. There have been some recent attempts to make them continuous in time, but they have been limited to toy experiments [79].\nGeometry: Certain neural operators are capable of handling irregular meshes and point cloud data [23, 55, 57], whereas most others are restricted to regular structured grids [11, 58–60].\nMulti-physics: Most neural operator usage has been for modelling a specific physics under consideration and they have failed to extend to a more generalisable understanding of the dynamics.\nBoundary Conditions: Within models like the FNO, the boundary conditions are expected to be periodic in nature. Other models operating on structured grids have the capability of handling different boundary conditions by modifying the type of convolution being deployed. These models do not exhibit any distinction of the PDE from its boundary conditions and once trained the boundary conditions are embedded to the dynamics distilled into the model, that they cannot be used to model the physics under different boundary conditions.\n\nPhysics-Informed Neural Operators\nPhysics-Informed Neural Operators (PINO) are a learning method for neural PDEs combining a neural operator architecture with the training objective of a PINN [67]. They are often used in a setting where a neural operator model trained on a general subset of simulation data is further finetuned to solve for a specific, well-defined PDE [80, 81] using the PDE loss alone. PINO models rely on using automatic differentiation or finite differences to evaluate the PDE residuals across the prediction [82]. In this work, message passing within a graph neural operator has been modified to behave as a finite volume method for modelling the dynamics of fluid flow [83].\nComposite neural operator models, constructed by combining several neural operator architectures, help bring the best out of each model as seen in this modified implicit neural representation [63]. By combining a GNO with an FNO, the model can handle irregular geometry and provides for efficient kernel integration [84].\n\n\n\nData-Driven Neural Networks\nSince the fundamental value of a surrogate model is to provide a quick, inexpensive approximation of the true function, traditional neural network architectures have a significant role to play in the landscape of neural PDE solvers. Through this part, we quickly touch upon fundamental machine learning models, their mathematical formulation, applications for PDE modelling and their limitations.\n\nMultilayer Perceptron (MLP)\nThe Multilayer Perceptron (MLP) is one of the foundational neural network architectures, consisting of at least three layers: an input layer, one or more hidden layers, and an output layer [85]. Each layer comprises multiple neurons (or nodes), with each neuron in a layer fully connected to all neurons in the subsequent layer. MLPs are feed-forward networks, meaning information flows only in one direction—from input to output—with no cycles or loops.\nGiven an input vector \\(\\mathbf{x} \\in \\mathbb{R}^{d_0}\\), an MLP with \\(L\\) layers computes:\n$ \\[\\begin{align}\n\\mathbf{h}_0 &= \\mathbf{x} \\\\\n\\mathbf{h}_\\ell &= \\sigma_\\ell(\\mathbf{W}_\\ell \\mathbf{h}_{\\ell-1} + \\mathbf{b}_\\ell), \\quad \\text{for } \\ell = 1, 2, \\ldots, L\n\\end{align}\\] $\nwhere \\(\\mathbf{h}_\\ell \\in \\mathbb{R}^{d_\\ell}\\) is the output of the \\(\\ell\\)-th layer, \\(\\mathbf{W}_\\ell \\in \\mathbb{R}^{d_\\ell \\times d_{\\ell-1}}\\) is the weight matrix for \\(\\ell\\)-th layer \\(\\mathbf{b}_\\ell \\in \\mathbb{R}^{d_\\ell}\\) is the bias vector for the \\(\\ell\\)-th layer \\(\\sigma_\\ell\\) is a non-linear activation function (e.g., ReLU, sigmoid, tanh). The final output \\(\\mathbf{y} = \\mathbf{h}_L\\) is used for prediction or classification. The network parameters \\(\\theta = \\{(\\mathbf{W}_\\ell, \\mathbf{b}_\\ell)\\}_{\\ell=1}^L\\) are learned by minimizing a loss function \\(\\mathcal{L}(\\mathbf{y}, \\mathbf{y}^*)\\) using gradient-based optimization methods, where \\(\\mathbf{y}^*\\) represents the ground truth labels. MLPs have been used as surrogates for modelling wind turbine blades [86], classifiers for high-energy physics [87], and for designing fusion reactors [88].\nLimitations: MLPs suffer from the curse of dimensionality, requiring exponentially more data as input dimensionality increases. They lack inherent mechanisms to handle sequential or spatial data structures, ignoring important local relationships in data like images or text. Despite their universal approximation properties, MLPs often require significant tuning of hyperparameters and are prone to overfitting on small datasets.\n\n\nRecurrent Neural Network (RNN)\nRecurrent Neural Networks (RNNs) are specialised neural architectures designed to process sequential data by maintaining an internal state (memory) that captures information from previous inputs. Unlike feed-forward networks, RNNs include feedback connections, allowing information to persist across time steps [89]. This recursive structure enables RNNs to model temporal dependencies in data, making them particularly suitable for tasks involving sequences of variable length.\nFor an input sequence \\(\\mathbf{x} = (\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_T)\\) where each \\(\\mathbf{x}_t \\in \\mathbb{R}^d\\), a simple RNN computes:\n$ \\[\\begin{align}\n\\mathbf{h}_t &= \\sigma_h(\\mathbf{W}_{hx}\\mathbf{x}_t + \\mathbf{W}_{hh}\\mathbf{h}_{t-1} + \\mathbf{b}_h) \\\\\n\\mathbf{y}_t &= \\sigma_y(\\mathbf{W}_{yh}\\mathbf{h}_t + \\mathbf{b}_y)\n\\end{align}\\] $\nwhere, \\(\\mathbf{h}_t \\in \\mathbb{R}^m\\) is the hidden state at time step \\(t\\), \\(\\mathbf{y}_t \\in \\mathbb{R}^n\\) is the output at time step \\(t\\), \\(\\mathbf{W}_{hx} \\in \\mathbb{R}^{m \\times d}\\) is the input-to-hidden weight matrix, \\(\\mathbf{W}_{hh} \\in \\mathbb{R}^{m \\times m}\\) is the hidden-to-hidden weight matrix, \\(\\mathbf{W}_{yh} \\in \\mathbb{R}^{n \\times m}\\) is the hidden-to-output weight matrix, \\(\\mathbf{b}_h \\in \\mathbb{R}^m\\) and \\(\\mathbf{b}_y \\in \\mathbb{R}^n\\) are bias vectors, \\(\\sigma_h\\) and \\(\\sigma_y\\) are activation functions.\nMore advanced RNN variants include Long Short-Term Memory (LSTM) networks [90] and Gated Recurrent Units (GRUs) [91], which introduce gating mechanisms to better control information flow and mitigate the vanishing gradient problem. RNNs and its variants have been used to better model the temporal dependencies of PDEs across a wide range of physical dynamical systems [92–94].\nLimitations: Standard RNNs suffer from the vanishing and exploding gradient problems, making them challenging to train on long sequences. Despite advancements like LSTMs and GRUs, RNNs still struggle to capture very long-range dependencies effectively. The sequential nature of RNN computation prevents parallelisation during training, resulting in slower convergence compared to architectures that allow for more parallel processing.\n\n\nConvolutional Neural Network (CNN)\nConvolutional Neural Networks (CNNs) are specialised neural architectures primarily designed for processing grid-like data, such as images. They incorporate three key principles: local receptive fields, shared weights, and spatial or temporal subsampling [95]. By leveraging these principles, CNNs automatically learn hierarchical representations of data, capturing low-level features (e.g., edges, textures) in early layers and more abstract, high-level features in deeper layers. The convolutional nature of these networks introduces translation invariance, making them robust to spatial shifts in the input data.\nFor a 2D input \\(\\mathbf{X} \\in \\mathbb{R}^{H \\times W \\times C}\\) (height, width, channels), a convolutional layer computes:\n$ \\[\\begin{align}\n\\mathbf{Z}_{i,j,k} &= \\sum_{m=0}^{F_H-1} \\sum_{n=0}^{F_W-1} \\sum_{c=0}^{C-1} \\mathbf{X}_{i\\cdot s + m, j \\cdot s + n, c} \\cdot \\mathbf{K}_{m,n,c,k} + \\mathbf{b}_k \\\\\n\\mathbf{A}_{i,j,k} &= \\sigma(\\mathbf{Z}_{i,j,k})\n\\end{align}\\] $\nwhere, \\(\\mathbf{Z}_{i,j,k}\\) is the pre-activation value at position \\((i,j)\\) for the \\(k\\)-th feature map, \\(\\mathbf{A}_{i,j,k}\\) is the activation value after applying non-linearity \\(\\sigma\\), \\(\\mathbf{K} \\in \\mathbb{R}^{F_H \\times F_W \\times C \\times K}\\) represents \\(K\\) convolutional filters of size \\(F_H \\times F_W \\times C\\), \\(\\mathbf{b} \\in \\mathbb{R}^K\\) is a bias vector, \\(s\\) is the stride parameter.\nA typical CNN architecture also includes pooling layers for downsampling:\n$ {i,j,k} = ({{i p + m, j p + n, k} m, n &lt; P}) $\nwhere \\(P\\) is the pool size and \\(p\\) is the pooling stride. Common pooling operations include max-pooling and average-pooling. The final layers of a CNN often consist of fully connected layers for classification or regression tasks, however U-Nets with a fully convolutional network have found tremendous applications in spatio-temporal modelling [96]. They have been used for modelling weather complex PDEs [97], nuclear fusion [98] and even to mimic a multigrid method for fluid modelling [99].\nLimitations: CNNs lack explicit mechanisms to model long-range dependencies in data, limiting their effectiveness on tasks requiring global context understanding. They typically require large amounts of labelled data for effective training, making them less suitable for domains with limited annotated examples. Despite their translation invariance, CNNs are not inherently invariant to other transformations such as rotation, scaling, or viewpoint changes without specific architectural modifications or data augmentation.\n\n\nVision Transformer (ViT)\nThe Vision Transformer (ViT) adapts the Transformer architecture, originally designed for natural language processing, to computer vision tasks. Unlike CNNs, which use convolution operations to process images, ViT treats an image as a sequence of patches and processes them using self-attention mechanisms [100]. This approach allows the model to capture global dependencies in the data without the inductive biases inherent in CNNs.\nGiven an input image \\(\\mathbf{X} \\in \\mathbb{R}^{H \\times W \\times C}\\), ViT first divides it into \\(N\\) non-overlapping patches \\(\\mathbf{x}_p \\in \\mathbb{R}^{P^2 \\cdot C}\\), where \\((P, P)\\) is the patch size. These patches are linearly projected to obtain patch embeddings:\n$ 0 = [_{}; _1; _2; ; _N] + {} $\nwhere, \\(\\mathbf{x}_{\\text{class}}\\) is a learnable classification token prepended to the sequence, \\(\\mathbf{E} \\in \\mathbb{R}^{D \\times (P^2 \\cdot C)}\\) is a learnable linear projection, \\(\\mathbf{E}_{\\text{pos}} \\in \\mathbb{R}^{(N+1) \\times D}\\) contains positional embeddings.\nThe Transformer encoder processes these embeddings through \\(L\\) layers of multi-head self-attention (MSA) and MLP blocks:\n$ \\[\\begin{align}\n\\mathbf{z}'_\\ell &= \\text{MSA}(\\text{LN}(\\mathbf{z}_{\\ell-1})) + \\mathbf{z}_{\\ell-1}, \\quad \\ell = 1 \\ldots L \\\\\n\\mathbf{z}_\\ell &= \\text{MLP}(\\text{LN}(\\mathbf{z}'_\\ell)) + \\mathbf{z}'_\\ell, \\quad \\ell = 1 \\ldots L\n\\end{align}\\] $\nwhere LN denotes layer normalization. The self-attention mechanism is defined as:\n$ (, , ) = () $\nwhere \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) are query, key, and value matrices. The multi-head attention extends this by projecting the queries, keys, and values \\(h\\) times with different learned projections. Transformers have found application in PDE solving [71, 101, 102] and weather modelling [103].\nLimitations: Vision Transformers require large datasets for training from scratch, often performing worse than CNNs when trained on smaller datasets without strong regularization. The quadratic complexity of self-attention mechanisms with respect to sequence length makes ViTs computationally expensive for high-resolution images. Despite their strong performance, ViTs lack some of the useful inductive biases of CNNs, such as translation equivariance and locality."
  },
  {
    "objectID": "Blog/upgrade_viva.html#uncertainty-quantification",
    "href": "Blog/upgrade_viva.html#uncertainty-quantification",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "Uncertainty Quantification",
    "text": "Uncertainty Quantification\nThough uncertainty quantification for neural networks is a thesis by itself, we focus primarily on three broad categories that find consistent application or are emerging methods within the field of scientific machine learning. As given in Figure 4, we categorise UQ methods into three categories: Bayesian, frequentist and verified methods. Within each category, we focus on three submethods that find application for modelling PDEs and other dynamical systems. The key features, advantages and disadvantages of each method are outlined in Table 2. We dive deeper into the formulation, application and limitations of each of the methods in the sections below.\n\n\n\n\n\n\nFigure 4: High-level landscape of various UQ methods available for Neural PDEs\n\n\n\n\n\n\nTable 2: Comparison of Uncertainty Quantification Methods for Neural Networks\n\n\n\n\n\n\n\n\n\n\n\nUQ Method\nKey Features\nAdvantages\nDisadvantages\n\n\n\n\nBayesian Methods\n\n\n\n\n\nBayesian Neural Networks [104]\nTreats weights as random variables • Learns posterior distribution • Various inference techniques (VI, HMC)\nPrincipled uncertainty representation • Captures aleatoric and epistemic uncertainty • Natural incorporation of priors\nHigh computational cost • Challenging to scale • Often requires approximations • Fails to guarantee coverage\n\n\nMonte Carlo Dropout [15]\nDropout during inference • Multiple forward passes • Approximates VI in infinite width networks\nSimple to implement • Works with existing architectures • Minimal training overhead\nLimited expressiveness • Dropout rate affects quality • Often underestimates uncertainty\n\n\nDeep Ensembles [16]\nMultiple networks with different initializations • Can use adversarial training • Combined at inference\n• Captures multi-modal uncertainties • Robust to distribution shift\nHigh training cost • Memory intensive • Multiple model maintenance • Noisy and fails to guarantee coverage\n\n\nFrequentist Methods\n\n\n\n\n\nEvidential Networks [105]\nPredicts parameters of conjugate priors • Second-order uncertainty • End-to-end training\nSingle forward pass at inference • Uncertainty in uncertainty • Aleatoric/epistemic separation\nConstrained by prior family • Potential overconfidence • Careful loss design needed\n\n\nConformal Prediction [106]\nDistribution-free intervals • Guarantees coverage probability • Can be applied post-hoc\nRigorous statistical guarantees • Model-agnostic • Calibrates any prediction model\nRequires calibration set • May produce wide intervals • Limited for OOD data\n\n\nBootstrap Ensembles [107]\nResamples training data • Training on different subsets • Frequentist posterior analog\nNo distributional assumptions • Simple implementation • Parallelizable training\nComputationally expensive • Data inefficient • May underestimate uncertainty\n\n\nVerified Methods\n\n\n\n\n\nInterval Arithmetic [108]\nPropagates interval bounds • Guaranteed to contain true output • Maintains bounds per operation\nMathematically rigorous • Complete verification • Conceptually simple\nOften produces loose bounds • Dependency problem • Limited scalability\n\n\nTaylor Models [109]\nPolynomial approximation with error bounds • Higher-order tracking\nTighter bounds than intervals • Captures dependencies • Handles nonlinearities well\nComplex implementation • Expensive for higher order • Inefficient for deep networks\n\n\nZonotopes [110]\nLinear transformations of hypercubes • Affine set representation • Specialized for ReLU networks\nFormal guarantees with rigorous bounds • Exact for affine layers • Capture non-convex output distributions\nApproximation for nonlinearities • Memory grows with depth • Computing exact bounds expensive for high dimensions\n\n\n\n\n\n\n\nBayesian Methods\n\nBayesian Neural Networks (BNNs)\nBayesian Neural Networks (BNNs) extend traditional neural networks by treating weights as probability distributions rather than point estimates, providing a principled framework for uncertainty quantification in regression tasks. Unlike deterministic networks that output a single predicted value for each input, BNNs produce predictive distributions that capture both the expected value and the associated uncertainty, enabling more robust predictions in regression problems where quantifying confidence is crucial, such as in scientific and engineering applications where safety considerations or decision-making under uncertainty are paramount.\nIn standard regression neural networks, weights \\(\\mathbf{w}\\) are optimised as point estimates to minimise a loss function (typically mean squared error):\n$ {} (, ) = {} _{i=1}^{N} (y_i - f(_i; ))^2 $\nwhere \\(\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{N}\\) represents the training data, and \\(f(\\mathbf{x}; \\mathbf{w})\\) is the neural network function. BNNs, however, place a prior distribution \\(p(\\mathbf{w})\\) over the weights (often Gaussian) and learn a posterior distribution \\(p(\\mathbf{w}|\\mathcal{D})\\) via Bayes’ rule:\n$ p(|) = $\nFor regression, the likelihood \\(p(\\mathcal{D}|\\mathbf{w})\\) is typically modeled as Gaussian:\n$ p(|) = _{i=1}^{N} (y_i | f(_i; ), ^2) $\nwhere \\(\\sigma^2\\) represents the observation noise variance. Predictions for new inputs \\(\\mathbf{x}^*\\) are made by marginalising over the posterior to obtain a predictive distribution:\n$ p(y^|, ) = p(y^|, )p(|)d $\nThis integral is typically approximated either via variational inference (by minimising the KL divergence between an approximate distribution \\(q_\\theta(\\mathbf{w})\\) and the true posterior) or using Monte Carlo methods to draw samples from the posterior.\nIn scientific regression problems, BNNs have been successfully employed for uncertainty-aware surrogate modelling of complex physical systems, providing probabilistic approximations to solutions of partial differential equations with quantified confidence bounds [19, 111, 112]. BNNs have been deployed to provide UQ for PINNs [113], Physics-informed priors have allowed for better posterior estimation in BNNs [114] and have found utility in PDE discovery [115]. Multifidelity BNNs have found success in modelling PDEs in data-constraint settings [116].\nLimitations: Computational complexity remains a significant challenge, as training BNNs through variational inference or MCMC methods requires substantially more computation than their deterministic counterparts, with this cost scaling poorly for deep architectures with millions of parameters. The specification of appropriate priors over weights significantly impacts the quality of uncertainty estimates but lacks principled guidelines beyond conjugate Gaussian priors, which may not adequately capture the true weight distribution in complex neural networks. Furthermore, posterior approximation methods introduce additional limitations: variational inference tends to underestimate predictive uncertainty due to its mode-seeking behavior, while more accurate MCMC methods struggle with convergence in high-dimensional parameter spaces typical of neural networks. Finally, even well-trained BNNs often exhibit calibration issues in regression tasks, where predictive distributions may be systematically too narrow or too wide relative to the true data distribution, requiring additional post-processing calibration techniques to obtain reliable uncertainty estimates for critical applications.\n\n\nMonte Carlo Dropout\nMonte Carlo (MC) Dropout represents a practical approximation to Bayesian inference in neural networks, reinterpreting the common regularisation technique of dropout as a variational Bayesian approximation when applied during both training and inference. This approach enables uncertainty quantification in regression tasks without the full computational burden of traditional Bayesian methods, leveraging the insight that a neural network with dropout applied at test time can be viewed as an ensemble of thinned networks sharing parameters, thus providing a computationally efficient way to approximate prediction uncertainty through the statistical properties of multiple stochastic forward passes.\nIn standard dropout training, network units are randomly masked during training with probability \\(p\\) to prevent co-adaptation and improve generalisation:\n$ = f(; ) $\nwhere \\(\\mathbf{z}\\) is a binary mask with elements drawn from a Bernoulli distribution with probability \\(1-p\\) of being 1, and \\(\\odot\\) represents element-wise multiplication. It was shown that applying dropout at test time and performing multiple forward passes can be interpreted as an approximation to variational inference in a Bayesian neural network with a specific variational distribution \\(q_\\theta(\\mathbf{W})\\) over the weights [15]:\n$ q_(_i) = i ([z_{i,j}]{j=1}^{K_i}) $\nwhere \\(\\mathbf{M}_i\\) are variational parameters and \\(z_{i,j} \\sim \\text{Bernoulli}(1-p)\\). For regression tasks, the predictive mean and variance for a new input \\(\\mathbf{x}^*\\) can be approximated using \\(T\\) stochastic forward passes:\n$ \\[\\begin{align}\n\\mathbb{E}[y^* | \\mathbf{x}^*] &\\approx \\frac{1}{T}\\sum_{t=1}^{T} f(\\mathbf{x}^*; \\mathbf{W}_t) \\\\\n\\text{Var}[y^* | \\mathbf{x}^*] &\\approx \\tau^{-1} + \\frac{1}{T}\\sum_{t=1}^{T} f(\\mathbf{x}^*; \\mathbf{W}_t)^2 - \\left(\\frac{1}{T}\\sum_{t=1}^{T} f(\\mathbf{x}^*; \\mathbf{W}_t)\\right)^2\n\\end{align}\\] $\nwhere \\(\\mathbf{W}_t\\) represents the network weights with dropout applied for the \\(t\\)-th forward pass and \\(\\tau\\) is the model precision (inverse of the observation noise variance). This formulation decomposes the total predictive uncertainty into aleatoric uncertainty (data noise, \\(\\tau^{-1}\\)) and epistemic uncertainty (model uncertainty, captured by the variance of predictions across multiple forward passes).\nIn physics-informed neural networks, MC Dropout has found utility in searching for more efficient collocation points within the domain for more efficient convergence [117]. It has been used to provide UQ across spatio-temporal forecasting found within PDE modelling [118] and for getting error bounds for dynamics modelling [119].\nLimitations: Despite its practical appeal and widespread adoption for uncertainty quantification in regression tasks, MC Dropout exhibits several important limitations. The uncertainty estimates provided by MC Dropout tend to be uncalibrated without additional post-processing, often underestimating true predictive uncertainty compared to full Bayesian treatments, particularly for out-of-distribution inputs where overconfidence can be problematic. The method’s theoretical connection to Bayesian inference relies on restrictive assumptions about the form of approximate posterior distributions, constraining the expressiveness of the captured uncertainty. Additionally, the quality of uncertainty estimates depends critically on dropout rate and network architecture choices, with no systematic way to determine optimal values for these hyperparameters beyond heuristics or expensive cross-validation. MC Dropout also faces computational efficiency concerns during inference, as multiple forward passes (typically 50-100) are required to obtain stable uncertainty estimates, significantly increasing prediction latency in time-sensitive applications. Finally, recent research suggests that MC Dropout may fail to capture important modes of the posterior distribution, particularly in complex regression scenarios with multimodal solutions, as the implicit ensemble created by dropout tends to produce similar network behaviours rather than truly diverse function approximations that reflect the full range of plausible models given the data [120].\n\n\nDeep Ensembles\nDeep Ensembles represent an approximate Bayesian approach to uncertainty quantification in neural networks, leveraging the diversity of multiple independently trained networks to capture predictive uncertainty in regression tasks. This method constructs an ensemble of neural networks with identical architectures but different random initializations and trained on potentially different bootstrap samples of the data, providing a practical and scalable alternative to Bayesian methods while demonstrating competitive or superior uncertainty quantification performance, particularly for out-of-distribution detection and in capturing epistemic uncertainty arising from model misspecification or data scarcity [16].\nIn the Deep Ensembles framework for regression, each network in the ensemble directly outputs both the predicted mean \\(\\mu_\\theta(\\mathbf{x})\\) and variance \\(\\sigma^2_\\theta(\\mathbf{x})\\) of a Gaussian distribution for each input \\(\\mathbf{x}\\):\n$ p(y|, ) = (y|(), ^2()) $\nwhere \\(\\theta\\) represents the parameters of a neural network. The networks are trained by minimizing the negative log-likelihood (NLL) loss:\n$ () = ^2_() + $\nGiven an ensemble of \\(M\\) networks with parameters \\(\\{\\theta_1, \\theta_2, ..., \\theta_M\\}\\), the predictive distribution for a new input \\(\\mathbf{x}^*\\) is approximated as a mixture of Gaussians:\n$ p(y^|^) _{m=1}^{M} (y^*|_{_m}(^*), ^2_{_m}(^*)) $\nThe predictive mean and variance of this mixture distribution are given by:\n$ \\[\\begin{align}\n\\mathbb{E}[y^*|\\mathbf{x}^*] &= \\frac{1}{M}\\sum_{m=1}^{M} \\mu_{\\theta_m}(\\mathbf{x}^*) \\\\\n\\text{Var}[y^*|\\mathbf{x}^*] &= \\frac{1}{M}\\sum_{m=1}^{M} \\sigma^2_{\\theta_m}(\\mathbf{x}^*) + \\frac{1}{M}\\sum_{m=1}^{M} \\mu_{\\theta_m}(\\mathbf{x}^*)^2 - \\left(\\frac{1}{M}\\sum_{m=1}^{M} \\mu_{\\theta_m}(\\mathbf{x}^*)\\right)^2\n\\end{align}\\] $\nThe first term represents aleatoric uncertainty (average of predicted variances), while the second term captures epistemic uncertainty (variance of predicted means), thus providing a decomposition of the total predictive uncertainty into these two fundamental components.\nDeep ensembles have found utility in providing UQ for surrogates from a range of scenarios, ranging from computational fluid dynamics [121], electromagnetics [122], weather modelling [123] and nuclear fusion [124]. The method has also proven valuable in multi-fidelity modelling scenarios, where it helps quantify uncertainty when transferring knowledge between simulation scales or between simulations and experimental data, and in active learning frameworks for neural PDEs where uncertainty estimates guide the adaptive sampling of new simulation points to efficiently explore parameter spaces in computational physics problems [122, 125].\nLimitations: The computational cost of training and storing multiple independent neural networks represents a significant drawback, scaling linearly with ensemble size and becoming prohibitive for large models or resource-constrained environments, though techniques like snapshot ensembles or batch ensemble training partially mitigate this issue [126, 127]. Deep Ensembles lack the principled Bayesian foundation for uncertainty quantification, potentially missing certain types of uncertainties that would be captured by a proper posterior approximation, particularly when prior knowledge should constrain the solution space. The method’s effectiveness depends critically on achieving sufficient diversity among ensemble members, which can be challenging when training converges to similar local minima despite different initialisations, limiting the ensemble’s ability to explore multiple modes of the solution space in complex regression problems. Furthermore, determining the optimal ensemble size involves a trade-off between computational resources and uncertainty estimation quality, with no theoretical guidance beyond empirical validation. Finally, while Deep Ensembles often demonstrate good empirical performance for out-of-distribution detection, they can still produce overconfident predictions in certain far-from-training regions where all ensemble members make similar errors, as each network is trained to minimize the same objective function and may share similar inductive biases that do not reflect the true predictive uncertainty in these regions [128].\n\n\n\nFrequentist Methods\n\nEvidential Networks\nEvidential Neural Networks (ENNs) represent a class of models that directly estimate prediction uncertainty without requiring sampling or ensemble techniques. They formulate learning as an evidence acquisition process, where training examples add support to an evidential distribution over model parameters or predictions, resulting in a single-forward-pass approach to uncertainty quantification that is computationally efficient during inference.\nThe core principle of evidential regression is to place evidential priors over the likelihood function parameters. For a regression task with target \\(y\\), traditional neural networks predict a Gaussian likelihood with parameters \\(\\theta = \\{\\mu, \\sigma^2\\}\\). In contrast, ENNs model a Normal-Inverse-Gamma (NIG) distribution as an evidential prior over these parameters:\n$ p(, ^2 | , , , ) = ()^{+1}{-} $\nHere, a neural network outputs the evidential parameters \\(\\{\\gamma, \\upsilon, \\alpha, \\beta\\}\\). From these parameters, we can directly compute:\n\nPrediction: \\(\\mathbb{E}[\\mu] = \\gamma\\)\nAleatoric uncertainty: \\(\\mathbb{E}[\\sigma^2] = \\frac{\\beta}{\\alpha-1}\\)\nEpistemic uncertainty: \\(\\text{Var}[\\mu] = \\frac{\\beta}{\\upsilon(\\alpha-1)}\\)\n\nThe loss function typically includes both a negative log-likelihood term and a regularisation term \\(\\lambda|\\gamma - y_i|\\Phi\\) where \\(\\Phi = 2\\upsilon + \\alpha\\) represents the total evidence, penalising overconfidence in incorrect predictions.\nEvidential regression has been successfully applied in scientific machine learning contexts, including molecular property prediction for drug discovery [129], fault diagnosis in mechanical systems [130], and surrogate modelling of PDEs [131]. In PDE solving, ENNs provide valuable uncertainty quantification that helps identify solution regions where predictions may be unreliable due to complex dynamics, boundary conditions, or insufficient training data, allowing researchers to make informed decisions about model trustworthiness without the computational overhead of sampling-based approaches.\nLimitations: Despite their advantages, evidential neural networks face limitations including sensitivity to the regularisation coefficient \\(\\lambda\\), potential underestimation of uncertainty in high-dimensional spaces, and degraded calibration in transfer learning scenarios. Additionally, the theoretical foundations are less established than fully Bayesian approaches. ENNs may still produce overconfident predictions in some adversarial settings or under significant distribution shifts, requiring careful validation against traditional UQ methods for safety-critical applications.\n\n\nConformal Prediction\nConformal Prediction (CP) is a distribution-free uncertainty quantification framework that constructs prediction intervals with rigorous coverage guarantees. Unlike Bayesian or ensemble methods that often rely on distributional assumptions, CP provides valid statistical guarantees for finite samples without requiring model or distribution assumptions.\nGiven i.i.d. samples \\((X_i, Y_i) \\sim P\\), \\(i = 1, \\ldots, n\\), where \\(X_i \\in \\mathcal{X}\\) are inputs and \\(Y_i \\in \\mathbb{R}\\) are real-valued outputs, the goal is to construct a prediction interval \\(\\hat{C}_n(x)\\) for a new input \\(x\\) such that:\n$ P(Y_{n+1} n(X{n+1}) | (X_i, Y_i)_{i=1}^n) - $\nwhere \\((X_{n+1}, Y_{n+1})\\) is a new data point and \\(\\alpha \\in (0,1)\\) is a user-specified error rate. In regression settings, the split conformal algorithm operates as follows: (1) divide data into training and calibration sets; (2) train a regression model \\(\\hat{f}\\) on the training set; (3) define a nonconformity score function, typically \\(s(x,y) = |y - \\hat{f}(x)|\\); (4) compute scores on calibration data: \\(s_i = s(X_i, Y_i)\\); (5) calculate the \\((1-\\alpha)\\)-quantile \\(\\hat{q} = \\text{Quantile}(s_1,...,s_n; \\lceil(n+1)(1-\\alpha)\\rceil/n)\\); and (6) for a new input \\(x\\), construct the prediction interval \\(\\hat{C}(x) = [\\hat{f}(x) - \\hat{q}, \\hat{f}(x) + \\hat{q}]\\).\nIn scientific machine learning and PDE solving, CP has demonstrated considerable utility for regression tasks requiring reliable uncertainty estimates. CP has found utility in providing UQ for dynamical systems [132], PINNs [133], DeepONets [134] as well as functional calibration for operator learning [135].\nLimitations: CP provides only marginal coverage guarantees rather than conditional coverage, meaning the prediction intervals may be unnecessarily wide in some regions of the input space and too narrow in others. The method requires a separate calibration dataset which reduces available training data, and the quality of the prediction intervals heavily depends on the underlying regression model’s accuracy—poor models lead to uniformly wide intervals with limited practical utility. For time series or sequential regression problems with distribution drift, standard CP may fail to maintain coverage unless properly modified to account for temporal dependencies using techniques like weighted conformal prediction, which introduces additional complexity and hyperparameters that require careful tuning.\n\n\nBootstrapping Ensembles\nBootstrapping ensembles represent a practical non-parametric approach to uncertainty quantification (UQ) in neural network regression by training multiple models on resampled datasets, leveraging the statistical principle of bootstrapping to estimate predictive uncertainty without requiring modifications to the underlying network architecture or regression loss function, making it particularly attractive for regression tasks where quantifying prediction confidence intervals is essential.\nGiven a regression dataset \\(\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N\\) where \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) are input features and \\(y_i \\in \\mathbb{R}\\) are continuous target values, the bootstrap ensemble approach operates as follows:\n\nCreate \\(M\\) bootstrap datasets \\(\\{\\mathcal{D}_m\\}_{m=1}^M\\), where each \\(\\mathcal{D}_m\\) is generated by sampling \\(N\\) points from \\(\\mathcal{D}\\) with replacement\nTrain \\(M\\) identical regression models \\(\\{f_{\\theta_m}(\\mathbf{x})\\}_{m=1}^M\\), each on a different bootstrap dataset \\(\\mathcal{D}_m\\)\nFor a test point \\(\\mathbf{x}_*\\), compute:\n\nMean: \\(\\mu(\\mathbf{x}_*) = \\frac{1}{M}\\sum_{m=1}^M f_{\\theta_m}(\\mathbf{x}_*)\\)\nVariance: \\(\\sigma^2(\\mathbf{x}_*) = \\frac{1}{M}\\sum_{m=1}^M (f_{\\theta_m}(\\mathbf{x}_*) - \\mu(\\mathbf{x}_*))^2\\)\n\n\nwhere \\(\\mu(\\mathbf{x}_*)\\) represents the ensemble’s predicted value at \\(\\mathbf{x}_*\\), and \\(\\sigma^2(\\mathbf{x}_*)\\) quantifies the epistemic uncertainty (model uncertainty) in the regression prediction, enabling the construction of confidence intervals as \\(\\mu(\\mathbf{x}_*) \\pm z_{\\alpha/2} \\cdot \\sigma(\\mathbf{x}_*)\\), with \\(z_{\\alpha/2}\\) being the critical value for the desired confidence level.\nIn scientific regression contexts, bootstrapping ensembles have demonstrated significant utility across multiple domains including: modelling epidemic breakouts [136], predicting outputs of photovoltaic cells [137], and providing uncertainty for financial derivatives modelling [138].\nLimitations: The computational cost scales linearly with ensemble size, requiring training and storing multiple regression models; they primarily capture epistemic uncertainty from data sampling variations but may inadequately represent aleatoric uncertainty inherent in noisy regression data; small or imbalanced datasets may lead to unreliable uncertainty estimates due to insufficient sampling diversity; individual ensemble members may converge to different local minima in complex regression landscapes, potentially overestimating predictive uncertainty; ensemble variance often requires post-hoc calibration to provide statistically valid confidence intervals for regression predictions, particularly in extrapolation regions; and the method assumes independent and identically distributed data points, which may not hold for spatiotemporal regression problems with complex correlation structures.\n\n\n\nVerified Methods\n\nInterval Arithmetic\nInterval arithmetic (IA) provides rigorous uncertainty bounds for neural networks by replacing standard floating-point operations with interval operations that track lower and upper bounds. For regression tasks, IA propagates intervals through each layer, ensuring that true function values always remain contained within the computed intervals, thereby offering guaranteed error bounds on predictions without requiring statistical assumptions [108].\nThe core principle involves representing values as intervals \\([a,b]\\) where \\(a \\leq b\\), and defining operations that maintain correctness:\n$ \\[\\begin{align}\n[a,b] + [c,d] &= [a+c, b+d] \\\\\n[a,b] - [c,d] &= [a-d, b-c] \\\\\n[a,b] \\times [c,d] &= [\\min(ac,ad,bc,bd), \\max(ac,ad,bc,bd)] \\\\\n[a,b] \\; /\\ [c,d] &= [a,b] \\times [1/d,1/c] \\text{ when } 0 \\not\\in [c,d]\n\\end{align}\\] $\nFor neural networks with ReLU activations, Interval Bound Propagation (IBP) extends these principles to matrix operations:\n$ \\[\\begin{align}\n\\mathbf{z}^{(l)} &= \\mathbf{W}^{(l)}\\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} \\\\\n\\mathbf{z}^{(l)}_{\\text{lower}} &= \\mathbf{W}^{(l)}_+\\mathbf{a}^{(l-1)}_{\\text{lower}} + \\mathbf{W}^{(l)}_-\\mathbf{a}^{(l-1)}_{\\text{upper}} + \\mathbf{b}^{(l)} \\\\\n\\mathbf{z}^{(l)}_{\\text{upper}} &= \\mathbf{W}^{(l)}_+\\mathbf{a}^{(l-1)}_{\\text{upper}} + \\mathbf{W}^{(l)}_-\\mathbf{a}^{(l-1)}_{\\text{lower}} + \\mathbf{b}^{(l)} \\\\\n\\mathbf{a}^{(l)} &= \\text{ReLU}(\\mathbf{z}^{(l)}) \\\\\n\\mathbf{a}^{(l)}_{\\text{lower}} &= \\text{ReLU}(\\mathbf{z}^{(l)}_{\\text{lower}}) \\\\\n\\mathbf{a}^{(l)}_{\\text{upper}} &= \\text{ReLU}(\\mathbf{z}^{(l)}_{\\text{upper}})\n\\end{align}\\] $\nwhere \\(\\mathbf{W}^{(l)}_+\\) and \\(\\mathbf{W}^{(l)}_-\\) denote the positive and negative components of the weight matrix respectively.\nIn regression contexts, IA has been applied to provide verified bounds on neural network approximations of complex functions, partial differential equations, and dynamical systems [139, 140]. They have found utility in propagating input uncertainty across safety-critical systems [141] and short-term windspeed prediction [142].\nLimitations: The primary limitation of interval arithmetic for neural networks is the dependency problem, where multiple occurrences of the same variable are treated as independent, leading to overestimation of bounds that grows exponentially with network depth and width. For regression tasks, this manifests as excessively conservative uncertainty intervals that may be too wide for practical use. Fundamental theoretical limits, established in [143], demonstrate that certain simple specifications cannot be proven using interval analysis alone, regardless of network architecture. These limitations have motivated advanced methods that maintain correlations between variables, such as affine arithmetic and zonotopes, though these come with increased computational complexity.\n\n\nTaylor Models\nTaylor models provide a rigorous approach to quantifying uncertainty in neural networks by representing functions as a multivariate polynomial plus a bounded remainder interval. This approach combines interval arithmetic with symbolic computation to track functional dependencies accurately, making it particularly powerful for regression tasks where guaranteed bounds on neural network outputs are required [109].\nA \\(d\\)-dimensional Taylor model of order \\(k\\) is defined as a tuple \\(T = (p, \\Delta, D)\\) where \\(p = (p_1, \\ldots, p_d)^T\\) is a vector of multivariate polynomials \\(p_i: D \\rightarrow \\mathbb{R}\\) of degree at most \\(k\\), \\(\\Delta = \\Delta_1 \\times \\cdots \\times \\Delta_d\\) is a hyperrectangle containing the origin that bounds the remainder, and \\(D \\subseteq \\mathbb{R}^d\\) is the domain. For a neural network function \\(f\\), a Taylor model ensures that \\(f(x) \\in p_n(x) + \\Delta\\) for all \\(x \\in D\\), providing guaranteed bounds on the network’s predictions that account for approximation errors.\nIn regression contexts, Taylor models enable rigorous uncertainty quantification in neural networks by providing guaranteed error bounds rather than probabilistic estimates. They have been successfully applied to validate neural surrogate models for physical systems, verify neural network controllers in safety-critical applications, and certify the behaviour of physics-informed neural networks solving partial differential equations. Recent advances in [144] allow for automatically computing sharp bounds on the Taylor remainder series, making this approach increasingly practical for complex neural architectures.\nLimitations: The primary challenges of Taylor models for neural network uncertainty quantification include: computational complexity that scales exponentially with input dimension, limiting applicability to high-dimensional problems; the wrapping effect where dependencies between variables are lost during operations, leading to overestimation when propagating through deep networks; difficulty in handling non-polynomial activation functions which require additional approximation steps; and significant computational overhead compared to non-verified approaches such as Bayesian neural networks or dropout. Recent work integrating Taylor models with zonotopes [145] has addressed some of these limitations by preserving dependencies across control cycles in neural network verification tasks.\n\n\nZonotopes\nZonotope-based methods provide formal guarantees for neural network uncertainty quantification through geometric set representations that can efficiently capture non-convex output distributions. These methods compute tight enclosures of the regression function outputs by abstracting input-output relations using polynomial approximations, making them particularly suitable for safety-critical applications where guaranteed bounds on predictions are essential [146].\nA zonotope \\(Z \\subset \\mathbb{R}^n\\) is defined as:\n$ Z = {c + _{i=1}^p _i g_i _i } $\nwhere \\(c \\in \\mathbb{R}^n\\) is the centre vector and \\(g_i \\in \\mathbb{R}^n\\) are the generator vectors that define the geometry of the set. For neural network regression, polynomial zonotopes extend this representation to capture more complex non-linear behaviours:\n$ PZ = {c + {i=1}^h ({k=1}^p k^{E(k,i)})G{(:,i)} + _{j=1}^q j G{I(:,j)} _k, _j } $\nHere, \\(G \\in \\mathbb{R}^{n \\times h}\\) represents dependent generators, \\(G_I \\in \\mathbb{R}^{n \\times q}\\) represents independent generators, and \\(E \\in \\mathbb{N}_0^{p \\times h}\\) is an exponent matrix. This formulation allows polynomial zonotopes to be closed under polynomial maps, enabling precise propagation through neural networks with various activation functions [147].\nPolynomial zonotopes have demonstrated significant advantages in regression tasks for scientific computing, particularly for providing guaranteed confidence bands around predictions in critical domains [148]. They excel in quantifying uncertainty for surrogate models like Fourier Neural Operators, solving differential equations such as Burger’s equation, where both aleatoric and epistemic uncertainties need guaranteed bounds [149]. They have also found utility in verifying Lagrangian neural networks [150]. The method allows efficient propagation of input uncertainties through complex neural network architectures while preserving dependencies between inputs and outputs, which is crucial for multi-dimensional regression problems where correlation structures should be maintained.\nLimitations: The primary limitations of zonotope-based uncertainty quantification methods involve computational scalability with increasing network depth. The representation size of polynomial zonotopes grows substantially when propagated through deep networks, necessitating periodic order reduction that can introduce additional over-approximation. Computing tight bounds on regression outputs becomes computationally expensive for high-dimensional problems, requiring approximation techniques that balance precision and efficiency. Recent progress has shown that this can be circumvented by using dimensionality reduction techniques such as singular value decomposition [149]. While the approach excels at capturing non-convex behaviours in regression outputs, it still requires considerable expertise to tune parameters controlling the trade-off between computational efficiency and tightness of the uncertainty bounds, particularly when applying the method to complex scientific regression tasks with highly non-linear behaviours."
  },
  {
    "objectID": "Blog/upgrade_viva.html#references",
    "href": "Blog/upgrade_viva.html#references",
    "title": "Uncertainty Quantification for Surrogate Models of Partial Differential Equations",
    "section": "References",
    "text": "References\n\n\n1. OpenFOAM Foundation (2025) OpenFOAM: The open source CFD toolbox\n\n\n2. Giudicelli G, Lindsay A, Harbour L, et al (2024) 3.0 - MOOSE: Enabling massively parallel multiphysics simulations. SoftwareX 26:101690. https://doi.org/https://doi.org/10.1016/j.softx.2024.101690\n\n\n3. Hoelzl M, Huijsmans GTA, Pamela SJP, et al (2021) The JOREK non-linear extended MHD code and applications to large-scale instabilities and their control in magnetically confined fusion plasmas. Nuclear Fusion 61(6):065001. https://doi.org/10.1088/1741-4326/abf99f\n\n\n4. Danabasoglu G, Lamarque J-F, Bacmeister J, et al (2020) The community earth system model version 2 (CESM2). Journal of Advances in Modeling Earth Systems 12(2):e2019MS001916. https://doi.org/https://doi.org/10.1029/2019MS001916\n\n\n5. Horwitz JAK (2024) Estimating the carbon footprint of computational fluid dynamics. Physics of Fluids 36(4):045109. https://doi.org/10.1063/5.0199350\n\n\n6. Smith GD (1985) Numerical solution of partial differential equations: Finite difference methods, 3rd ed. Oxford University Press, Oxford\n\n\n7. Versteeg HK, Malalasekera W (1995) An introduction to computational fluid dynamics: The finite volume method. Addison-Wesley, Reading, MA\n\n\n8. Reddy JN (2006) Introduction to the finite element method, 3rd ed. McGraw-Hill Education, New York\n\n\n9. Fornberg B (1996) A practical guide to pseudospectral methods. Cambridge University Press, Cambridge, UK\n\n\n10. Lavin A, Krakauer D, Zenil H, et al (2021) Simulation intelligence: Towards a new generation of scientific methods. arXiv:211203235. https://doi.org/10.48550/ARXIV.2112.03235\n\n\n11. Li Z, Kovachki NB, Azizzadenesheli K, et al (2021) Fourier neural operator for parametric partial differential equations. In: International conference on learning representations\n\n\n12. Pfaff T, Fortunato M, Sanchez-Gonzalez A, Battaglia P (2021) Learning mesh-based simulation with graph networks. In: International conference on learning representations\n\n\n13. Gopakumar V, Pamela S, Zanisi L, et al (2024) Plasma surrogate modelling using fourier neural operators. Nuclear Fusion 64(5):056025. https://doi.org/10.1088/1741-4326/ad313a\n\n\n14. Abdar M, Pourpanah F, Hussain S, et al (2021) A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information Fusion 76:243–297. https://doi.org/https://doi.org/10.1016/j.inffus.2021.05.008\n\n\n15. Gal Y, Ghahramani Z (2016) Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In: International conference on machine learning\n\n\n16. Lakshminarayanan B, Pritzel A, Blundell C (2017) Simple and scalable predictive uncertainty estimation using deep ensembles\n\n\n17. Gopakumar V, Gray A, Oskarsson J, et al (2024) Uncertainty quantification of surrogate models using conformal prediction\n\n\n18. Bodnar C, Bruinsma W, Lucic A, et al (2024) Aurora: A foundation model of the atmosphere. Microsoft Research AI for Science\n\n\n19. Psaros AF, Meng X, Zou Z, Guo L, Karniadakis GE (2023) Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons. Journal of Computational Physics 477:111902. https://doi.org/https://doi.org/10.1016/j.jcp.2022.111902\n\n\n20. Karniadakis GE, Kevrekidis IG, Lu L, Perdikaris P, Wang S, Yang L (2021) Physics-informed machine learning. Nature Reviews Physics 3(6):422–440. https://doi.org/10.1038/s42254-021-00314-5\n\n\n21. Raissi M, Perdikaris P, Karniadakis GE (2019) Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics 378:686–707. https://doi.org/https://doi.org/10.1016/j.jcp.2018.10.045\n\n\n22. Chen Z, Zhang H (2019) Learning implicit fields for generative shape modeling\n\n\n23. Sitzmann V, Martel J, Bergman A, Lindell D, Wetzstein G (2020) Implicit neural representations with periodic activation functions. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 7462–7473\n\n\n24. Cuomo S, Di Cola VS, Giampaolo F, Rozza G, Raissi M, Piccialli F (2022) Scientific machine learning through physics–informed neural networks: Where we are and what’s next. Journal of Scientific Computing 92(3):88. https://doi.org/10.1007/s10915-022-01939-z\n\n\n25. Cai S, Mao Z, Wang Z, Yin M, Karniadakis GE (2021) Physics-informed neural networks (PINNs) for fluid mechanics: A review. Acta Mechanica Sinica 37(12):1727–1738. https://doi.org/10.1007/s10409-021-01148-1\n\n\n26. Jang B, Kaptanoglu AA, Gaur R, Pan S, Landreman M, Dorland W (2024) Grad–shafranov equilibria via data-free physics informed neural networks. Physics of Plasmas 31(3):032510. https://doi.org/10.1063/5.0188634\n\n\n27. Sirignano J, Spiliopoulos K (2018) DGM: A deep learning algorithm for solving partial differential equations. Journal of Computational Physics 375:1339–1364. https://doi.org/10.1016/j.jcp.2018.08.029\n\n\n28. E W, Yu B (2018) The deep ritz method: A deep learning-based numerical algorithm for solving variational problems. Communications in Mathematics and Statistics 6(1):1–12. https://doi.org/10.1007/s40304-018-0127-z\n\n\n29. Jagtap AD, Kharazmi E, Karniadakis GE (2020) Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems. Computer Methods in Applied Mechanics and Engineering 365:113028. https://doi.org/https://doi.org/10.1016/j.cma.2020.113028\n\n\n30. Gopakumar V, Pamela S, Samaddar D (2023) Loss landscape engineering via data regulation on PINNs. Machine Learning with Applications 12:100464. https://doi.org/https://doi.org/10.1016/j.mlwa.2023.100464\n\n\n31. Wang S, Yu X, Perdikaris P (2020) When and why PINNs fail to train: A neural tangent kernel perspective\n\n\n32. Rathore P, Lei W, Frangella Z, Lu L, Udell M (2024) Challenges in training PINNs: A loss landscape perspective\n\n\n33. Sahli Costabal F, Pezzuto S, Perdikaris P (2024) \\(\\Delta\\)-PINNs: Physics-informed neural networks on complex geometries. Engineering Applications of Artificial Intelligence 127:107324. https://doi.org/10.1016/j.engappai.2023.107324\n\n\n34. Zhu Y, Zabaras N, Koutsourelakis P-S, Perdikaris P (2019) Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computational Physics 394:56–81. https://doi.org/https://doi.org/10.1016/j.jcp.2019.05.024\n\n\n35. Sun L, Gao H, Pan S, Wang J-X (2020) Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data. Computer Methods in Applied Mechanics and Engineering 361:112732. https://doi.org/https://doi.org/10.1016/j.cma.2019.112732\n\n\n36. Mathews A, Francisquez M, Hughes JW, Hatch DR, Zhu B, Rogers BN (2021) Uncovering turbulent plasma dynamics via deep learning from partial observations. Phys Rev E 104:025205. https://doi.org/10.1103/PhysRevE.104.025205\n\n\n37. Jin X, Cai S, Li H, Karniadakis GE (2021) NSFnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics 426:109951. https://doi.org/https://doi.org/10.1016/j.jcp.2020.109951\n\n\n38. Cranmer M, Greydanus S, Hoyer S, Battaglia P, Spergel D, Ho S (2020) Lagrangian neural networks\n\n\n39. Greydanus S, Dzamba M, Yosinski J (2019) Hamiltonian neural networks\n\n\n40. Brandstetter J, Worrall DE, Welling M (2022) Message passing neural PDE solvers. In: International conference on learning representations\n\n\n41. Cohen TS, Welling M (2016) Group equivariant convolutional networks\n\n\n42. Lagrave P-Y, Tron E (2022) Equivariant neural networks and differential invariants theory for solving partial differential equations. Physical Sciences Forum 5(1). https://doi.org/10.3390/psf2022005013\n\n\n43. Knigge DM, Wessels D, Valperga R, et al (2024) Space-time continuous PDE forecasting using equivariant neural fields. In: The thirty-eighth annual conference on neural information processing systems\n\n\n44. Weiler M, Geiger M, Welling M, Boomsma W, Cohen T (2018) 3D steerable CNNs: Learning rotationally equivariant features in volumetric data\n\n\n45. Thomas N, Smidt T, Kearnes S, et al (2018) Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds\n\n\n46. Kurz M, Beck A, Sanderse B (2025) Harnessing equivariance: Modeling turbulence with graph neural networks\n\n\n47. Cohen TS, Geiger M, Koehler J, Welling M (2018) Spherical CNNs\n\n\n48. Weyn JA, Durran DR, Caruana R (2020) Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere. Journal of Advances in Modeling Earth Systems 12(9):e2020MS002109. https://doi.org/https://doi.org/10.1029/2020MS002109\n\n\n49. Unke OT, Bogojeski M, Gastegger M, Geiger M, Smidt T, Müller K-R (2021) SE(3)-equivariant prediction of molecular wavefunctions and electronic densities\n\n\n50. Brandstetter J, Welling M, Worrall DE (2022) Lie point symmetry data augmentation for neural PDE solvers\n\n\n51. Satorras VG, Hoogeboom E, Welling M (2022) E(n) equivariant graph neural networks\n\n\n52. Bronstein MM, Bruna J, Cohen T, Veličković P (2021) Geometric deep learning: Grids, groups, graphs, geodesics, and gauges\n\n\n53. Finzi M, Stanton S, Izmailov P, Wilson AG (2020) Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In: III HD, Singh A (eds) Proceedings of the 37th international conference on machine learning. PMLR, pp 3165–3176\n\n\n54. Kovachki N, Li Z, Liu B, et al (2023) Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research 24(89):1–97\n\n\n55. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Neural operator: Graph kernel network for partial differential equations\n\n\n56. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Multipole graph neural operator for parametric partial differential equations\n\n\n57. Lu L, Jin P, Pang G, Zhang Z, Karniadakis GE (2021) Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nature Machine Intelligence 3(3):218–229. https://doi.org/10.1038/s42256-021-00302-5\n\n\n58. Raonić B, Molinaro R, Ryck TD, et al (2023) Convolutional neural operators for robust and accurate learning of PDEs\n\n\n59. Cao Q, Goswami S, Karniadakis GE (2023) LNO: Laplace neural operator for solving differential equations\n\n\n60. Tripura T, Chakraborty S (2023) Wavelet neural operator for solving parametric partial differential equations in computational mechanics problems. Computer Methods in Applied Mechanics and Engineering 404:115783. https://doi.org/https://doi.org/10.1016/j.cma.2022.115783\n\n\n61. Lam R, Sanchez-Gonzalez A, Willson M, et al (2023) Learning skillful medium-range global weather forecasting. Science 382(6677):1416–1421. https://doi.org/10.1126/science.adi2336\n\n\n62. Kurth T, Subramanian S, Harrington P, et al (2023) FourCastNet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators. In: Proceedings of the platform for advanced scientific computing conference. Association for Computing Machinery, New York, NY, USA\n\n\n63. Serrano L, Boudec LL, Koupaı̈ AK, et al (2023) Operator learning with neural fields: Tackling PDEs on general geometries. In: Thirty-seventh conference on neural information processing systems\n\n\n64. Bedi S, Tiwari K, A. P. P, Kota SH, Krishnan NMA (2025) A neural operator for forecasting carbon monoxide evolution in cities. npj Clean Air 1(1):2. https://doi.org/10.1038/s44407-024-00002-5\n\n\n65. Wen G, Li Z, Long Q, Azizzadenesheli K, Anandkumar A, Benson SM (2023) Real-time high-resolution CO\\(_2\\) geological storage prediction using nested fourier neural operators. Energy & Environmental Science 16(4):1732–1741. https://doi.org/10.1039/d2ee04204e\n\n\n66. Shukla K, Oommen V, Peyvan A, et al (2024) Deep neural operators as accurate surrogates for shape optimization. Engineering Applications of Artificial Intelligence 129:107615. https://doi.org/https://doi.org/10.1016/j.engappai.2023.107615\n\n\n67. Li Z, Zheng H, Kovachki N, et al (2024) Physics-informed neural operator for learning partial differential equations. ACM / IMS J Data Sci 1(3). https://doi.org/10.1145/3648506\n\n\n68. Gopakumar V, Pamela S, Zanisi L, Li Z, Anandkumar A, Team M (2023) Fourier neural operator for plasma modelling\n\n\n69. Carey N, Zanisi L, Pamela S, et al (2025) Data efficiency and long-term prediction capabilities for neural operator surrogate models of edge plasma simulations\n\n\n70. Pamela SJP, Carey N, Brandstetter J, et al (2024) Neural-parareal: Dynamically training neural operators as coarse solvers for time-parallelisation of fusion MHD simulations\n\n\n71. Herde M, Raonić B, Rohner T, et al (2024) Poseidon: Efficient foundation models for PDEs\n\n\n72. Rahman MA, George RJ, Elleithy M, et al (2024) Pretraining codomain attention neural operators for solving multiphysics PDEs. Advances in Neural Information Processing Systems 37\n\n\n73. Alkin B, Fürst A, Schmid SL, Gruber L, Holzleitner M, Brandstetter J (2024) Universal physics transformers: A framework for efficiently scaling neural operators. In: The thirty-eighth annual conference on neural information processing systems\n\n\n74. Lanthaler S, Stuart AM, Trautner M (2024) Discretization error of fourier neural operators\n\n\n75. Bartolucci F, Bezenac E de, Raonic B, Molinaro R, Mishra S, Alaifari R (2023) Representation equivalent neural operators: A framework for alias-free operator learning. In: Thirty-seventh conference on neural information processing systems\n\n\n76. Kossaifi J, Kovachki N, Azizzadenesheli K, Anandkumar A (2023) Multi-grid tensorized fourier neural operator for high-resolution PDEs\n\n\n77. Lee YYR (2023) Autoregressive renaissance in neural PDE solvers\n\n\n78. Koehler F, Niedermayr S, Westermann R, Thuerey N (2024) APEBench: A benchmark for autoregressive neural emulators of PDEs\n\n\n79. Zhou A, Farimani AB (2024) Predicting change, not states: An alternate framework for neural PDE surrogates\n\n\n80. Rosofsky SG, Al Majed H, Huerta EA (2023) Applications of physics informed neural operators. Machine Learning: Science and Technology 4(2):025022. https://doi.org/10.1088/2632-2153/acd168\n\n\n81. Goswami S, Bora A, Yu Y, Karniadakis GE (2022) Physics-informed deep neural operator networks\n\n\n82. White C, Berner J, Kossaifi J, et al (2023) Physics-informed neural operators with exact differentiation on arbitrary geometries. In: The symbiosis of deep learning and differential equations III\n\n\n83. Li T, Zou S, Chang X, Zhang L, Deng X (2024) Predicting unsteady incompressible fluid dynamics with finite volume informed neural network. Physics of Fluids 36(4). https://doi.org/10.1063/5.0197425\n\n\n84. Li Z, Kovachki NB, Choy C, et al (2023) Geometry-informed neural operator for large-scale 3D PDEs. In: Thirty-seventh conference on neural information processing systems\n\n\n85. Haykin S (1994) Neural networks: A comprehensive foundation. Prentice Hall PTR\n\n\n86. Lalonde ER, Vischschraper B, Bitsuamlak G, Dai K (2021) Comparison of neural network types and architectures for generating a surrogate aerodynamic wind turbine blade model. Journal of Wind Engineering and Industrial Aerodynamics 216:104696. https://doi.org/https://doi.org/10.1016/j.jweia.2021.104696\n\n\n87. Baldi P, Cranmer K, Faucett T, Sadowski P, Whiteson D (2016) Parameterized neural networks for high-energy physics. The European Physical Journal C 76(5):235. https://doi.org/10.1140/epjc/s10052-016-4099-4\n\n\n88. Mánek P, Goffrier GV, Gopakumar V, Nikolaou N, Shimwell J, Waldmann I (2023) Fast regression of the tritium breeding ratio in fusion reactors. Machine Learning: Science and Technology 4(1):015008. https://doi.org/10.1088/2632-2153/acb2b3\n\n\n89. Rumelhart DE, Hinton GE, Williams RJ (1986) Learning internal representations by error propagation. In: Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1: foundations\n\n\n90. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Computation 9(8):1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735\n\n\n91. Cho K, Merrienboer B van, Gulcehre C, et al (2014) Learning phrase representations using RNN encoder-decoder for statistical machine translation\n\n\n92. Gopakumar V, Pamela S, Zanisi L (2023) Fourier-RNNs for modelling noisy physics data\n\n\n93. Hu Y, Zhao T, Xu S, Xu Z, Lin L (2022) Neural-PDE: A RNN based neural network for solving time dependent PDEs\n\n\n94. Saha P, Dash S, Mukhopadhyay S (2021) Physics-incorporated convolutional recurrent neural networks for source identification and forecasting of dynamical systems\n\n\n95. Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet classification with deep convolutional neural networks. In: Pereira F, Burges CJ, Bottou L, Weinberger KQ (eds) Advances in neural information processing systems. Curran Associates, Inc.\n\n\n96. Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for biomedical image segmentation. In: Navab N, Hornegger J, Wells WM, Frangi AF (eds) Medical image computing and computer-assisted intervention – MICCAI 2015. Springer International Publishing, Cham, pp 234–241\n\n\n97. Gupta JK, Brandstetter J (2023) Towards multi-spatiotemporal-scale generalized PDE modeling. Transactions on Machine Learning Research\n\n\n98. Gopakumar V, Samaddar D (2020) Image mapping the temporal evolution of edge characteristics in tokamaks using neural networks. Machine Learning: Science and Technology 1(1):015006. https://doi.org/10.1088/2632-2153/ab5639\n\n\n99. Le QT, Ooi C (2021) Surrogate modeling of fluid dynamics with a multigrid inspired neural network architecture. Machine Learning with Applications 6:100176. https://doi.org/https://doi.org/10.1016/j.mlwa.2021.100176\n\n\n100. Dosovitskiy A, Beyer L, Kolesnikov A, et al (2021) An image is worth 16x16 words: Transformers for image recognition at scale\n\n\n101. Li Z, Meidani K, Farimani AB (2023) Transformer for partial differential equations’ operator learning. Transactions on Machine Learning Research\n\n\n102. McCabe M, Blancard BR-S, Parker L, et al (2023) Multiple physics pretraining for physical surrogate models. In: NeurIPS 2023 AI for science workshop\n\n\n103. Nguyen T, Brandstetter J, Kapoor A, Gupta JK, Grover A (2023) ClimaX: A foundation model for weather and climate\n\n\n104. MacKay DJC (1992) A practical bayesian framework for backpropagation networks. Neural Computation 4(3):448–472. https://doi.org/10.1162/neco.1992.4.3.448\n\n\n105. Amini A, Schwarting W, Soleimany A, Rus D (2020) Deep evidential regression\n\n\n106. Vovk V, Gammerman A, Shafer G (2005) Algorithmic learning in a random world. Springer\n\n\n107. Breiman L (1996) Bagging predictors. Machine Learning 24(2):123–140. https://doi.org/10.1007/BF00058655\n\n\n108. Hickey T, Ju Q, Van Emden MH (2001) Interval arithmetic: From principles to implementation. J ACM 48(5):1038–1068. https://doi.org/10.1145/502102.502106\n\n\n109. Makino K, Berz M (2003) TAYLOR MODELS AND OTHER VALIDATED FUNCTIONAL INCLUSION METHODS. In: 2003 international journal of pure and applied mathematics\n\n\n110. Gehr T, Mirman M, Drachsler-Cohen D, Tsankov P, Chaudhuri S, Vechev M (2018) AI2: Safety and robustness certification of neural networks with abstract interpretation. In: 2018 IEEE symposium on security and privacy (SP). pp 3–18\n\n\n111. Linka K, Schäfer A, Meng X, Zou Z, Karniadakis GE, Kuhl E (2022) Bayesian physics informed neural networks for real-world nonlinear dynamical systems. Computer Methods in Applied Mechanics and Engineering 402:115346. https://doi.org/https://doi.org/10.1016/j.cma.2022.115346\n\n\n112. Zou Z, Meng X, Psaros AF, Karniadakis GE (2024) NeuralUQ: A comprehensive library for uncertainty quantification in neural differential equations and operators. SIAM Review 66(1):161–190. https://doi.org/10.1137/22M1518189\n\n\n113. Yang L, Meng X, Karniadakis GE (2021) B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data. Journal of Computational Physics 425:109913. https://doi.org/10.1016/j.jcp.2020.109913\n\n\n114. Sam D, Pukdee R, Jeong DP, Byun Y, Kolter JZ (2024) Bayesian neural networks with domain knowledge priors\n\n\n115. Bonneville C, Earls C (2022) Bayesian deep learning for partial differential equation parameter discovery with sparse and noisy data. Journal of Computational Physics: X 16:100115. https://doi.org/https://doi.org/10.1016/j.jcpx.2022.100115\n\n\n116. Meng X, Babaee H, Karniadakis GE (2021) Multi-fidelity bayesian neural networks: Algorithms and applications. Journal of Computational Physics 438:110361. https://doi.org/https://doi.org/10.1016/j.jcp.2021.110361\n\n\n117. Aikawa Y, Ueda N, Tanaka T (2024) Improving the efficiency of training physics-informed neural networks using active learning. New Generation Computing 42(4):739–760. https://doi.org/10.1007/s00354-024-00253-6\n\n\n118. Choubineh A, Chen J, Coenen F, Ma F (2023) Applying monte carlo dropout to quantify the uncertainty of skip connection-based convolutional neural networks optimized by big data. Electronics 12(6). https://doi.org/10.3390/electronics12061453\n\n\n119. Xu X, Wang J (2025) Comparative analysis of physics-guided bayesian neural networks for uncertainty quantification in dynamic systems. Forecasting 7(1). https://doi.org/10.3390/forecast7010009\n\n\n120. Folgoc LL, Baltatzis V, Desai S, et al (2021) Is MC dropout bayesian?\n\n\n121. Halder R, Ataei M, Salehipour H, Fidkowski K, Maki K (2024) Reduced-order modeling of unsteady fluid flow using neural network ensembles. Physics of Fluids 36(7). https://doi.org/10.1063/5.0207978\n\n\n122. Pestourie R, Mroueh Y, Rackauckas C, Das P, Johnson SG (2023) Physics-enhanced deep surrogates for partial differential equations. Nature Machine Intelligence 5(12):1458–1465. https://doi.org/10.1038/s42256-023-00761-y\n\n\n123. Scher S, Messori G (2021) Ensemble methods for neural network-based weather forecasts\n\n\n124. Zanisi L, Ho A, Barr J, et al (2024) Efficient training sets for surrogate models of tokamak turbulence with active deep ensembles. Nuclear Fusion 64(3):036022. https://doi.org/10.1088/1741-4326/ad240d\n\n\n125. Musekamp D, Kalimuthu M, Holzmüller D, Takamoto M, Niepert M (2024) Active learning for neural PDE solvers\n\n\n126. Wen Y, Tran D, Ba J (2020) BatchEnsemble: An alternative approach to efficient ensemble and lifelong learning\n\n\n127. Huang G, Li Y, Pleiss G, Liu Z, Hopcroft JE, Weinberger KQ (2017) Snapshot ensembles: Train 1, get m for free\n\n\n128. Gopakumar V, Gray A, Zanisi L, et al (2025) Calibrated physics-informed uncertainty quantification\n\n\n129. Soleimany AP, Amini A, Goldman S, Rus D, Bhatia SN, Coley CW (2021) Evidential deep learning for guided molecular property prediction and discovery. ACS central science 7(8):1356–1367\n\n\n130. Zhou H, Chen W, Cheng L, Liu J, Xia M (2023) Trustworthy fault diagnosis with uncertainty estimation through evidential convolutional neural networks. IEEE Transactions on Industrial Informatics 19(11):10842–10852\n\n\n131. Tan HS, Wang K, McBeth R (2025) Evidential physics-informed neural networks\n\n\n132. Liang A, Liu Q, Xu L, et al (2024) Conformal prediction on quantifying uncertainty of dynamic systems\n\n\n133. Podina L, Rad MT, Kohandel M (2024) Conformalized physics-informed neural networks\n\n\n134. Moya C, Mollaali A, Zhang Z, Lu L, Lin G (2025) Conformalized-DeepONet: A distribution-free framework for uncertainty quantification in deep operator networks. Physica D: Nonlinear Phenomena 471:134418. https://doi.org/https://doi.org/10.1016/j.physd.2024.134418\n\n\n135. Ma Z, Azizzadenesheli K, Anandkumar A (2024) Calibrated uncertainty quantification for operator learning via conformal prediction. arXiv preprint arXiv:240201960\n\n\n136. Chowell G, Luo R (2021) Ensemble bootstrap methodology for forecasting dynamic growth processes using differential equations: Application to epidemic outbreaks. BMC Medical Research Methodology 21(1):34. https://doi.org/10.1186/s12874-021-01226-9\n\n\n137. Omer ZM, Shareef H (2019) Adaptive boosting and bootstrapped aggregation based ensemble machine learning methods for photovoltaic systems output current prediction. In: 2019 29th australasian universities power engineering conference (AUPEC). pp 1–6\n\n\n138. Plooy R du, Venter PJ (2021) A comparison of artificial neural networks and bootstrap aggregating ensembles in a modern financial derivative pricing framework. Journal of Risk and Financial Management 14(6). https://doi.org/10.3390/jrfm14060254\n\n\n139. Tretiak K, Schollmeyer G, Ferson S (2023) Neural network model for imprecise regression with interval dependent variables. Neural Networks 161:550–564. https://doi.org/https://doi.org/10.1016/j.neunet.2023.02.005\n\n\n140. Chetwynd D, Worden K, Manson G (2006) An application of interval-valued neural networks to a regression problem. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 462(2074):3097–3114. https://doi.org/10.1098/rspa.2006.1717\n\n\n141. Betancourt D, Muhanna RL (2022) Interval deep learning for computational mechanics problems under input uncertainty. Probabilistic Engineering Mechanics 70:103370. https://doi.org/https://doi.org/10.1016/j.probengmech.2022.103370\n\n\n142. Ak R, Vitelli V, Zio E (2015) An interval-valued neural network approach for uncertainty quantification in short-term wind speed prediction. IEEE Transactions on Neural Networks and Learning Systems 26(11):2787–2800. https://doi.org/10.1109/TNNLS.2015.2396933\n\n\n143. Mirman M, Baader M, Vechev M (2021) The fundamental limits of interval arithmetic for neural networks\n\n\n144. Streeter M, Dillon JV (2023) Automatically bounding the taylor remainder series: Tighter bounds and new applications\n\n\n145. Schilling C, Forets M, Guadalupe S (2022) Verification of neural-network control systems by integrating taylor models and zonotopes. Proceedings of the AAAI Conference on Artificial Intelligence 36(7):8169–8177. https://doi.org/10.1609/aaai.v36i7.20790\n\n\n146. Kochdumper N, Schilling C, Althoff M, Bak S (2023) Open- and closed-loop neural network verification using polynomial zonotopes. In: NASA formal methods. Springer Nature Switzerland, pp 16–36\n\n\n147. Ladner T, Althoff M (2024) Exponent relaxation of polynomial zonotopes and its applications in formal neural network verification. In: Proceedings of the thirty-eighth AAAI conference on artificial intelligence and thirty-sixth conference on innovative applications of artificial intelligence and fourteenth symposium on educational advances in artificial intelligence. AAAI Press\n\n\n148. Lemesle A, Lehmann J, Gall TL (2024) Neural network verification with PyRAT\n\n\n149. Gray A, Gopakumar V, Rousseau S, Destercke S (2025) Guaranteed confidence-band enclosures for PDE surrogates\n\n\n150. Jordan M, Hayase J, Dimakis AG, Oh S (2022) Zonotope domains for lagrangian neural network verification"
  }
]