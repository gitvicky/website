[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Gopakumar",
    "section": "",
    "text": "I am a senior research scientist at the UK Atomic Energy Authority (UKAEA), where I lead a team developing “actionable” surrogate models for exascale simulations and data-driven models for the Fusion industry. My research focusses on enhancing machine learning models’ performance, robustness and interpretability through physics-based approaches.\nI am also a visiting researcher with the SciML group at STFC’s Rutherford Appleton Laboratory.\nConcurrently, I’m pursuing a PhD in Machine Learning under Marc Deisenroth in the Sustainability and Machine Learning Group at University College London.\n\n\nCurrent Research:\n\nPhysics-Informed Machine Learning\nContinuous Dynamics Modelling\nUncertainty Quantification\nDesign of Experiments\n\nBuilding simvue.io : AI-driven, open-source simulation management and tracking dashboard for streamlining engineering workflows. Developed with public funding from the UK Government. Currently in private \\(\\beta\\). \n\n“If there is a God, it must be a differential equation” - Bertrand Russell"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Calibrated Physics-Informed Uncertainty Quantification - Workshop on Calibrating Uncertainties, Isaac Newton Institute for Mathematical Sciences (Cambridge 2025) [Video] [Slides]\nOn Surrogate Modelling for Fusion - PhysicsX (London 2024) [Slides]\nFNO for Plasma Modelling - IAEA Workshop on AI for Accelerating Fusion and Plasma Science (Vienna 2023) [Slides]\nFNO for Plasma Modelling - IAEA Fusion Energy Conference (London, 2023) [Slides]\nFNO for Plasma Modelling - AI for sustainability workshop @ UCL (London, 2023)\nFourier RNNs for modelling noisy physics data - IEEE ICMLA (Bahamas, 2022) [Slides]\nInformed Sampling of the Plasma Hyperspace for Digital Twinning - IAEA Fusion Data Processing, Validation, Analysis (Chengdu, 2021) [Slides]\nOptimising Physics Informed Neural Networks - PyTorch Ecosystem Day (Virtual, 2021)[Poster]\nFluid Surrogates using Neural PDEs - SciML at RAL, STFC (Oxford, 2020) [Slides]\nSolving Fluid Dynamics with Neural Networks - FusionEP (Virtual, 2020) [Video] [Slides]\nData Driven Modelling of Plasma in Tokamaks - IOP Physics in the spotlight (London, 2019) [Slides]\nData Driven Modelling and Control of Plasma in Fusion Reactors - O’Reilly AI London (London, 2019) [Video]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Neural_ODE.html",
    "href": "Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#markdown",
    "href": "Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#code-cell",
    "href": "Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "Neural_ODE.html#equation",
    "href": "Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html",
    "href": "blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019)\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "blogposts/Neural_ODE.html#markdown",
    "href": "blogposts/Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#code-cell",
    "href": "blogposts/Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#equation",
    "href": "blogposts/Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#original-paper",
    "href": "blogposts/Neural_ODE.html#original-paper",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Blogposts/Neural_ODE.html",
    "href": "Blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019),\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_ODE.html",
    "href": "Blog/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_Operators.html",
    "href": "Blog/Neural_Operators.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nCalibrated Physics-Informed Uncertainty Quantification\nVignesh Gopakumar, Ander Gray, Lorenzo Zanisi, Timorthy Nunn,\nDaniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth\nICML, 2025\nTLDR: Calibrated uncertainty quantification of neural PDE solvers using physics residual errors as non-conformity scores for data-free conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nUncertainty Quantification of Surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi,\nDaniel Giles, Matt J. Kusner, Stanislas Pamela, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Blog/geometry.html",
    "href": "Blog/geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. (Li et al. 2023), (Li et al. 2020)\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF.\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions.\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that klatent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nThese seems to be adjacent to the kind of structure that we are looking at with a division of power and responsibilities, with specific networks and models learning differnet parts of the task.\n\n\n\n\nReferences\n\nLi, Zongyi, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, et al. 2023. “Geometry-Informed Neural Operator for Large-Scale 3D PDEs.” In Thirty-Seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=86dXbqT5Ua.\n\n\nLi, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. “Multipole Graph Neural Operator for Parametric Partial Differential Equations.” In Advances in Neural Information Processing Systems, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6755–66. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
  },
  {
    "objectID": "Blog/Geometry.html",
    "href": "Blog/Geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. [1–4]\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF [5–7].\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions. [8]\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture as shown below.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; B\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that latent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nUsing a structured NO such as the FNO, that has inductive bias, quick to evaluate has found to be rather beneficial, but the open questions still lie within the choice of the encoder-decoder. Papers such as [1, 3] utilise graph neural networks as the encoder-decoder, but however they have significant computational and memory requirements.\nI am currently exploring the idea of maybe using coordinate-based MLPs for the encoder and a GP as the decoder, with a neural operator deployed as a neural ODE with operator-splitting as the processor (neural-UDE):\n\n\n\n\n\nflowchart LR\n  A(NeRF) --&gt; B(Neural-UDE)\n  B --&gt; B\n  B --&gt; C(GP)\n\n\n\n\n\n\nThe challenge with the NeRF is that though allow for continuous space representations, they are terrible at enforcing strict boundaries within the domain and have soft gradients across them. These leads to losing strutcures with significant gradients being lost in the initial condition and geometry. The advantages could be that the IC might not have sharp enough gradients so representative capacity might not be that much of a concern. The other advantage is that they are small, light models, based on simple MLPs.\nAs for the GP, the challenges remain the same as always, whether they can be scaled to handle the dimension and size as we might need. Might have to train multiple GPs or Mixture of GPs? The advantage is that we can get UQ built into the models.\nThis approach brings in a division of labour, having models learn a specific task and then connected together in an approach similar to integrated modelling. Is this a kind of Mixture of Experts ? \n\nExploring Kernel Methods\n\nConsverative Remapping\n\n\nNon-Uniform FFT\nFFT over non-uniform grids (nuFFT) as demonstrated within this pytorch repository, where the grids are structured using a Kaisser-Bessel window functions as interpolation kernels. Essentially we are using a weighted kernel approach to move from unstructured grids to structured grids.\n\n\n\n\n\n\nReferences\n\n1. Li Z, Kovachki NB, Choy C, et al (2023) Geometry-informed neural operator for large-scale 3D PDEs. In: Thirty-seventh conference on neural information processing systems\n\n\n2. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Multipole graph neural operator for parametric partial differential equations. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 6755–6766\n\n\n3. Brandstetter J, Worrall DE, Welling M (2022) Message passing neural PDE solvers. In: International conference on learning representations\n\n\n4. Li T, Zou S, Chang X, Zhang L, Deng X (2024) Predicting unsteady incompressible fluid dynamics with finite volume informed neural network. Physics of Fluids 36(4). https://doi.org/10.1063/5.0197425\n\n\n5. Sitzmann V, Martel J, Bergman A, Lindell D, Wetzstein G (2020) Implicit neural representations with periodic activation functions. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 7462–7473\n\n\n6. Serrano L, Boudec LL, Koupaı̈ AK, et al (2023) Operator learning with neural fields: Tackling PDEs on general geometries. In: Thirty-seventh conference on neural information processing systems\n\n\n7. Yin Y, Kirchmeyer M, Franceschi J-Y, Rakotomamonjy A, gallinari patrick (2023) Continuous PDE dynamics forecasting with implicit neural representations. In: The eleventh international conference on learning representations\n\n\n8. Chen Y, Hosseini B, Owhadi H, Stuart AM (2021) Solving and learning nonlinear PDEs with gaussian processes. Journal of Computational Physics 447:110668. https://doi.org/https://doi.org/10.1016/j.jcp.2021.110668"
  },
  {
    "objectID": "Personal/personal.html",
    "href": "Personal/personal.html",
    "title": "Personal Blog",
    "section": "",
    "text": "The best and the worst"
  },
  {
    "objectID": "Personal/best_and_worst.html",
    "href": "Personal/best_and_worst.html",
    "title": "The Best and the Worst",
    "section": "",
    "text": "There’s a big conundrum where I am torn between feeling that the best is yet to come and that it’s already over."
  },
  {
    "objectID": "Blog/UPE.html",
    "href": "Blog/UPE.html",
    "title": "Universal Physics Engine",
    "section": "",
    "text": "Can AI serve as a Universal Physics Engine ?\n\n\nIn Stephen Wolfram’s (one of the legends in the field of computational physics and mathematics) latest writing, he explores ideas on the fields and methods with which he thinks AI will impact scientific disciplines. He starts the blog with:\n“To the ultimate question of whether AI can solve Science, we’re going to see that the answer is inevitably and firmly no.”\nSo probably there is not much of a reason for me to explore the idea of building a universal physics engine using AI, but hey I love attempting the impossible (why do you think I ended up doing Fusion !!!).\nScience can be broadly defined to fall under three categories:\n\n\n\n\n\nflowchart LR\n  A(Prediction)\n  B(Discovery)\n  C(Explanation)\n\n\n\n\n\n\nNeed to a background on the different kind of approaches being taken:\n\nSurrogate Models of all kinds.\nFoundation Models for Physics\nGenerative Enginering - PhysicsX, Zoo.dev and the varioud likes of those. Generative models for statistical physics.\n\nMath proofing approaches\nText-Code is all you need with the right software - Bring in the Karpathy tweet.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nCurrent Limitations\nPotential AI Capabilities\nNovel Solutions\nChallenges\n\n\n\n\nMathematical Framework\nLimited to specific classes of PDEs; Separate frameworks for different physics domains\nUnified mathematical framework spanning quantum to classical physics; Automatic detection of symmetries and conservation laws; Dynamic generation of problem-specific basis functions\nDevelopment of new mathematical structures beyond tensors and operators; Creation of hybrid symbolic-numerical methods; Discovery of new transformations between problem domains\nEnsuring mathematical consistency across scales; Proving convergence for new methods; Handling mathematical singularities\n\n\nGeometric Processing\nPre-defined mesh types; Manual domain decomposition; Limited handling of complex boundaries\nAutomated optimal mesh generation for arbitrary geometries; Intelligent boundary condition handling; Adaptive multi-resolution techniques\nSelf-designing coordinate systems; Topology-aware discretization; Geometry-informed basis functions\nDealing with moving boundaries; Handling topological changes; Ensuring mesh quality\n\n\nMulti-physics Coupling\nManual coupling between different physics models; Limited cross-scale interactions\nAutomated detection of relevant physics; Seamless coupling across scales; Self-adaptive model selection\nCreation of unified multi-physics formulations; Development of scale-bridging operators; Automatic derivation of reduced-order models\nMaintaining conservation properties; Handling disparate time scales; Managing computational complexity\n\n\nError Control & Stability\nFixed error estimators; Predefined stability criteria; Manual parameter tuning\nReal-time error prediction; Adaptive stability preservation; Automated parameter optimization\nDevelopment of new error metrics; Creation of self-stabilizing schemes; Learning-based error estimation\nGuaranteeing global stability; Balancing accuracy vs. efficiency; Handling chaos and sensitivity\n\n\nComputational Methods\nFixed numerical schemes; Limited parallelization; Domain-specific optimizations\nDynamic algorithm selection; Automated parallelization strategies; Problem-specific method synthesis\nCreation of new numerical algorithms; Development of quantum-inspired methods; Adaptive hybrid schemes\nScaling to large problems; Managing memory hierarchy; Ensuring reproducibility\n\n\nUser Interaction\nLimited feedback on solution quality; Fixed visualization options; Preset parameter ranges\nInteractive problem refinement; Adaptive visualization; Automated parameter exploration\nDevelopment of intuitive interfaces; Creation of explanation systems; Generation of physical insights\nCommunicating complex concepts; Handling ambiguous specifications; Providing meaningful feedback\n\n\nPhysical Consistency\nManual enforcement of conservation laws; Fixed constitutive relations; Predefined material models\nAutomatic constraint preservation; Learning-based constitutive relations; Adaptive material modeling\nDiscovery of new conservation principles; Creation of physics-informed neural operators; Development of universal material models\nEnsuring physical realizability; Handling unknown physics; Maintaining causality\n\n\nData Integration\nLimited use of experimental data; Fixed model parameters; Separate calibration steps\nReal-time data assimilation; Automated model calibration; Dynamic parameter updating\nDevelopment of physics-data hybrid methods; Creation of adaptive measurement operators; Automated experiment design\nHandling noisy data; Dealing with sparse measurements; Ensuring model validity"
  },
  {
    "objectID": "Slides/slides.html",
    "href": "Slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Plasma Surrogate Modelling using FNO - IAEA Workshop on AI in Fusion (December 2023)"
  },
  {
    "objectID": "Personal/visual_art_portfoilo.html",
    "href": "Personal/visual_art_portfoilo.html",
    "title": "Visual Art",
    "section": "",
    "text": "Kovalam December 2024"
  },
  {
    "objectID": "Blog/blog.html",
    "href": "Blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Iterative Solvers for Linear Systems\nConvolution as a form of Integration\nGeometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "href": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "title": "Integration by way of Convolution",
    "section": "The Mathematical Foundation of Regularized Inversion",
    "text": "The Mathematical Foundation of Regularized Inversion\nWhen we have a signal \\(y\\) that results from the convolution of an unknown signal \\(f\\) with a known kernel \\(g\\):\n\\[y = f * g\\]\nIn the frequency domain (using the Fourier transform), this becomes:\n\\[Y(\\omega) = F(\\omega) \\cdot G(\\omega)\\]\nWhere \\(Y\\), \\(F\\), and \\(G\\) are the Fourier transforms of \\(y\\), \\(f\\), and \\(g\\) respectively.\nIdeally, we could recover \\(f\\) by:\n\\[F(\\omega) = \\frac{Y(\\omega)}{G(\\omega)}\\]\nAnd then applying the inverse Fourier transform to get \\(f\\) in the time domain.\n\nThe Problem of Ill-Conditioning\nThe difficulty arises when \\(G(\\omega)\\) approaches zero at certain frequencies. This occurs with many important kernels, including our [1, -2, 1] second derivative kernel.\nFor the second derivative kernel, the frequency response is approximately:\n\\[G(\\omega) \\approx -\\omega^2\\]\nThis means \\(G(\\omega)\\) is very small near \\(\\omega = 0\\) (the DC component and low frequencies). Division by these small values causes numerical instability, amplifying noise and errors.\n\n\nTikhonov Regularization\nWhat we’re doing with epsilon is a form of Tikhonov regularization, which can be mathematically represented as:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{G(\\omega) + \\epsilon}\\]\nThis is equivalent to finding the solution to the minimization problem:\n\\[\\min_f \\|g * f - y\\|^2 + \\epsilon \\|f\\|^2\\]\nWhere the first term measures how well our recovered signal explains the observed data, and the second term penalizes large values in the solution, providing stability.\n\n\nMathematical Properties of the Regularization\nTo understand what epsilon does mathematically, let’s analyze its effect at different frequencies:\n\nWhere \\(|G(\\omega)| \\gg \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{G(\\omega)} \\approx F(\\omega)\\] The recovery is accurate at frequencies where the kernel has significant response.\nWhere \\(|G(\\omega)| \\ll \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{\\epsilon} \\approx 0\\] The recovery suppresses components at frequencies where the kernel has near-zero response.\nWhere \\(|G(\\omega)| \\approx \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{2G(\\omega)} \\approx \\frac{F(\\omega)}{2}\\] The recovery partially retrieves information, with some attenuation.\n\nThis creates a smooth transition between fully recovered frequencies and suppressed frequencies, avoiding the sharp discontinuities that would cause ringing artifacts."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "href": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "title": "Integration by way of Convolution",
    "section": "Alternative Approaches to Signal Recovery",
    "text": "Alternative Approaches to Signal Recovery\nThere are several alternative approaches for recovering a signal after convolution, especially for the case of integration following differentiation:\n\n1. Direct Integration (for Differential Kernels)\nSince our [1, -2, 1] kernel approximates the second derivative, integration is a natural inverse operation. We can recover an approximation to the original signal by integrating twice:\n\\[\\hat{f}(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nWhere \\(C_1\\) and \\(C_2\\) are integration constants that need to be determined from boundary conditions or additional information.\nFor discrete signals, this becomes cumulative summation:\ndef double_integrate(signal):\n    # First integration (cumulative sum)\n    first_integral = np.cumsum(signal)\n    # Second integration\n    second_integral = np.cumsum(first_integral)\n    return second_integral\nThe challenge is determining the correct integration constants, which represent the linear and constant components lost during differentiation.\n\n\n2. Wiener Deconvolution\nWiener deconvolution incorporates knowledge about the signal-to-noise ratio (SNR):\n\\[F_{\\text{Wiener}}(\\omega) = \\frac{G^*(\\omega)}{|G(\\omega)|^2 + \\frac{1}{\\text{SNR}(\\omega)}} \\cdot Y(\\omega)\\]\nWhere \\(G^*(\\omega)\\) is the complex conjugate of \\(G(\\omega)\\) and \\(\\text{SNR}(\\omega)\\) is the signal-to-noise ratio at each frequency.\nThis approach is more adaptive than simple regularization, as it adjusts the regularization based on the expected noise level at each frequency.\n\n\n3. Iterative Methods\nFor very ill-conditioned problems, iterative methods like conjugate gradient or LSMR can be more stable:\n\\[f_{k+1} = f_k + \\alpha_k(g^* * (y - g * f_k))\\]\nWhere \\(g^*\\) is the adjoint (time-reversed) kernel and \\(\\alpha_k\\) is a step size.\nThese methods gradually refine the solution, avoiding direct division in the frequency domain.\n\n\n4. Wavelet-Based Deconvolution\nWavelets provide localization in both time and frequency, making them well-suited for deconvolution problems:\n\nTransform the signal to the wavelet domain\nApply regularized inversion in the wavelet domain\nTransform back to the time domain\n\nThis approach can better handle signals with localized features and non-stationary properties."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "href": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "title": "Integration by way of Convolution",
    "section": "Specific Case: Integration After Differentiation",
    "text": "Specific Case: Integration After Differentiation\nFor your specific interest in performing integration after a differential kernel has been applied, let me explain the mathematical connection more explicitly.\nIf we have applied the second derivative kernel [1, -2, 1] to a signal \\(f\\), obtaining \\(y\\):\n\\[y[n] = f[n+1] - 2f[n] + f[n-1] \\approx \\frac{d^2f}{dt^2}\\]\nThen to recover \\(f\\), we need to integrate \\(y\\) twice. In the continuous domain, this would be:\n\\[f(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nIn the frequency domain, integration corresponds to division by \\(j\\omega\\). So double integration is division by \\((j\\omega)^2 = -\\omega^2\\). The frequency response of our [1, -2, 1] kernel is approximately \\(-\\omega^2\\), which means the ideal recovery filter would be \\(\\frac{1}{-\\omega^2} = -\\frac{1}{\\omega^2}\\).\nThis perfectly matches our regularized inverse:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{-\\omega^2 + \\epsilon}\\]\nThe regularization term \\(\\epsilon\\) prevents division by zero at \\(\\omega = 0\\), which corresponds to the integration constants we would need to determine in the time domain approach."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "href": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "title": "Integration by way of Convolution",
    "section": "Practical Implementation for Integration Recovery",
    "text": "Practical Implementation for Integration Recovery\nLet me outline a robust approach for your integration task:\n\nSpectral Domain Method with Regularization\ndef recover_by_integration_spectral(signal, kernel, epsilon=1e-6):\n    n_fft = len(signal) + len(kernel) - 1\n    padded_signal = F.pad(signal, (0, n_fft - len(signal)))\n    padded_kernel = F.pad(kernel, (0, n_fft - len(kernel)))\n\n    # FFT\n    signal_fft = torch.fft.rfft(padded_signal)\n    kernel_fft = torch.fft.rfft(padded_kernel)\n\n    # Inverse filtering with regularization\n    recovered_fft = signal_fft / (kernel_fft + epsilon)\n\n    # IFFT\n    recovered = torch.fft.irfft(recovered_fft)\n\n    return recovered[:len(signal)]\nTime Domain Integration with Boundary Correction\ndef recover_by_double_integration(signal, boundary_values=None):\n    # First integration\n    first_integral = torch.cumsum(signal, dim=0)\n\n    # Correct for linear drift (first integration constant)\n    if boundary_values and 'start_slope' in boundary_values:\n        first_integral = first_integral + boundary_values['start_slope'] * torch.arange(len(signal))\n\n    # Second integration\n    second_integral = torch.cumsum(first_integral, dim=0)\n\n    # Correct for constant offset (second integration constant)\n    if boundary_values and 'start_value' in boundary_values:\n        second_integral = second_integral + boundary_values['start_value']\n\n    return second_integral\nHybrid Approach\nFor the most robust recovery, we can combine spectral and time domain methods:\ndef hybrid_recovery(signal, kernel, epsilon=1e-6):\n    # Spectral recovery for high frequencies\n    spectral_recovery = recover_by_integration_spectral(signal, kernel, epsilon)\n\n    # Time domain integration for low frequency components\n    time_recovery = recover_by_double_integration(signal)\n\n    # High-pass filter for spectral recovery\n    high_freq = high_pass_filter(spectral_recovery)\n\n    # Low-pass filter for time domain recovery\n    low_freq = low_pass_filter(time_recovery)\n\n    # Combine the two\n    return high_freq + low_freq"
  },
  {
    "objectID": "Blog/Inverse_kernel.html#conclusion",
    "href": "Blog/Inverse_kernel.html#conclusion",
    "title": "Integration by way of Convolution",
    "section": "Conclusion",
    "text": "Conclusion\nThe epsilon regularization parameter provides a mathematically sound approximation to the inverse problem, creating a balance between recovery fidelity and numerical stability. It’s essentially solving a regularized least-squares problem that finds the solution with the best trade-off between data fidelity and solution stability.\nFor the specific case of recovering a signal after applying a differential kernel, direct integration methods can be combined with spectral approaches for robust results. The key challenge is determining the integration constants (or equivalently, the low-frequency components) that were lost during differentiation.\nThe most effective approach often combines multiple methods, using the strengths of each to compensate for the weaknesses of others. This hybrid approach can provide more reliable signal recovery across a wide range of practical scenarios."
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-i-the-landscape-of-linear-system-solvers",
    "href": "Blog/iterative_solvers.html#part-i-the-landscape-of-linear-system-solvers",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part I: The Landscape of Linear System Solvers",
    "text": "Part I: The Landscape of Linear System Solvers\n\nSection 1: Foundational Concepts: Direct vs. Iterative Solvers\nIn computational mathematics, one of the most fundamental and ubiquitous tasks is the solution of a system of linear equations, which can be expressed in the matrix form:\n\\[Ax = b\\]\nHere, \\(A\\) is a known \\(n \\times n\\) coefficient matrix, \\(b\\) is a known \\(n\\)-dimensional vector, and \\(x\\) is the \\(n\\)-dimensional solution vector to be found. The methods developed to solve this problem fall into two primary categories: direct methods and iterative methods.\nThe choice between these two families of algorithms is a critical decision in the design of numerical simulations, dictated by the size, structure, and origin of the linear system.\n\nDirect Methods\nDirect methods are algorithms that, in the absence of round-off error, compute the exact solution \\(x\\) in a finite and predetermined number of arithmetic operations. The most well-known of these is Gaussian elimination, which systematically transforms the original system into an equivalent upper triangular system that can be easily solved via back substitution. In modern numerical linear algebra, direct methods are almost always implemented through a matrix factorization, such as the LU, Cholesky, or QR decompositions.\nThe principal advantages of direct methods are their robustness and generality. They are applicable to a wide range of problems, and their behavior is highly predictable. For a non-singular matrix, a direct solver is guaranteed to produce a solution. This reliability has led to the development of mature, highly optimized, and robust software libraries like LAPACK, which are a cornerstone of scientific computing.\nHowever, the primary challenge and ultimate limitation of direct methods is their scaling behavior with respect to problem size, \\(n\\). When the matrix \\(A\\) is dense (i.e., has few zero entries), forming its LU factorization requires approximately \\(2n^3/3\\) arithmetic operations, with storage requirements of \\(O(n^2)\\). While formidable, this is often acceptable for problems of moderate size.\nThe true difficulty arises when dealing with the large, sparse matrices that are characteristic of many scientific and engineering applications, particularly those originating from the discretization of partial differential equations (PDEs). The factorization process for a sparse matrix introduces new non-zero elements in positions that were originally zero in the factors \\(L\\) and \\(U\\). This phenomenon, known as fill-in, can be catastrophic. For a sparse matrix arising from a 2D PDE discretization, where the number of non-zeros is proportional to \\(N\\), the storage required for the LU factors can grow to \\(O(N^{3/2})\\) and the computational work to \\(O(N^2)\\). For 3D problems, the situation is even more dire, with storage scaling as \\(O(N^{5/3})\\) and work as \\(O(N^{7/3})\\). For the million-variable systems common in modern simulations, storing the dense factors would require terabytes of memory, rendering direct methods completely impractical.\n\n\nIterative Methods\nIn stark contrast to direct methods, iterative methods do not compute the exact solution in a fixed number of steps. Instead, they begin with an initial guess for the solution, denoted \\(x^{(0)}\\) (often a vector of zeros), and progressively generate a sequence of improved approximations, \\(x^{(1)}, x^{(2)}, x^{(3)}, \\ldots\\), that ideally converge to the true solution \\(x\\). At each step \\(k\\), the quality of the approximation is typically measured by the norm of the residual vector, \\(r^{(k)} = b - Ax^{(k)}\\). The iteration continues until this residual is smaller than a user-specified tolerance, or a maximum number of iterations is reached.\nThe primary advantages of iterative methods directly address the shortcomings of direct solvers for large-scale problems:\nMemory Efficiency: Iterative methods typically only require the storage of the non-zero elements of the matrix \\(A\\) along with a handful of auxiliary vectors. For a sparse matrix with an average of \\(k\\) non-zeros per row, the memory footprint is \\(O(Nk)\\), which is vastly more efficient than the memory required for the dense factors of a direct method.\nComputational Efficiency: The core computational kernel of most modern iterative methods is the matrix-vector product \\((A \\cdot v)\\). For a sparse matrix, this operation is very cheap, requiring only \\(O(Nk)\\) floating-point operations. If the method converges to a satisfactory solution in \\(m\\) iterations, and if \\(m \\ll N\\), the total computational work, roughly \\(O(Nkm)\\), can be orders of magnitude lower than that of a direct solve. For certain classes of problems, particularly those from elliptic PDEs, advanced iterative methods can converge in a number of iterations that is nearly independent of the problem size \\(N\\), achieving so-called linear cost.\nParallelism: The fundamental operations of iterative methods—matrix-vector products, inner products, and vector updates (AXPY operations)—are composed of many independent calculations. This structure makes them far easier to implement efficiently on parallel computing architectures compared to the complex data dependencies and communication patterns inherent in parallel matrix factorizations.\nTunable Precision: Iterative methods provide the flexibility to trade off computational effort for solution accuracy. In many application contexts, such as the inner loops of a nonlinear solver or a time-dependent simulation, a highly precise solution to the linear system at each step is unnecessary. An approximate solution is often sufficient, and an iterative method can be stopped early, saving significant computation time.\nThe main drawback of iterative methods is that their performance is not as predictable as that of direct methods. Convergence is not guaranteed for all linear systems, and the rate of convergence can vary dramatically depending on the properties of the matrix \\(A\\), most notably its condition number. For many challenging problems, a “naive” iterative method will converge too slowly to be practical, or it may fail to converge at all. Consequently, the successful application of iterative methods often requires careful selection of the algorithm and the use of sophisticated preconditioning techniques, which are designed to transform the problem into one that is more amenable to iterative solution.\n\n\nEconomic Perspective\nThe choice between direct and iterative solvers can be viewed through an economic lens, balancing predictable but potentially prohibitive costs against lower per-unit costs with uncertain total effort. A direct solver is akin to purchasing a custom manufacturing machine: the upfront cost (factorization) is high and determined by the problem size, but once paid, it can produce solutions (for different right-hand sides, \\(b\\)) relatively cheaply and in a known amount of time. An iterative solver is more like hiring a worker on an hourly basis: the rate per hour (cost per iteration) is low and predictable, but the total time required to complete the job (number of iterations) is not known in advance and depends on the difficulty of the task (the condition of the matrix). For small, simple jobs (small, dense matrices), the certainty of the machine is preferable. For massive, complex projects (large, sparse matrices), the lower hourly rate is the only feasible option, and the focus shifts to managing the project efficiently to minimize the total hours worked—a role perfectly analogous to preconditioning in the world of iterative solvers.\n\n\n\n\n\n\n\n\nFeature\nDirect Methods\nIterative Methods\n\n\n\n\nSolution Accuracy\nExact (in absence of round-off error)\nApproximate, up to a specified tolerance\n\n\nComputational Cost (Dense N×N)\nHigh, typically \\(O(N^3)\\)\nVery high, generally not used\n\n\nComputational Cost (Sparse N×N)\nHigh due to fill-in, e.g., \\(O(N^2)\\) for 2D PDEs\nLow if convergence is fast, \\(O(Nkm)\\) where \\(m \\ll N\\)\n\n\nMemory Usage (Sparse)\nHigh due to fill-in, e.g., \\(O(N^{3/2})\\) for 2D PDEs\nLow, typically \\(O(Nk)\\)\n\n\nApplicability\nGeneral-purpose for any non-singular matrix\nMethod choice depends on matrix properties (e.g., symmetry)\n\n\nRobustness\nVery high; predictable behavior\nConvergence is not guaranteed; can be slow or fail\n\n\nParallelism\nDifficult to parallelize efficiently due to data dependencies\nCore operations are highly parallelizable\n\n\nKey Challenge\nManaging fill-in and the associated high memory/computational cost\nEnsuring and accelerating convergence\n\n\n\nTable 1: Comparison of Direct vs. Iterative Solvers"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-ii-a-deep-dive-into-modern-krylov-subspace-methods",
    "href": "Blog/iterative_solvers.html#part-ii-a-deep-dive-into-modern-krylov-subspace-methods",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part II: A Deep Dive into Modern Krylov Subspace Methods",
    "text": "Part II: A Deep Dive into Modern Krylov Subspace Methods\nThe most powerful and widely used iterative techniques today belong to the family of Krylov subspace methods. These methods work by generating an optimal approximate solution from a special subspace—the Krylov subspace—which is built using successive applications of the matrix \\(A\\) to an initial vector, typically the initial residual \\(r_0\\). The Krylov subspace of order \\(m\\) is defined as:\n\\[\\mathcal{K}_m(A, r_0) = \\text{span}\\{r_0, Ar_0, A^2r_0, \\ldots, A^{m-1}r_0\\}\\]\nBy searching for a solution within this subspace, these methods implicitly construct a polynomial in the matrix \\(A\\) that approximates \\(A^{-1}\\). This section delves into the two most important Krylov subspace methods: the Conjugate Gradient method for symmetric positive-definite systems and the Generalized Minimal Residual method for general non-symmetric systems.\n\nSection 2: The Conjugate Gradient (CG) Method: The Workhorse for Symmetric Positive-Definite Systems\nThe Conjugate Gradient (CG) method, developed by Magnus Hestenes and Eduard Stiefel in 1952, is arguably the most important iterative method ever devised. It is the algorithm of choice for solving large, sparse linear systems where the coefficient matrix \\(A\\) is symmetric and positive-definite (SPD). An SPD matrix is a symmetric matrix for which \\(x^T Ax &gt; 0\\) for any non-zero vector \\(x\\), or equivalently, all its eigenvalues are positive.\n\nMathematical Foundation: An Optimization Perspective\nThe power and elegance of the CG method stem from its deep connection to optimization. For an SPD matrix \\(A\\), solving the linear system \\(Ax = b\\) is mathematically equivalent to finding the unique vector \\(x\\) that minimizes the quadratic function (often called a quadratic form or energy functional):\n\\[\\phi(x) = \\frac{1}{2}x^T Ax - b^T x\\]\nThis equivalence is established by examining the gradient of \\(\\phi(x)\\). The gradient is given by \\(\\nabla \\phi(x) = Ax - b\\). The minimum of the convex function \\(\\phi(x)\\) occurs where its gradient is zero, which means \\(Ax - b = 0\\), or \\(Ax = b\\). Thus, the linear system solution is the minimizer of the quadratic form.\nThis reframing allows us to approach the linear algebra problem using optimization techniques. A simple starting point is the method of steepest descent, where one iteratively takes steps in the direction of the negative gradient, which is the direction of the fastest local decrease of \\(\\phi(x)\\). The search direction at step \\(k\\) is simply \\(p_k = r_k = b - Ax_k\\). While intuitive, steepest descent often converges very slowly, as the search directions can become nearly orthogonal in successive steps, leading to a characteristic zig-zagging path toward the minimum.\nThe breakthrough of the CG method lies in its choice of search directions. Instead of using the steepest descent directions, it constructs a set of search directions \\(\\{p_0, p_1, \\ldots, p_{n-1}\\}\\) that are A-conjugate (or A-orthogonal). This property is defined as:\n\\[p_i^T A p_j = 0 \\quad \\text{for all } i \\neq j\\]\nThis condition is a generalization of standard orthogonality; if \\(A = I\\), it reduces to the familiar dot product being zero. The set of \\(n\\) A-conjugate vectors forms a basis for \\(\\mathbb{R}^n\\). The profound consequence of A-conjugacy is that when we perform a line search to minimize \\(\\phi(x)\\) along a new direction \\(p_k\\), this minimization does not interfere with the minimization already achieved in the previous directions \\(\\{p_0, \\ldots, p_{k-1}\\}\\). This allows the method to converge to the exact solution in at most \\(n\\) iterations (in exact arithmetic), since it effectively performs \\(n\\) independent one-dimensional minimizations along the basis directions.\n\n\nThe Conjugate Gradient Algorithm\nA remarkable feature of the CG method is that these A-conjugate directions can be generated “on the fly” using a simple and efficient three-term recurrence relation. Each new direction \\(p_k\\) is constructed from the current residual \\(r_k\\) and the previous search direction \\(p_{k-1}\\), without needing to store all previous vectors. This makes the algorithm computationally inexpensive and require minimal storage (only a few vectors need to be stored at any time).\nThe algorithm proceeds as follows:\nInitialization: 1. Choose an initial guess \\(x_0\\) (e.g., \\(x_0 = 0\\)). 2. Compute the initial residual: \\(r_0 = b - Ax_0\\). 3. Set the first search direction to be the residual: \\(p_0 = r_0\\). 4. Compute \\(\\text{rsold} = r_0^T r_0\\).\nIteration: For \\(k = 0, 1, 2, \\ldots\\) until convergence: 1. Compute the matrix-vector product: \\(v_k = Ap_k\\). 2. Compute the optimal step size to minimize \\(\\phi\\) along \\(p_k\\): \\(\\alpha_k = \\frac{\\text{rsold}}{p_k^T v_k}\\). 3. Update the solution: \\(x_{k+1} = x_k + \\alpha_k p_k\\). 4. Update the residual: \\(r_{k+1} = r_k - \\alpha_k v_k\\). 5. Check for convergence: if \\(||r_{k+1}||_2\\) is below a tolerance, stop. 6. Compute \\(\\text{rsnew} = r_{k+1}^T r_{k+1}\\). 7. Update the search direction to be A-conjugate to previous directions: \\(p_{k+1} = r_{k+1} + \\frac{\\text{rsnew}}{\\text{rsold}} p_k\\). 8. Prepare for the next iteration: \\(\\text{rsold} = \\text{rsnew}\\).\nThis algorithm requires only one matrix-vector product per iteration, along with a few inner products and vector updates, making it extremely efficient.\n\n\nPython Implementation\nThe following Python code provides a basic, self-contained implementation of the Conjugate Gradient algorithm, illustrating its structure:\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=1000):\n    \"\"\"\n    Solves the system Ax=b for an SPD matrix A using the Conjugate Gradient method.\n\n    Args:\n        A (np.ndarray): The symmetric positive-definite coefficient matrix.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray, optional): Initial guess. Defaults to a zero vector.\n        tol (float, optional): The tolerance for convergence. Defaults to 1e-6.\n        max_iter (int, optional): Maximum number of iterations. Defaults to 1000.\n\n    Returns:\n        tuple: A tuple containing the solution vector x and the number of iterations.\n    \"\"\"\n    n = len(b)\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    r = b - A @ x\n    p = r.copy()\n    rs_old = r @ r\n\n    if np.sqrt(rs_old) &lt; tol:\n        return x, 0\n\n    for i in range(max_iter):\n        Ap = A @ p\n        alpha = rs_old / (p @ Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = r @ r\n        \n        if np.sqrt(rs_new) &lt; tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return x, i + 1\n\n\nThe Dual Nature of Conjugate Gradient\nThe CG method occupies a unique position, blurring the line between direct and iterative solvers. Its theoretical foundation guarantees that for an \\(N \\times N\\) system, it will find the exact solution in at most \\(N\\) steps, a property characteristic of a direct method. This finite termination property is a direct consequence of constructing \\(N\\) A-orthogonal search directions that span the entire solution space \\(\\mathbb{R}^N\\).\nHowever, the true power of CG lies not in this finite termination property, but in its performance as an iterative method. For the very large systems where CG is applied (with \\(N\\) in the millions), performing \\(N\\) iterations is computationally infeasible and would be slower than a direct solve. The practical utility of CG comes from its optimality property: at each iteration \\(k\\), the CG algorithm finds the approximation \\(x_k\\) in the Krylov subspace \\(\\mathcal{K}_k(A, r_0)\\) that minimizes the A-norm of the error, \\(||x - x_k||_A = \\sqrt{(x - x_k)^T A (x - x_k)}\\).\nThis optimality ensures that CG makes the best possible progress toward the solution at every step, given the information available in the Krylov subspace. As a result, it often produces an excellent approximation to the solution in a number of iterations \\(k\\) that is much smaller than the matrix dimension \\(N\\), i.e., \\(k \\ll N\\). The rate of this convergence is closely tied to the distribution of the eigenvalues of \\(A\\). If the eigenvalues are clustered together, or if the condition number \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\) is small, convergence is very rapid. Therefore, while its theoretical underpinnings classify it as a direct method, its practical application and value are entirely as a fast iterative method. This dual identity resolves the apparent contradiction in its common descriptions and highlights its exceptional nature among numerical algorithms.\n\n\n\nSection 3: The Generalized Minimal Residual (GMRES) Method: Tackling Non-Symmetric Systems\nWhen the coefficient matrix \\(A\\) is not symmetric, or not positive-definite, the optimization framework of the CG method is no longer applicable. For these more general cases, the Generalized Minimal Residual (GMRES) method is the standard and most robust Krylov subspace technique. Developed by Youcef Saad and Martin Schultz in 1986, GMRES is designed to solve any non-singular linear system \\(Ax = b\\).\n\nMathematical Foundation: A Least-Squares Perspective\nThe core principle of GMRES is fundamentally different from that of CG. Instead of minimizing an energy functional, GMRES directly tackles the residual. At each iteration \\(m\\), GMRES finds the vector \\(x_m\\) in the affine Krylov subspace \\(x_0 + \\mathcal{K}_m(A, r_0)\\) that minimizes the 2-norm (Euclidean norm) of the corresponding residual vector. That is, it solves the minimization problem:\n\\[x_m = \\arg\\min_{z \\in x_0 + \\mathcal{K}_m(A, r_0)} ||b - Az||_2\\]\nTo solve this problem efficiently and in a numerically stable manner, GMRES employs the Arnoldi iteration. The Arnoldi process is an algorithm that constructs an orthonormal basis \\(\\{v_1, v_2, \\ldots, v_m\\}\\) for the Krylov subspace \\(\\mathcal{K}_m(A, r_0)\\). After \\(m\\) steps, the Arnoldi process yields two crucial outputs:\n\nA matrix \\(V_{m+1} = [v_1, v_2, \\ldots, v_{m+1}]\\) whose columns form an orthonormal basis for \\(\\mathcal{K}_{m+1}(A, r_0)\\).\nAn \\((m+1) \\times m\\) upper-Hessenberg matrix \\(\\tilde{H}_m\\) (a matrix with zeros below the first subdiagonal).\n\nThese matrices are related by the fundamental Arnoldi relation: \\(AV_m = V_{m+1}\\tilde{H}_m\\). This relation is the key to making GMRES practical. Any vector \\(z\\) in the search space can be written as \\(z = x_0 + V_m y\\) for some vector \\(y \\in \\mathbb{R}^m\\). Substituting this into the residual minimization problem gives:\n\\[\\min_{y \\in \\mathbb{R}^m} ||b - A(x_0 + V_m y)||_2 = \\min_{y \\in \\mathbb{R}^m} ||r_0 - AV_m y||_2\\]\nUsing the Arnoldi relation and the fact that \\(v_1 = r_0/||r_0||_2\\), this becomes:\n\\[\\min_{y \\in \\mathbb{R}^m} ||r_0||_2 v_1 - V_{m+1}\\tilde{H}_m y||_2\\]\nSince the columns of \\(V_{m+1}\\) are orthonormal, multiplying by \\(V_{m+1}^T\\) does not change the 2-norm. This transforms the original large, \\(N\\)-dimensional minimization problem into a small, \\((m+1) \\times m\\) linear least-squares problem that is cheap to solve:\n\\[\\min_{y \\in \\mathbb{R}^m} ||||r_0||_2 e_1 - \\tilde{H}_m y||_2\\]\nwhere \\(e_1\\) is the first standard basis vector. This small least-squares problem is typically solved using a QR factorization of \\(\\tilde{H}_m\\), which can be updated efficiently at each step using Givens rotations.\n\n\nThe GMRES Algorithm and its Practical Costs\nThe full GMRES algorithm involves an outer loop over the iteration count \\(m\\). Inside the loop, one step of the Arnoldi process is performed to generate the new basis vector \\(v_{m+1}\\) and the \\(m\\)-th column of the Hessenberg matrix \\(\\tilde{H}_m\\). Then, the small least-squares problem is solved to find the coefficients \\(y\\), and the approximate solution \\(x_m\\) is formed.\nA critical drawback of this process becomes apparent when compared to CG. The Arnoldi iteration is a “long-term” recurrence. To ensure the new vector \\(v_{m+1}\\) is orthogonal to all previous basis vectors, it must be explicitly orthogonalized against every one of them \\((v_1, \\ldots, v_m)\\) using a Gram-Schmidt process. This means that both the computational work and the storage required per iteration grow linearly with the iteration count \\(m\\). The storage cost is \\(O(Nm)\\) and the work per iteration is \\(O(Nm)\\) for the orthogonalization, plus the cost of the matrix-vector product. For problems that converge slowly, requiring a large \\(m\\), this becomes prohibitively expensive.\nThe standard solution to this practical limitation is restarted GMRES, denoted GMRES(k). In this variant, the algorithm is run for a fixed number of \\(k\\) iterations (where \\(k\\) is the restart parameter, typically a small number like 20 or 50). After \\(k\\) steps, the accumulated Krylov basis is discarded, an updated solution \\(x_k\\) is computed, and the entire process is restarted using \\(x_k\\) as the new initial guess. This strategy keeps the memory and computational costs bounded and manageable. However, this practicality comes at a price: by discarding the Krylov subspace, the algorithm loses its optimality and monotonic convergence properties. The residual norm is no longer guaranteed to decrease at every outer iteration, and the method can stagnate, especially for difficult problems.\n\n\nPython Implementation\nImplementing GMRES from scratch is considerably more involved than CG due to the Arnoldi process and the least-squares solve. Therefore, it is almost always used via a library function like scipy.sparse.linalg.gmres:\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import gmres\n\n# Define a non-symmetric matrix A and a right-hand side b\nA = csc_matrix([[3, 1, 0], [1, -1, 0], [0, 1, 2]], dtype=float)\nb = np.array([2, 4, -1], dtype=float)\n\n# Set an initial guess\nx0 = np.zeros(A.shape[0])\n\n# Solve the system using GMRES with a restart parameter of 20\n# and a tolerance of 1e-8.\nx, exit_code = gmres(A, b, x0=x0, restart=20, tol=1e-8)\n\nif exit_code == 0:\n    print(\"GMRES converged to a solution.\")\n    print(f\"Solution x: {x}\")\n    print(f\"Residual norm: {np.linalg.norm(b - A @ x)}\")\nelse:\n    print(f\"GMRES did not converge. Exit code: {exit_code}\")\n\n\nThe Optimality vs. Practicality Dilemma\nThe design of GMRES and its common restarted variant perfectly encapsulates a central trade-off in numerical algorithm design: the tension between theoretical optimality and practical feasibility. Full GMRES is “optimal” in the sense that it finds the approximation with the smallest possible residual norm within the ever-expanding Krylov subspace at each step. This guarantees that the residual norm decreases monotonically, a very desirable property.\nThis optimality, however, is built on the long-term recurrence of the Arnoldi process. To maintain the orthonormal basis, each new vector must be compared against all previous ones, leading to work and storage costs that grow with each iteration. The reason CG avoids this fate is the profound consequence of symmetry. For an SPD matrix, the Arnoldi process simplifies to the Lanczos process, which has a short three-term recurrence. This allows CG to generate its A-orthogonal basis with constant work and storage per iteration. This is a luxury not afforded to general non-symmetric matrices.\nGMRES(k) is the pragmatic compromise. It sacrifices the powerful optimality and guaranteed monotonic convergence of the full method to gain an algorithm with fixed, manageable memory and computational costs per cycle of \\(k\\) iterations. The choice of the restart parameter \\(k\\) is a heuristic balancing act: if \\(k\\) is too small, the algorithm may lose too much information from the Krylov subspace at each restart and converge very slowly or stagnate; if \\(k\\) is too large, the cost of the inner iterations becomes excessive. This illustrates that for the general class of non-symmetric problems, we must often accept less elegant and more heuristic solutions than those available for the highly structured SPD case.\n\n\n\n\n\n\n\n\nFeature\nConjugate Gradient (CG)\nGeneralized Minimal Residual (GMRES)\n\n\n\n\nApplicable Matrix Type\nSymmetric Positive-Definite (SPD)\nGeneral, Non-singular\n\n\nUnderlying Principle\nMinimization of a quadratic form \\(\\phi(x)\\)\nMinimization of the residual norm \\(||r||_2\\)\n\n\nOptimality Criterion\nMinimizes \\(||x - x_k||_A\\) in \\(\\mathcal{K}_k\\)\nMinimizes \\(||r_k||_2\\) in \\(x_0 + \\mathcal{K}_k\\)\n\n\nRecurrence Length\nShort (3-term recurrence)\nLong (depends on all previous vectors)\n\n\nWork per Iteration\nConstant, \\(O(Nk)\\)\nGrows with iteration \\(m\\), \\(O(Nm)\\)\n\n\nStorage per Iteration\nConstant (a few vectors)\nGrows with iteration \\(m\\), \\(O(Nm)\\)\n\n\nConvergence Guarantee\nGuaranteed for SPD matrices\nMonotonic residual reduction (full GMRES)\n\n\nCommon Variant\nPreconditioned CG (PCG)\nRestarted GMRES (GMRES(k))\n\n\n\nTable 2: Summary of Key Krylov Subspace Methods (CG and GMRES)"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-iii-the-primary-application-solving-partial-differential-equations",
    "href": "Blog/iterative_solvers.html#part-iii-the-primary-application-solving-partial-differential-equations",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part III: The Primary Application: Solving Partial Differential Equations",
    "text": "Part III: The Primary Application: Solving Partial Differential Equations\nWhile the study of iterative solvers is a rich subfield of numerical linear algebra, their development is not an abstract exercise. The primary motivation for creating and refining these algorithms is their application to solving problems from science and engineering. Overwhelmingly, this means solving the massive linear systems that arise from the numerical approximation of Partial Differential Equations (PDEs). This part of the report will bridge the gap between the algebraic methods described previously and their principal domain of application.\n\nSection 4: How PDEs Generate Large Linear Systems\nPDEs are mathematical equations that describe physical phenomena involving rates of change with respect to multiple independent variables, such as space and time. They are the language of physics, modeling everything from heat conduction and fluid dynamics to structural mechanics and electromagnetism. Except in very simple cases, these equations cannot be solved analytically. Instead, we must turn to computational methods, which requires transforming the continuous problem into a discrete one that a computer can handle. This process is known as discretization.\n\nDiscretization: From Continuous to Discrete\nThe core idea of discretization is to replace the continuous domain of the PDE with a finite set of points or volumes and to approximate the derivatives in the PDE with algebraic expressions involving the solution values at these discrete locations. Two of the most common discretization techniques are the Finite Difference Method and the Finite Element Method.\nFinite Difference Method (FDM): This is the most direct approach to discretization. The problem domain is overlaid with a regular grid of points. At each grid point, the partial derivatives in the PDE are replaced by finite difference approximations, which are derived from Taylor series expansions. For example, the second partial derivative of a function \\(u(x,y)\\) with respect to \\(x\\) at a grid point \\((i,j)\\) can be approximated by a centered difference:\n\\[\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2}\\]\nwhere \\(h\\) is the spacing between grid points and \\(u_{i,j}\\) is the approximate solution at \\((x_i, y_j)\\). By substituting these algebraic approximations for all derivatives in the PDE at every interior grid point, the differential equation is transformed into a large system of coupled linear equations. The unknowns in this system are the values of the solution at each grid point.\nFinite Element Method (FEM): FEM is a more versatile and powerful technique, particularly for problems with complex geometries or varying material properties. In this method, the continuous domain is partitioned into a mesh of smaller, simpler geometric shapes called “finite elements” (e.g., triangles in 2D, tetrahedra in 3D). Within each element, the unknown solution is approximated by a simple function, typically a polynomial, defined in terms of its values at the element’s nodes. The PDE is then reformulated into an equivalent integral or “weak” form. By requiring this integral form to hold over the collection of finite elements, a system of linear algebraic equations is generated, where the unknowns are the solution values at the nodes of the mesh.\n\n\nProperties of the Resulting Matrix A\nRegardless of the specific method used (FDM or FEM), the discretization of a PDE results in a linear system \\(Ax = b\\) with several defining characteristics:\nLarge-Scale: The number of equations and unknowns, \\(N\\), is equal to the number of degrees of freedom in the discrete model (e.g., the number of grid points in FDM). To achieve high accuracy, especially in three dimensions, the mesh must be very fine. It is common for \\(N\\) to be in the millions or even billions, making the system enormous.\nSparsity: A crucial feature of these systems is that they are sparse. The equation corresponding to a particular node or element only involves its immediate neighbors in the mesh. For instance, a standard 5-point finite difference stencil for the 2D Laplacian results in an equation at grid point \\((i,j)\\) that only involves values at \\((i,j)\\), \\((i±1,j)\\), and \\((i,j±1)\\). Consequently, the corresponding row in the matrix \\(A\\) will have at most five non-zero entries, regardless of how large \\(N\\) is. This inherent sparsity is what makes the use of iterative methods both possible and necessary.\nStructure: The matrices are not only sparse but often highly structured. For problems on regular grids, the non-zero entries form distinct patterns, such as bands along the main diagonal (e.g., a tridiagonal or block-tridiagonal structure). This structure can sometimes be exploited by specialized solvers.\n\n\n\nSection 5: Matching Solvers to PDE Types\nThe mathematical classification of a second-order PDE is not merely an abstract label; it reflects the fundamental physics of the problem it models. This classification, in turn, directly determines the mathematical properties of the matrix \\(A\\) that arises from its discretization, and this is the critical link that guides the selection of an appropriate iterative solver.\n\nElliptic PDEs\nPrototype and Physics: The canonical elliptic PDE is the Laplace equation (\\(\\nabla^2 u = 0\\)) or the Poisson equation (\\(\\nabla^2 u = f\\)). These equations model steady-state phenomena where the system has reached equilibrium and there is no time evolution. Physical examples include steady-state heat conduction, electrostatics, potential fluid flow, and stress analysis in solid mechanics.\nResulting Matrix Properties: When a self-adjoint elliptic operator like the Laplacian is discretized using standard methods such as centered finite differences or the Galerkin finite element method, the resulting matrix \\(A\\) is almost always Symmetric and Positive-Definite (SPD). The symmetry of \\(A\\) is a direct reflection of the self-adjoint property of the continuous operator. The positive-definiteness is linked to the dissipative or energy-minimizing nature of the underlying physics; for example, in heat transfer, the system seeks to minimize thermal energy.\nRecommended Solver: The SPD nature of the matrix makes the Conjugate Gradient (CG) method the ideal and most efficient iterative solver for these systems. Its convergence is guaranteed, and its performance is optimal for this class of matrices.\n\n\nParabolic PDEs\nPrototype and Physics: The classic parabolic PDE is the heat equation, \\(\\frac{\\partial u}{\\partial t} = \\alpha \\nabla^2 u\\). These equations model time-dependent diffusion processes, where a quantity like heat or a chemical concentration spreads and smooths out over time.\nResulting Matrix Properties: The properties of the matrix for a parabolic problem depend on how the time derivative is discretized. Using the method of lines, one discretizes in space first, yielding a system of ordinary differential equations (ODEs): \\(\\frac{du}{dt} = -A_s u + f\\), where \\(A_s\\) is the spatial discretization matrix. Applying a time-stepping scheme to solve this ODE system leads to a linear system at each time step.\nFor an implicit time-stepping scheme like Backward Euler, the system to be solved at each step is of the form \\((I + \\Delta t \\alpha A_s)u^{n+1} = u^n\\). If the spatial operator \\(A_s\\) (from the elliptic part \\(\\nabla^2 u\\)) is SPD, then the full system matrix \\(A = (I + \\Delta t \\alpha A_s)\\) is also SPD.\nHowever, if the problem includes a convection (or advection) term, such as in the convection-diffusion equation (\\(\\frac{\\partial u}{\\partial t} + v \\cdot \\nabla u = \\alpha \\nabla^2 u\\)), the first-order spatial derivative \\(\\nabla u\\) introduces a non-symmetric component into the spatial operator. The resulting system matrix \\(A\\) will be non-symmetric.\nRecommended Solver: For pure diffusion problems solved with implicit schemes, the resulting SPD system is well-suited for CG. When a convection term is present and significant, the matrix becomes non-symmetric, and GMRES becomes the necessary choice.\n\n\nHyperbolic PDEs\nPrototype and Physics: The quintessential hyperbolic PDE is the wave equation, \\(\\frac{\\partial^2 u}{\\partial t^2} = c^2 \\nabla^2 u\\). These equations describe transport and wave propagation phenomena, such as acoustics, electromagnetics, and fluid dynamics with shocks.\nResulting Matrix Properties: The discretization of hyperbolic equations, especially when written as a first-order system or when dominated by advection terms, almost invariably leads to non-symmetric matrices. These matrices can also be indefinite (having both positive and negative eigenvalues) and are often more ill-conditioned than those from elliptic problems.\nRecommended Solver: Due to the non-symmetric and often indefinite nature of the system matrix, GMRES is the standard iterative solver for discretized hyperbolic PDEs. CG is fundamentally inapplicable in this context.\nWhile this mapping from PDE class to solver choice is a powerful and generally reliable guide, it is essential to recognize that the properties of the matrix \\(A\\) are a function of both the continuous PDE operator and the specific numerical discretization scheme chosen: \\(A = \\text{Discretize}(\\text{Operator}_{\\text{PDE}})\\). For example, while the Laplacian is an elliptic operator, discretizing it with certain non-standard finite difference stencils or mixed finite element methods can lead to non-symmetric or indefinite saddle-point systems. An expert practitioner must therefore consider the details of the numerical method when selecting a solver, not just the broad classification of the PDE. This understanding underscores why robust numerical libraries often query the properties of the matrix itself (e.g., by testing for symmetry) rather than relying solely on user-provided metadata about the problem’s physical origin.\n\n\n\n\n\n\n\n\n\n\nPDE Class\nPhysical Example\nTypical Matrix Properties from Standard Discretization\nRecommended Iterative Solver\nCommon Preconditioners\n\n\n\n\nElliptic\nSteady-State Heat Conduction, Electrostatics\nSymmetric Positive-Definite (SPD)\nConjugate Gradient (CG)\nIncomplete Cholesky (IC), Multigrid\n\n\nParabolic (Diffusion)\nTransient Heat Transfer\nSymmetric Positive-Definite (SPD) (with implicit schemes)\nConjugate Gradient (CG)\nIncomplete Cholesky (IC), SSOR\n\n\nParabolic (Convection)\nPollutant Transport\nNon-symmetric\nGMRES\nIncomplete LU (ILU)\n\n\nHyperbolic\nAcoustics, Wave Propagation\nNon-symmetric, often indefinite and ill-conditioned\nGMRES\nIncomplete LU (ILU), Domain Decomposition\n\n\n\nTable 3: PDE Characteristics and Recommended Solver/Preconditioner Pairings"
  },
  {
    "objectID": "Blog/iterative_solvers.html#part-iv-the-crucial-enhancement-preconditioning",
    "href": "Blog/iterative_solvers.html#part-iv-the-crucial-enhancement-preconditioning",
    "title": "Iterative solvers for linear system of equations",
    "section": "Part IV: The Crucial Enhancement: Preconditioning",
    "text": "Part IV: The Crucial Enhancement: Preconditioning\nThe successful application of iterative methods to the large, complex linear systems arising from PDEs is rarely a simple matter of choosing a solver and running it. More often than not, the raw performance of a method like CG or GMRES is insufficient for practical use. The convergence can be painfully slow, or it may fail altogether. This section addresses this critical challenge and introduces preconditioning, the single most important family of techniques for making iterative solvers robust, efficient, and truly powerful.\n\nSection 6: The “Why” of Preconditioning: Battling the Condition Number\nThe convergence rate of Krylov subspace methods is intimately linked to the spectral properties of the coefficient matrix \\(A\\). For a problem to be “easy” for an iterative solver, the matrix must be “well-conditioned.”\n\nThe Problem: Ill-Conditioning and the Condition Number\nThe metric that quantifies the “difficulty” of a linear system is the condition number, denoted \\(\\kappa(A)\\). Formally, it is defined as \\(\\kappa(A) = ||A|| \\cdot ||A^{-1}||\\), where \\(||\\cdot||\\) is some matrix norm. For the SPD matrices relevant to CG, the 2-norm condition number has a simple and intuitive interpretation: it is the ratio of the largest eigenvalue to the smallest eigenvalue of the matrix, \\(\\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}\\).\nA small condition number (close to 1) indicates a well-conditioned problem. A very large condition number signifies an ill-conditioned problem, meaning the matrix is close to being singular (non-invertible). The effect of the condition number on iterative solver performance is dramatic:\n\nFor the Conjugate Gradient method, the number of iterations required to reach a certain tolerance is approximately proportional to the square root of the condition number, i.e., iterations \\(\\propto \\sqrt{\\kappa(A)}\\).\nFor GMRES, the relationship is more complex and depends on the full distribution of eigenvalues in the complex plane, but its convergence is also severely hampered by a large condition number.\n\nA major source of ill-conditioned systems is the discretization of PDEs. As the discretization mesh is refined to achieve higher physical accuracy (i.e., the mesh spacing \\(h\\) goes to zero), the condition number of the resulting matrix \\(A\\) typically blows up. For a second-order elliptic problem, \\(\\kappa(A)\\) grows like \\(O(h^{-2})\\). This creates a vicious cycle: the very act of improving the physical model’s accuracy makes the resulting algebraic problem exponentially harder to solve.\n\n\nThe Solution: Preconditioning\nPreconditioning is a strategy to combat this ill-conditioning. The core idea is not to solve the original system \\(Ax = b\\), but to solve a mathematically equivalent system that has more favorable spectral properties. This is achieved by introducing a preconditioner, which is a matrix \\(M\\) that is, in some sense, a cheap and simple approximation of \\(A\\). The preconditioned system can be formed in several ways:\nLeft Preconditioning: The system is transformed to \\((M^{-1}A)x = M^{-1}b\\). The iterative solver is then applied to the matrix \\(M^{-1}A\\) and the right-hand side \\(M^{-1}b\\).\nRight Preconditioning: The system is transformed to \\((AM^{-1})y = b\\), where the original solution is recovered via \\(x = M^{-1}y\\). Here, the solver is applied to the matrix \\(AM^{-1}\\). A key advantage of this approach is that the original residual \\(r = b - Ax\\) can still be monitored for convergence, which is often desirable.\nSymmetric Preconditioning: When \\(A\\) is SPD and we wish to use CG, it is crucial that the preconditioned matrix also be SPD. This is achieved with a preconditioner of the form \\(M = CC^T\\), where \\(C\\) is non-singular. The system becomes \\((C^{-1}AC^{-T})y = C^{-1}b\\), with \\(x = C^{-T}y\\). The matrix \\(C^{-1}AC^{-T}\\) is guaranteed to be SPD.\nThe ideal preconditioner \\(M\\) must satisfy two competing objectives:\nEffectiveness: \\(M\\) must be a good approximation of \\(A\\), such that the preconditioned matrix (\\(M^{-1}A\\) or \\(AM^{-1}\\)) is close to the identity matrix. This ensures that its condition number is close to 1, leading to rapid convergence.\nEfficiency: The action of the preconditioner, which involves solving a system of the form \\(Mz = r\\) for \\(z\\), must be computationally very cheap to perform at every iteration of the solver.\nThis duality defines the central challenge of preconditioning. The perfect preconditioner is \\(M = A\\), which makes \\(\\kappa(M^{-1}A) = \\kappa(I) = 1\\) and leads to convergence in one iteration. However, solving \\(Mz = r\\) is then equivalent to solving the original hard problem, making it useless. Conversely, the cheapest preconditioner is \\(M = I\\). Solving \\(Iz = r\\) is trivial, but it does nothing to improve the condition number. All practical preconditioners are therefore sophisticated compromises that lie on a spectrum between these two extremes. They are designed to capture the essential features of \\(A\\) that cause ill-conditioning while remaining simple enough to be inverted efficiently.\n\n\n\n\n\n\n\n\n\n\nPreconditioner Family\nCore Idea\nCost to Apply (per iteration)\nQuality/Effectiveness\nTypical Use Case\n\n\n\n\nSimple/Stationary (Jacobi, GS, SOR)\nUse a simple part of \\(A\\) (e.g., diagonal) as the preconditioner.\nVery Low, \\(O(N)\\)\nLow to Moderate. Effective for diagonally dominant matrices.\nGeneral-purpose first attempt; smoothers in Multigrid.\n\n\nFactorization-based (ILU/IC)\nCompute a sparse, approximate factorization \\(A \\approx \\tilde{L}\\tilde{U}\\).\nLow, cost of sparse triangular solves.\nModerate to High. A powerful “black-box” technique.\nGeneral-purpose preconditioning for sparse systems from PDEs.\n\n\nProblem-Specific (Multigrid, DD)\nExploit the underlying geometry and physics of the PDE.\nLow to Moderate, but with higher setup cost.\nVery High, often “optimal” (\\(O(N)\\) solvers).\nLarge-scale PDE problems, especially elliptic, on serial or parallel machines.\n\n\n\nTable 4: A Comparative Overview of Preconditioning Techniques\n\n\n\nSection 7: A Catalogue of Preconditioning Techniques\nThis section provides a detailed examination of common “black-box” or general-purpose preconditioning techniques. These are often the first methods to try when the specific structure of a problem is either unknown or not easily exploitable. They form the foundation of many preconditioning strategies.\n\n7.1 Classical Stationary Methods as Preconditioners\nThe classical iterative methods—Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR)—can be repurposed as simple and computationally inexpensive preconditioners. Their structure is derived from a splitting of the matrix \\(A\\) into its diagonal (\\(D\\)), strictly lower triangular (\\(-L\\)), and strictly upper triangular (\\(-U\\)) parts, such that \\(A = D - L - U\\).\nJacobi Preconditioner:\nMathematical Structure: The Jacobi preconditioner is the simplest possible choice: it uses only the diagonal of the matrix \\(A\\). The preconditioner is defined as \\(M_J = D\\).\nRationale: The action of this preconditioner, solving \\(Mz = r\\), reduces to a simple, perfectly parallelizable vector scaling operation: \\(z = D^{-1}r\\), where \\(D^{-1}\\) is a diagonal matrix whose entries are the reciprocals of the diagonal entries of \\(A\\). This is extremely cheap to compute. The Jacobi preconditioner is effective only when the matrix \\(A\\) is strongly diagonally dominant, meaning the magnitude of each diagonal entry is large compared to the sum of the magnitudes of the off-diagonal entries in its row.\nPython Snippet (for GMRES):\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import gmres, LinearOperator\nfrom scipy.sparse import csc_matrix\n\n# Assume A (a sparse matrix) and b are defined\n# A = csc_matrix(...)\n# b = np.array(...)\n\n# Create the Jacobi preconditioner M_inv = D^-1\ndiagonal_of_A = A.diagonal()\nif np.any(diagonal_of_A == 0):\n    raise ValueError(\"Matrix has zero on diagonal, Jacobi is not applicable.\")\n\nM_inv = diags(1.0 / diagonal_of_A)\n\n# The preconditioner M is defined by its action M_inv * r\ndef apply_jacobi_precond(r):\n    return M_inv @ r\n\nM = LinearOperator(A.shape, matvec=apply_jacobi_precond)\n\n# Solve the preconditioned system\nx, exit_code = gmres(A, b, M=M, tol=1e-8)\nGauss-Seidel (GS) and Successive Over-Relaxation (SOR) Preconditioners:\nMathematical Structure: The Gauss-Seidel preconditioner uses the lower triangular part of \\(A\\), including the diagonal: \\(M_{GS} = D - L\\). Applying this preconditioner requires solving the lower triangular system \\((D - L)z = r\\), which is done efficiently via forward substitution. The SOR preconditioner is a generalization, defined as \\(M_{SOR} = \\frac{1}{\\omega}(D - \\omega L)\\), where \\(\\omega \\in (0, 2)\\) is a relaxation parameter that can accelerate convergence.\nRationale: By incorporating the lower triangular part of \\(A\\), GS and SOR preconditioners capture more information about the matrix than the simple Jacobi diagonal, often leading to better convergence. However, the forward substitution process is inherently sequential, making these preconditioners more difficult to parallelize than Jacobi. In the context of advanced methods, these stationary methods are rarely used as standalone preconditioners for Krylov solvers. Instead, their true value lies in their role as smoothers within Multigrid cycles. They are exceptionally good at damping high-frequency (oscillatory) error components, which is precisely the task of the smoother.\n\n\n7.2 Incomplete Factorization Preconditioners (ILU/IC)\nIncomplete factorization preconditioners are among the most powerful and popular general-purpose techniques for matrices arising from PDEs. They are a direct attempt to address the “perfect but useless” nature of a full LU factorization as a preconditioner.\nMathematical Structure: The core idea is to compute an approximate LU factorization, \\(A \\approx \\tilde{L}\\tilde{U}\\), where \\(\\tilde{L}\\) and \\(\\tilde{U}\\) are sparse lower and upper triangular matrices. The preconditioner is then \\(M = \\tilde{L}\\tilde{U}\\). Applying the preconditioner involves solving \\(Mz = r\\), which is done via a two-step forward and backward substitution: solve \\(\\tilde{L}y = r\\) for \\(y\\), then solve \\(\\tilde{U}z = y\\) for \\(z\\). This is efficient as long as \\(\\tilde{L}\\) and \\(\\tilde{U}\\) remain sparse. For SPD matrices, the symmetric analogue, Incomplete Cholesky (IC) factorization (\\(A \\approx \\tilde{L}\\tilde{L}^T\\)), is used.\nRationale - Controlling Fill-in: The key to these methods is to perform a standard factorization process but to strategically discard entries to prevent excessive fill-in and maintain sparsity. There are several strategies for this:\nILU(0) / IC(0): This is the simplest variant, where the sparsity pattern of the incomplete factors \\(\\tilde{L}\\) and \\(\\tilde{U}\\) is constrained to be exactly the same as the sparsity pattern of the original matrix \\(A\\). No new non-zero entries are allowed. This is computationally cheap and requires minimal extra storage, but its effectiveness can be limited.\nILUT (Incomplete LU with Thresholding): This is a more robust and flexible approach. During the factorization, fill-in is permitted, but any newly created entry whose magnitude is below a specified drop tolerance (droptol) is discarded. This allows the user to tune the trade-off between the accuracy of the preconditioner and its storage/computational cost. A smaller tolerance results in a denser, more accurate preconditioner that accelerates convergence more but is more expensive to compute and apply.\nPython Snippet (ILU Preconditioner with SciPy):\nimport numpy as np\nfrom scipy.sparse.linalg import spilu, gmres, LinearOperator\nfrom scipy.sparse import csc_matrix\n\n# Assume A (a sparse matrix) and b are defined\n# A = csc_matrix(...)\n# b = np.array(...)\n\n# Create the ILUT preconditioner object\n# drop_tol controls dropping of small terms, fill_factor controls memory usage\nilu = spilu(A, drop_tol=1e-5, fill_factor=20)\n\n# Define the action of the preconditioner M^-1\ndef apply_ilu_precond(r):\n    return ilu.solve(r)\n\nM = LinearOperator(A.shape, matvec=apply_ilu_precond)\n\n# Solve the preconditioned system using GMRES\nx, exit_code = gmres(A, b, M=M, tol=1e-8)\nThe preconditioners discussed in this section represent a spectrum of generality. Jacobi is the most basic and broadly applicable, but often the weakest. ILU is significantly more powerful and serves as a robust black-box choice for a wide array of problems arising from PDEs. However, its construction can be complex, and it may still fail or perform poorly for extremely ill-conditioned or indefinite systems. The failure of these general algebraic methods to adequately solve a problem is often a strong signal that the problem possesses a special structure that is not being exploited. This realization motivates the development of the problem-aware, physics-based preconditioners discussed in the next section, which represent the state of the art for high-performance scientific computing.\n\n\n\nSection 8: Advanced, Problem-Aware Preconditioners for PDEs\nWhen general-purpose algebraic preconditioners are insufficient, the next step is to employ methods that are explicitly designed to exploit the underlying structure of the PDE problem. These advanced techniques, such as Multigrid and Domain Decomposition methods, are not just algebraic manipulations; they are numerical algorithms that incorporate knowledge of the problem’s physics and geometry to construct highly effective, often optimal, preconditioners.\n\n8.1 Multigrid Methods\nMultigrid is widely regarded as one of the most efficient solution methods for the linear systems arising from elliptic PDEs, often achieving optimal complexity, meaning the computational cost to solve the system is proportional to the number of unknowns, \\(O(N)\\).\nMathematical Structure and Rationale: The power of multigrid stems from a fundamental insight into the nature of error in iterative methods. Simple iterative methods like Jacobi or Gauss-Seidel, when applied to PDE problems, are very effective at reducing high-frequency (or oscillatory) components of the error but are extremely inefficient at reducing low-frequency (or smooth) error components. Smooth error components change very little between adjacent grid points, so local relaxation operations have little effect on them.\nThe genius of multigrid is to recognize that an error component that is smooth on a fine grid will appear more oscillatory on a coarser grid. The method leverages a hierarchy of grids, from the fine grid where the solution is desired down to a very coarse grid. The core idea is to use a simple iterative method to handle the high-frequency error on the fine grid and then transfer the remaining smooth error to a coarser grid, where it can be eliminated efficiently.\nA single multigrid cycle consists of the following steps:\n\nPre-Smoothing: On the current (fine) grid, apply a few iterations of a simple relaxation method (e.g., Gauss-Seidel). This step effectively damps the high-frequency error components.\nResidual Computation: Calculate the residual of the smoothed approximation: \\(r_f = b_f - A_f x_f\\).\nRestriction: Transfer the fine-grid residual to the next coarser grid: \\(r_c = R(r_f)\\). The restriction operator \\(R\\) performs a weighted averaging of fine-grid values to produce coarse-grid values.\nCoarse-Grid Solve: On the coarse grid, solve the residual equation \\(A_c e_c = r_c\\) to find the error correction \\(e_c\\). This step is the recursive part of the algorithm: the coarse-grid system is itself solved using a multigrid cycle. This continues until a grid is reached that is so coarse it can be solved cheaply with a direct method.\nProlongation (Interpolation): Transfer the computed error correction from the coarse grid back to the fine grid: \\(e_f = P(e_c)\\). The prolongation operator \\(P\\) interpolates the coarse-grid values to produce fine-grid values.\nCorrection: Update the fine-grid solution with the interpolated correction: \\(x_f \\leftarrow x_f + e_f\\).\nPost-Smoothing: Apply a few more relaxation sweeps to smooth out any high-frequency errors introduced by the interpolation process.\n\nThe pattern of recursion defines the type of cycle. The V-cycle is the simplest, involving one recursive call. The W-cycle makes two recursive calls at each level, making it more robust but also more expensive per cycle. The F-cycle is an intermediate compromise. When used as a preconditioner for a Krylov method like CG, one multigrid cycle serves as the application of the preconditioner inverse, \\(M^{-1}\\).\nPython Snippet (using PyAMG): Implementing a multigrid solver from scratch is a significant undertaking. Libraries like PyAMG (Algebraic Multigrid) provide powerful implementations. Algebraic Multigrid (AMG) is a variant that automatically constructs the grid hierarchy and operators based only on the matrix \\(A\\), without needing explicit geometric information.\nimport pyamg\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.linalg import cg\n\n# Assume A is a sparse matrix from a discretized elliptic PDE\n# and b is the right-hand side vector.\n# A = csr_matrix(...)\n# b = np.array(...)\n\n# 1. Create the multigrid hierarchy (this is the setup phase)\n#    'smoothed_aggregation_solver' is a common AMG method.\nml = pyamg.smoothed_aggregation_solver(A)\n\n# 2. Use the multigrid hierarchy as a preconditioner for CG.\n#    The aspreconditioner() method returns a LinearOperator that\n#    applies one V-cycle.\nM = ml.aspreconditioner(cycle='V')\n\n# 3. Solve the preconditioned system\nx, info = cg(A, b, M=M, tol=1e-8)\n\nif info == 0:\n    print(\"CG with AMG preconditioner converged.\")\nelse:\n    print(f\"CG with AMG did not converge in {info} iterations.\")\n\n\n8.2 Domain Decomposition Methods\nDomain Decomposition (DD) methods are a family of techniques fundamentally designed to enable parallel computing for PDE problems. The guiding principle is “divide and conquer.” The large, global physical domain is partitioned into a set of smaller, simpler subdomains. The original PDE problem is then solved on these subdomains concurrently, with each subdomain typically assigned to a different processor.\nMathematical Structure and Rationale: The main challenge in DD methods is to correctly enforce the solution’s continuity and the PDE’s governing laws across the artificial interfaces created between subdomains. The independent subdomain solves are coordinated through an iterative process that exchanges information across these interfaces. Like multigrid, DD methods are most often used as preconditioners for Krylov solvers. The action of the preconditioner, \\(M^{-1}r\\), involves solving the independent problems on all subdomains in parallel, followed by a communication step to update the interface values.\nThere are two main classes of DD methods:\nOverlapping Schwarz Methods: In these methods, the subdomains are constructed to have a small region of overlap with their neighbors. The iterative process is simple and intuitive: solve the PDE on subdomain \\(\\Omega_i\\), use the resulting solution values in the overlap region as boundary conditions for the solve on the neighboring subdomain \\(\\Omega_j\\), and repeat this process until the solution across all interfaces converges.\nNon-overlapping Methods (Iterative Substructuring): Here, the subdomains intersect only at their boundaries (interfaces and corners). These methods are algebraically more complex. They reformulate the global problem into one that explicitly solves for the unknowns on the interfaces, coupled with independent solves in the interior of each subdomain. Prominent examples include the FETI (Finite Element Tearing and Interconnecting) methods, which use Lagrange multipliers to enforce continuity at the interfaces, and primal methods like BDDC (Balancing Domain Decomposition by Constraints).\nPython Snippet (Conceptual): A practical implementation of a DD method requires a parallel computing framework (like MPI) and is highly complex. The following conceptual code illustrates the logic of a two-domain additive Schwarz preconditioner.\n# Conceptual snippet for a two-domain Additive Schwarz preconditioner\n# A_i: matrix for interior of subdomain i\n# B_i: matrix coupling interior of i to interface\n# S: Schur complement matrix for the interface problem\n\ndef apply_additive_schwarz(r):\n    # r is the global residual vector\n\n    # 1. Restrict residual to each subdomain\n    r1 = R1 @ r  # R1 is a restriction operator\n    r2 = R2 @ r  # R2 is a restriction operator\n\n    # 2. Solve local problems on subdomains (in parallel)\n    # This is the core of the parallel computation\n    z1 = solve_subdomain_1(A1, r1)\n    z2 = solve_subdomain_2(A2, r2)\n\n    # 3. Solve a coarse/interface problem to get global correction\n    # This step requires communication\n    r_interface = R_interface @ r\n    z_interface = solve_interface(S, r_interface)\n\n    # 4. Prolongate local solutions back to global vector\n    z = P1 @ z1 + P2 @ z2 + P_interface @ z_interface\n    return z\nThe most powerful preconditioners, like Multigrid and Domain Decomposition, are not merely algebraic constructs. Their success derives from the fact that they create a simplified but physically meaningful approximation of the original problem. An ill-conditioned matrix from a PDE reflects strong coupling across different scales or spatial regions. Multigrid directly addresses the multi-scale nature of elliptic problems by explicitly representing the problem on a hierarchy of grids, handling local physics with the smoother and global physics with the coarse-grid correction. Similarly, Domain Decomposition respects the spatial locality of physical laws by solving the problem exactly within subdomains and then iteratively correcting for the coupling between them. This reveals a deep principle in modern computational science: the most effective algorithms are often those that are “problem-aware.” To achieve true scalability and efficiency, the solver must incorporate knowledge of the physical and geometric problem it is designed to solve."
  },
  {
    "objectID": "Blog/iterative_solvers.html#conclusion",
    "href": "Blog/iterative_solvers.html#conclusion",
    "title": "Iterative solvers for linear system of equations",
    "section": "Conclusion",
    "text": "Conclusion\nIterative methods represent a cornerstone of modern scientific computing, providing the only feasible path for solving the vast linear systems that arise from the discretization of partial differential equations. The journey from fundamental concepts to state-of-the-art application reveals a landscape of increasing sophistication, driven by the need to balance computational cost, memory usage, and robustness.\nAt the heart of modern techniques lie the Krylov subspace methods, with the Conjugate Gradient (CG) method providing an elegant and optimal solution for symmetric positive-definite systems, and the Generalized Minimal Residual (GMRES) method offering a robust, if more costly, alternative for general non-symmetric problems. The choice between them is dictated by the mathematical properties of the system matrix, which are, in turn, a direct reflection of the underlying PDE’s physical character—elliptic problems typically yield SPD matrices suited for CG, while parabolic and hyperbolic problems often lead to non-symmetric systems requiring GMRES.\nHowever, the practical power of these solvers is only fully unlocked through preconditioning. The challenge of ill-conditioning, where the difficulty of the algebraic problem increases dramatically with the physical model’s fidelity, necessitates the transformation of the original system into one with more favorable spectral properties. The spectrum of preconditioners—from simple algebraic techniques like Jacobi and Incomplete LU factorization to advanced, problem-aware strategies like Multigrid and Domain Decomposition—highlights a crucial theme: the most powerful numerical methods are those that are not “black boxes” but are intelligently designed to exploit the physical and geometric structure of the problem at hand. Multigrid’s hierarchical approach to error smoothing and Domain Decomposition’s parallel “divide and conquer” strategy are prime examples of this principle.\nFor the practitioner, this leads to a clear workflow: identify the PDE class, choose the appropriate Krylov solver (CG or GMRES), and, most critically, select a preconditioner that matches the problem’s complexity and the available computational resources. For the researcher, the field remains vibrant, with ongoing work in developing more robust preconditioners for challenging multi-physics problems, adapting algorithms for emerging computer architectures, and further blurring the lines between algebraic solvers and physical models. Ultimately, the continued advancement in iterative methods is a key enabler for pushing the boundaries of scientific discovery and engineering innovation."
  }
]