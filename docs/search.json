[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\n\nSome of these blogposts are rather crude and written more as a note to myself. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Gopakumar",
    "section": "",
    "text": "I am a Research Scientist at the UK Atomic Energy Authority (UKAEA), where I lead a team developing “actionable” surrogate models for exascale simulations and data-driven models for the Fusion industry. My research focusses on enhancing machine learning models’ performance, robustness and interpretability through physics-based approaches.\nI am also a visiting researcher with the SciML group at STFC’s Rutherford Appleton Laboratory.\nConcurrently, I’m pursuing a PhD in Machine Learning under Marc Deisenroth at University College London.\n\n\nCurrent Research:\n\nPhysics-Informed Machine Learning\nContinuous Dynamics Modelling\nUncertainty Quantification\nDesign of Experiments\n\nBuilding simvue.io : AI-driven, open-source simulation management and tracking dashboard for streamlining engineering workflows. Developed with public funding from the UK Government. Currently in private \\(\\beta\\).\n\n“If there is a God, it must be a differential equation” - Bertrand Russell"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "On Surrogate Modelling for Fusion - PhysicsX (London 2024)\nFNO for Plasma Modelling (Invited) - IAEA Workshop on AI for Accelerating Fusion and Plasma Science (Vienna 2023)\nFNO for Plasma Modelling - IAEA Fusion Energy Conference (London, 2023)\nFNO for Plasma Modelling - AI for sustainability workshop @ UCL (London, 2023)\nFourier RNNs for modelling noisy physics data - IEEE ICMLA (Bahamas, 2022)\nInformed Sampling of the Plasma Hyperspace for Digital Twinning - IAEA Fusion Data Processing, Validation, Analysis (Chengdu, 2021)\nOptimising Physics Informed Neural Networks - PyTorch Ecosystem Day (Virtual, 2021)\nFluid Surrogates using Neural PDEs - SciML at RAL, STFC (Oxford, 2020)\nSolving Fluid Dynamics with Neural Networks - FusionEP (Virtual, 2020)\nData Driven Modelling of Plasma in Tokamaks - IOP Physics in the spotlight (London, 2019)\nData Driven Modelling and Control of Plasma in Fusion Reactors - O’Reilly AI London (London, 2019)"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Neural_ODE.html",
    "href": "Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#markdown",
    "href": "Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#code-cell",
    "href": "Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "Neural_ODE.html#equation",
    "href": "Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html",
    "href": "blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019)\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "blogposts/Neural_ODE.html#markdown",
    "href": "blogposts/Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#code-cell",
    "href": "blogposts/Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#equation",
    "href": "blogposts/Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#original-paper",
    "href": "blogposts/Neural_ODE.html#original-paper",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Blogposts/Neural_ODE.html",
    "href": "Blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019),\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_ODE.html",
    "href": "Blog/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_Operators.html",
    "href": "Blog/Neural_Operators.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of Surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Blog/geometry.html",
    "href": "Blog/geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. (Li et al. 2023), (Li et al. 2020)\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF.\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions.\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that klatent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nThese seems to be adjacent to the kind of structure that we are looking at with a division of power and responsibilities, with specific networks and models learning differnet parts of the task.\n\n\n\n\nReferences\n\nLi, Zongyi, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, et al. 2023. “Geometry-Informed Neural Operator for Large-Scale 3D PDEs.” In Thirty-Seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=86dXbqT5Ua.\n\n\nLi, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. “Multipole Graph Neural Operator for Parametric Partial Differential Equations.” In Advances in Neural Information Processing Systems, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6755–66. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
  }
]