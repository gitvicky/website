[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Gopakumar",
    "section": "",
    "text": "I am a Research Scientist at the UK Atomic Energy Authority (UKAEA), where I lead a team developing “actionable” surrogate models for exascale simulations and data-driven models for the Fusion industry. My research focusses on enhancing machine learning models’ performance, robustness and interpretability through physics-based approaches.\nI am also a visiting researcher with the SciML group at STFC’s Rutherford Appleton Laboratory.\nConcurrently, I’m pursuing a PhD in Machine Learning under Marc Deisenroth at University College London.\n\n\nCurrent Research:\n\nPhysics-Informed Machine Learning\nContinuous Dynamics Modelling\nUncertainty Quantification\nDesign of Experiments\n\nBuilding simvue.io : AI-driven, open-source simulation management and tracking dashboard for streamlining engineering workflows. Developed with public funding from the UK Government. Currently in private \\(\\beta\\). \n\n“If there is a God, it must be a differential equation” - Bertrand Russell"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "On Surrogate Modelling for Fusion - PhysicsX (London 2024)\nFNO for Plasma Modelling - IAEA Workshop on AI for Accelerating Fusion and Plasma Science (Vienna 2023) [Slides]\nFNO for Plasma Modelling - IAEA Fusion Energy Conference (London, 2023)\nFNO for Plasma Modelling - AI for sustainability workshop @ UCL (London, 2023)\nFourier RNNs for modelling noisy physics data - IEEE ICMLA (Bahamas, 2022)\nInformed Sampling of the Plasma Hyperspace for Digital Twinning - IAEA Fusion Data Processing, Validation, Analysis (Chengdu, 2021)\nOptimising Physics Informed Neural Networks - PyTorch Ecosystem Day (Virtual, 2021)\nFluid Surrogates using Neural PDEs - SciML at RAL, STFC (Oxford, 2020)\nSolving Fluid Dynamics with Neural Networks - FusionEP (Virtual, 2020) [Video]\nData Driven Modelling of Plasma in Tokamaks - IOP Physics in the spotlight (London, 2019)\nData Driven Modelling and Control of Plasma in Fusion Reactors - O’Reilly AI London (London, 2019) [Video]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Neural_ODE.html",
    "href": "Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#markdown",
    "href": "Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#code-cell",
    "href": "Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "Neural_ODE.html#equation",
    "href": "Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html",
    "href": "blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019)\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "blogposts/Neural_ODE.html#markdown",
    "href": "blogposts/Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#code-cell",
    "href": "blogposts/Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#equation",
    "href": "blogposts/Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#original-paper",
    "href": "blogposts/Neural_ODE.html#original-paper",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Blogposts/Neural_ODE.html",
    "href": "Blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019),\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_ODE.html",
    "href": "Blog/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_Operators.html",
    "href": "Blog/Neural_Operators.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of Surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Blog/geometry.html",
    "href": "Blog/geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. (Li et al. 2023), (Li et al. 2020)\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF.\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions.\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that klatent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nThese seems to be adjacent to the kind of structure that we are looking at with a division of power and responsibilities, with specific networks and models learning differnet parts of the task.\n\n\n\n\nReferences\n\nLi, Zongyi, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, et al. 2023. “Geometry-Informed Neural Operator for Large-Scale 3D PDEs.” In Thirty-Seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=86dXbqT5Ua.\n\n\nLi, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. “Multipole Graph Neural Operator for Parametric Partial Differential Equations.” In Advances in Neural Information Processing Systems, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6755–66. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
  },
  {
    "objectID": "Blog/Geometry.html",
    "href": "Blog/Geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. [1–4]\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF [5–7].\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions. [8]\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that latent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\n\n\n\n\n\nReferences\n\n1. Li Z, Kovachki NB, Choy C, et al (2023) Geometry-informed neural operator for large-scale 3D PDEs. In: Thirty-seventh conference on neural information processing systems\n\n\n2. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Multipole graph neural operator for parametric partial differential equations. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 6755–6766\n\n\n3. Brandstetter J, Worrall DE, Welling M (2022) Message passing neural PDE solvers. In: International conference on learning representations\n\n\n4. Li T, Zou S, Chang X, Zhang L, Deng X (2024) Predicting unsteady incompressible fluid dynamics with finite volume informed neural network. Physics of Fluids 36(4). https://doi.org/10.1063/5.0197425\n\n\n5. Sitzmann V, Martel J, Bergman A, Lindell D, Wetzstein G (2020) Implicit neural representations with periodic activation functions. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 7462–7473\n\n\n6. Serrano L, Boudec LL, Koupaı̈ AK, et al (2023) Operator learning with neural fields: Tackling PDEs on general geometries. In: Thirty-seventh conference on neural information processing systems\n\n\n7. Yin Y, Kirchmeyer M, Franceschi J-Y, Rakotomamonjy A, gallinari patrick (2023) Continuous PDE dynamics forecasting with implicit neural representations. In: The eleventh international conference on learning representations\n\n\n8. Chen Y, Hosseini B, Owhadi H, Stuart AM (2021) Solving and learning nonlinear PDEs with gaussian processes. Journal of Computational Physics 447:110668. https://doi.org/https://doi.org/10.1016/j.jcp.2021.110668"
  },
  {
    "objectID": "Personal/personal.html",
    "href": "Personal/personal.html",
    "title": "Personal Blog",
    "section": "",
    "text": "The best and the worst"
  },
  {
    "objectID": "Personal/best_and_worst.html",
    "href": "Personal/best_and_worst.html",
    "title": "The Best and the Worst",
    "section": "",
    "text": "There’s a big conundrum where I am torn between feeling that the best is yet to come and that it’s already over."
  },
  {
    "objectID": "Blog/UPE.html",
    "href": "Blog/UPE.html",
    "title": "Universal Physics Engine",
    "section": "",
    "text": "Can AI serve as a Universal Physics Engine ?\n\n\nNeed to a background on the different kind of approaches being taken:\n\nSurrogate Models of all kinds.\nFoundation Models for Physics\nGenerative Enginering - PhysicsX, Zoo.dev and the varioud likes of those.\nMath proofing approaches\nText-Code is all you need with the right software - Bring in the Karpathy tweet.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nCurrent Limitations\nPotential AI Capabilities\nNovel Solutions\nChallenges\n\n\n\n\nMathematical Framework\nLimited to specific classes of PDEs; Separate frameworks for different physics domains\nUnified mathematical framework spanning quantum to classical physics; Automatic detection of symmetries and conservation laws; Dynamic generation of problem-specific basis functions\nDevelopment of new mathematical structures beyond tensors and operators; Creation of hybrid symbolic-numerical methods; Discovery of new transformations between problem domains\nEnsuring mathematical consistency across scales; Proving convergence for new methods; Handling mathematical singularities\n\n\nGeometric Processing\nPre-defined mesh types; Manual domain decomposition; Limited handling of complex boundaries\nAutomated optimal mesh generation for arbitrary geometries; Intelligent boundary condition handling; Adaptive multi-resolution techniques\nSelf-designing coordinate systems; Topology-aware discretization; Geometry-informed basis functions\nDealing with moving boundaries; Handling topological changes; Ensuring mesh quality\n\n\nMulti-physics Coupling\nManual coupling between different physics models; Limited cross-scale interactions\nAutomated detection of relevant physics; Seamless coupling across scales; Self-adaptive model selection\nCreation of unified multi-physics formulations; Development of scale-bridging operators; Automatic derivation of reduced-order models\nMaintaining conservation properties; Handling disparate time scales; Managing computational complexity\n\n\nError Control & Stability\nFixed error estimators; Predefined stability criteria; Manual parameter tuning\nReal-time error prediction; Adaptive stability preservation; Automated parameter optimization\nDevelopment of new error metrics; Creation of self-stabilizing schemes; Learning-based error estimation\nGuaranteeing global stability; Balancing accuracy vs. efficiency; Handling chaos and sensitivity\n\n\nComputational Methods\nFixed numerical schemes; Limited parallelization; Domain-specific optimizations\nDynamic algorithm selection; Automated parallelization strategies; Problem-specific method synthesis\nCreation of new numerical algorithms; Development of quantum-inspired methods; Adaptive hybrid schemes\nScaling to large problems; Managing memory hierarchy; Ensuring reproducibility\n\n\nUser Interaction\nLimited feedback on solution quality; Fixed visualization options; Preset parameter ranges\nInteractive problem refinement; Adaptive visualization; Automated parameter exploration\nDevelopment of intuitive interfaces; Creation of explanation systems; Generation of physical insights\nCommunicating complex concepts; Handling ambiguous specifications; Providing meaningful feedback\n\n\nPhysical Consistency\nManual enforcement of conservation laws; Fixed constitutive relations; Predefined material models\nAutomatic constraint preservation; Learning-based constitutive relations; Adaptive material modeling\nDiscovery of new conservation principles; Creation of physics-informed neural operators; Development of universal material models\nEnsuring physical realizability; Handling unknown physics; Maintaining causality\n\n\nData Integration\nLimited use of experimental data; Fixed model parameters; Separate calibration steps\nReal-time data assimilation; Automated model calibration; Dynamic parameter updating\nDevelopment of physics-data hybrid methods; Creation of adaptive measurement operators; Automated experiment design\nHandling noisy data; Dealing with sparse measurements; Ensuring model validity"
  },
  {
    "objectID": "Slides/slides.html",
    "href": "Slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Plasma Surrogate Modelling using FNO - IAEA Workshop on AI in Fusion (December 2023)"
  },
  {
    "objectID": "Personal/visual_art_portfoilo.html",
    "href": "Personal/visual_art_portfoilo.html",
    "title": "Visual Art",
    "section": "",
    "text": "Kovalam December 2024"
  },
  {
    "objectID": "Blog/blog.html",
    "href": "Blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\n\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  }
]