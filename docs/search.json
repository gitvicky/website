[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Neural ODE\nGeometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignesh Gopakumar",
    "section": "",
    "text": "I am a Research Scientist at the UK Atomic Energy Authority (UKAEA), where I lead a team developing “actionable” surrogate models for exascale simulations and data-driven models for the Fusion industry. My research focusses on enhancing machine learning models’ performance, robustness and interpretability through physics-based approaches.\nI am also a visiting researcher with the SciML group at STFC’s Rutherford Appleton Laboratory.\nConcurrently, I’m pursuing a PhD in Machine Learning under Marc Deisenroth at University College London.\n\n\nCurrent Research:\n\nPhysics-Informed Machine Learning\nContinuous Dynamics Modelling\nUncertainty Quantification\nDesign of Experiments\n\nBuilding simvue.io : AI-driven, open-source simulation management and tracking dashboard for streamlining engineering workflows. Developed with public funding from the UK Government. Currently in private \\(\\beta\\). \n\n“If there is a God, it must be a differential equation” - Bertrand Russell"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Calibrated Physics-Informed Uncertainty Quantification - Workshop on Calibrating Uncertainties, University of Cambridge (Cambridge 2025) [Slides]\nOn Surrogate Modelling for Fusion - PhysicsX (London 2024)\nFNO for Plasma Modelling - IAEA Workshop on AI for Accelerating Fusion and Plasma Science (Vienna 2023) [Slides]\nFNO for Plasma Modelling - IAEA Fusion Energy Conference (London, 2023)\nFNO for Plasma Modelling - AI for sustainability workshop @ UCL (London, 2023)\nFourier RNNs for modelling noisy physics data - IEEE ICMLA (Bahamas, 2022) [Slides]\nInformed Sampling of the Plasma Hyperspace for Digital Twinning - IAEA Fusion Data Processing, Validation, Analysis (Chengdu, 2021)\nOptimising Physics Informed Neural Networks - PyTorch Ecosystem Day (Virtual, 2021)\nFluid Surrogates using Neural PDEs - SciML at RAL, STFC (Oxford, 2020)\nSolving Fluid Dynamics with Neural Networks - FusionEP (Virtual, 2020) [Video] [Slides]\nData Driven Modelling of Plasma in Tokamaks - IOP Physics in the spotlight (London, 2019) [Slides]\nData Driven Modelling and Control of Plasma in Fusion Reactors - O’Reilly AI London (London, 2019) [Video]"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Neural_ODE.html",
    "href": "Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#markdown",
    "href": "Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Neural_ODE.html#code-cell",
    "href": "Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "Neural_ODE.html#equation",
    "href": "Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html",
    "href": "blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019)\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "blogposts/Neural_ODE.html#markdown",
    "href": "blogposts/Neural_ODE.html#markdown",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#code-cell",
    "href": "blogposts/Neural_ODE.html#code-cell",
    "title": "Neural ODEs",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#equation",
    "href": "blogposts/Neural_ODE.html#equation",
    "title": "Neural ODEs",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\Sigma = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "blogposts/Neural_ODE.html#original-paper",
    "href": "blogposts/Neural_ODE.html#original-paper",
    "title": "Neural ODEs",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/"
  },
  {
    "objectID": "Blogposts/Neural_ODE.html",
    "href": "Blogposts/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Original Paper\nNeural ODE, introduced in (Chen et al. 2019),\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_ODE.html",
    "href": "Blog/Neural_ODE.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "Blog/Neural_Operators.html",
    "href": "Blog/Neural_Operators.html",
    "title": "Neural ODEs",
    "section": "",
    "text": "Neural ODE, introduced in (Chen et al. 2019), represents a class of neural networks capable of performing continuous dynamics modelling. Based on the idea that resnets and recurrent models operate with transformations in the hidden state \\[h_{t+1}= h_t + f(h_t, \\theta_t), \\; \\text{as} \\; \\Delta t \\rightarrow 0,\\] the neural network parameterises the continous dynamics of an ODE \\[\\frac{dh(t)}{dt} = f(h(t), t, \\theta)\\].\nThe key contributions from this paper are:\n1. Modelling continous dynamics \n2. Obeying an Ordinary Differential Equation (ODE)\n3. Using the adjoint-method to scale to deeper and more complex networks without memory constraints. \nBefore we dive into the details of this work. Lets take a look at what it means to have a neural network to obey an ODE.\nODEs are solved as a systems of linear equations Here is a Python code cell:\n\nimport os\nos.cpu_count()\n\n12\n\n\n\n\n\n\nReferences\n\nChen, Ricky T. Q., Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. 2019. “Neural Ordinary Differential Equations.” https://arxiv.org/abs/1806.07366."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2024\n\n\n\nUncertainty Quantification of Surrogate Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela,\nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2408.09881, 2024\nTLDR: Guaranteed and valid error bars across spatio-temporal domains using conformal prediction.\n\nPaper Code\n\n\n\n\n\n\n\nValid Error Bars for Neural Weather Models using Conformal Prediction\nVignesh Gopakumar, Ander Gray, Joel Oskarsson, Lorenzo Zanisi, Stanislas Pamela, \nDaniel Giles, Matt J. Kusner, Marc Peter Deisenroth\narXiv preprint arXiv:2406.14483, 2024\nTLDR: Marginal conformal prediction as a method of guaranteed error bars across neural weather models.  \nPaper Code\n\n\n\n\n\n\n\nPlasma Surrogate Modelling using Fourier Neural Operators\nVignesh Gopakumar, Stanislas Pamela, Lorenzo Zanisi, Zongyi Li, Ander Gray, \nDaniel Brennand, Nitesh Bhatia, Gregory Stathopoulos, Matt Kusner, \nMarc Peter Deisenroth, Anima Anandkumar, JOREK Team, MAST Team\nNuclear Fusion, Volume 64, Number 5, 2024\nTLDR: Multi-variable FNO designed to model the plasma evolution within a Tokamak across both simulations and experiment on the MAST Tokamak.\n\nPaper Code\n\n\n\n\n\n\n\n2023\n\n\n\nFourier-RNNs for Modelling Noisy Physics data\nVignesh Gopakumar, Lorenzo Zanisi, Stanislas Pamela\narXiv preprint arXiv:2302.06534, 2023\nTLDR: Recurrent Fourier neural operators with hidden state representations for non-markovian physcial modelling.\n\nPaper\n\n\n\n\n\n\n\nLoss Landscape Engineering via Data Regulation on PINNs\nVignesh Gopakumar, Stanislas Pamela, Debasmita Samaddar\nMachine Learning with Applications, Volume 12, 2023\nTLDR: Impact Data-Regulation has on smoothening the loss landscape of physics-informed neural networks for better convergence.\nPaper Code\n\n\n\n\n\n\n\n2022\n\n\n\nImage Mapping the Temporal Evolution of Edge Characteristics in Tokamaks using Neural Networks\nVignesh Gopakumar, Debasmita Samaddar\nMachine Learning: Science and Technology, Volume 1, Number 1, 2020\nTLDR: Branched fully convolutional network designed to emulate the plasma at the scrape-off layer with coupled plasma and neutral behaviour.\n\nPaper\n\n\n\n\n\n\n\nYou can find the latest list of publications in my google scholar page."
  },
  {
    "objectID": "Blog/geometry.html",
    "href": "Blog/geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. (Li et al. 2023), (Li et al. 2020)\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF.\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions.\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that klatent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nThese seems to be adjacent to the kind of structure that we are looking at with a division of power and responsibilities, with specific networks and models learning differnet parts of the task.\n\n\n\n\nReferences\n\nLi, Zongyi, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Mohammad Amin Nabian, et al. 2023. “Geometry-Informed Neural Operator for Large-Scale 3D PDEs.” In Thirty-Seventh Conference on Neural Information Processing Systems. https://openreview.net/forum?id=86dXbqT5Ua.\n\n\nLi, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya, and Anima Anandkumar. 2020. “Multipole Graph Neural Operator for Parametric Partial Differential Equations.” In Advances in Neural Information Processing Systems, edited by H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, 33:6755–66. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf."
  },
  {
    "objectID": "Blog/Geometry.html",
    "href": "Blog/Geometry.html",
    "title": "Geometry for Neural-PDE surrogates",
    "section": "",
    "text": "Most neural PDE solvers within the research ecosystem are built looking at regular geometry with structured uniform grids. Though this conjures up a great starting point to look at solving complex phenomena, it is only mildly representative of the computational physics cases that we are interested in practice. Regular grids have been a beneficial starting point as it is easy to port the wealth of research done within computer vision with its clear 3D vioxel structure to more scientific applications. Though there are certain areas where regular geometry finds application, before we can move this research to product, we will need to address for irregular unstructured grids at vast scales (~ millions of cells).\nThis has been a large area of interest in the recent years and the work at modelling spatio-temporal data across irregular geometries can be split into the following categories:\n\nGraph-based Methods : GNNs, GCN, GINOs. Involves message passing to emphasise the geometric structure. [1–4]\nPoint Cloud Approaches : Coordinate based-MLPs, Neural Fields, INRs. No emphasis on graph structure apart from coordinates and perhaps the associated SDF [5–7].\n\nKernel-based Methods : GPs. Function fitting and infering the field values at specific geometric positions. [8]\n\nThe table below outlines a structured comparison table of machine learning methods for handling irregular geometries:\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nLimitations\n\n\n\n\nGraph based Methods\n- Natural handling of irregular spatial relationships through message passing- Excellent capture of local geometric features- Works with varying numbers of spatial points- Maintains permutation invariance- Easy incorporation of additional features at spatial locations\n- Struggles with global geometric patterns- Computationally expensive for dense graphs- Performance depends heavily on graph construction- Training stability issues in deep architectures- Message passing can be inefficient for long-range dependencies\n\n\nPoint Cloud Approaches\n- Direct processing of unstructured data without connectivity information- Effective for 3D data representation- Handles varying point densities- Natural permutation invariance- Flexible with varying point cloud sizes\n- Difficulty capturing fine geometric details- Sensitive to point density variations- Requires careful input data preprocessing- Poor scaling of memory requirements with point count- May miss local spatial relationships\n\n\nKernel-Based Methods\n- Natural uncertainty quantification- Effective handling of irregular sampling- Works well with limited data- Incorporates prior knowledge via kernel design- Provides smooth interpolation\n- Poor scaling with dataset size- Sensitive to kernel choice- Hyperparameter selection challenges- Struggles with discontinuities- Difficulty capturing sharp geometric features\n\n\n\nBut for large scale neural PDE models modelling complex spatio-temporal domain with large context lengths, the modelling task is split into a encoder-processor-decoder architecture as shown below.\n\n\n\n\n\nflowchart LR\n  A(Encoder) --&gt; B(Processor)\n  B --&gt; B\n  B --&gt; C(Decoder)\n\n\n\n\n\n\nThe layout breaks down the modelling task into three parts. The encoder works within the spatial domain to deconstruct the initial conditions of the fields and the inherent geometry of the problem into a latent, more structured space. The processor learns to map the temporal evolution of the PDE within that latent space. The choice of the processor is often taken to be NN architecture that is quick to evaluate and could be setup as a Neural ODE. The decoder maps the final solution from the latent space to the actual unstructured grid that we are interested in.\nUsing a structured NO such as the FNO, that has inductive bias, quick to evaluate has found to be rather beneficial, but the open questions still lie within the choice of the encoder-decoder. Papers such as [1, 3] utilise graph neural networks as the encoder-decoder, but however they have significant computational and memory requirements.\nI am currently exploring the idea of maybe using coordinate-based MLPs for the encoder and a GP as the decoder, with a neural operator deployed as a neural ODE with operator-splitting as the processor (neural-UDE):\n\n\n\n\n\nflowchart LR\n  A(NeRF) --&gt; B(Neural-UDE)\n  B --&gt; B\n  B --&gt; C(GP)\n\n\n\n\n\n\nThe challenge with the NeRF is that though allow for continuous space representations, they are terrible at enforcing strict boundaries within the domain and have soft gradients across them. These leads to losing strutcures with significant gradients being lost in the initial condition and geometry. The advantages could be that the IC might not have sharp enough gradients so representative capacity might not be that much of a concern. The other advantage is that they are small, light models, based on simple MLPs.\nAs for the GP, the challenges remain the same as always, whether they can be scaled to handle the dimension and size as we might need. Might have to train multiple GPs or Mixture of GPs? The advantage is that we can get UQ built into the models.\nThis approach brings in a division of labour, having models learn a specific task and then connected together in an approach similar to integrated modelling. Is this a kind of Mixture of Experts ? \n\nExploring Kernel Methods\n\nConsverative Remapping\n\n\nNon-Uniform FFT\nFFT over non-uniform grids (nuFFT) as demonstrated within this pytorch repository, where the grids are structured using a Kaisser-Bessel window functions as interpolation kernels. Essentially we are using a weighted kernel approach to move from unstructured grids to structured grids.\n\n\n\n\n\n\nReferences\n\n1. Li Z, Kovachki NB, Choy C, et al (2023) Geometry-informed neural operator for large-scale 3D PDEs. In: Thirty-seventh conference on neural information processing systems\n\n\n2. Li Z, Kovachki N, Azizzadenesheli K, et al (2020) Multipole graph neural operator for parametric partial differential equations. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 6755–6766\n\n\n3. Brandstetter J, Worrall DE, Welling M (2022) Message passing neural PDE solvers. In: International conference on learning representations\n\n\n4. Li T, Zou S, Chang X, Zhang L, Deng X (2024) Predicting unsteady incompressible fluid dynamics with finite volume informed neural network. Physics of Fluids 36(4). https://doi.org/10.1063/5.0197425\n\n\n5. Sitzmann V, Martel J, Bergman A, Lindell D, Wetzstein G (2020) Implicit neural representations with periodic activation functions. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H (eds) Advances in neural information processing systems. Curran Associates, Inc., pp 7462–7473\n\n\n6. Serrano L, Boudec LL, Koupaı̈ AK, et al (2023) Operator learning with neural fields: Tackling PDEs on general geometries. In: Thirty-seventh conference on neural information processing systems\n\n\n7. Yin Y, Kirchmeyer M, Franceschi J-Y, Rakotomamonjy A, gallinari patrick (2023) Continuous PDE dynamics forecasting with implicit neural representations. In: The eleventh international conference on learning representations\n\n\n8. Chen Y, Hosseini B, Owhadi H, Stuart AM (2021) Solving and learning nonlinear PDEs with gaussian processes. Journal of Computational Physics 447:110668. https://doi.org/https://doi.org/10.1016/j.jcp.2021.110668"
  },
  {
    "objectID": "Personal/personal.html",
    "href": "Personal/personal.html",
    "title": "Personal Blog",
    "section": "",
    "text": "The best and the worst"
  },
  {
    "objectID": "Personal/best_and_worst.html",
    "href": "Personal/best_and_worst.html",
    "title": "The Best and the Worst",
    "section": "",
    "text": "There’s a big conundrum where I am torn between feeling that the best is yet to come and that it’s already over."
  },
  {
    "objectID": "Blog/UPE.html",
    "href": "Blog/UPE.html",
    "title": "Universal Physics Engine",
    "section": "",
    "text": "Can AI serve as a Universal Physics Engine ?\n\n\nIn Stephen Wolfram’s (one of the legends in the field of computational physics and mathematics) latest writing, he explores ideas on the fields and methods with which he thinks AI will impact scientific disciplines. He starts the blog with:\n“To the ultimate question of whether AI can solve Science, we’re going to see that the answer is inevitably and firmly no.”\nSo probably there is not much of a reason for me to explore the idea of building a universal physics engine using AI, but hey I love attempting the impossible (why do you think I ended up doing Fusion !!!).\nScience can be broadly defined to fall under three categories:\n\n\n\n\n\nflowchart LR\n  A(Prediction)\n  B(Discovery)\n  C(Explanation)\n\n\n\n\n\n\nNeed to a background on the different kind of approaches being taken:\n\nSurrogate Models of all kinds.\nFoundation Models for Physics\nGenerative Enginering - PhysicsX, Zoo.dev and the varioud likes of those. Generative models for statistical physics.\n\nMath proofing approaches\nText-Code is all you need with the right software - Bring in the Karpathy tweet.\n\n\n\n\n\n\n\n\n\n\n\nAspect\nCurrent Limitations\nPotential AI Capabilities\nNovel Solutions\nChallenges\n\n\n\n\nMathematical Framework\nLimited to specific classes of PDEs; Separate frameworks for different physics domains\nUnified mathematical framework spanning quantum to classical physics; Automatic detection of symmetries and conservation laws; Dynamic generation of problem-specific basis functions\nDevelopment of new mathematical structures beyond tensors and operators; Creation of hybrid symbolic-numerical methods; Discovery of new transformations between problem domains\nEnsuring mathematical consistency across scales; Proving convergence for new methods; Handling mathematical singularities\n\n\nGeometric Processing\nPre-defined mesh types; Manual domain decomposition; Limited handling of complex boundaries\nAutomated optimal mesh generation for arbitrary geometries; Intelligent boundary condition handling; Adaptive multi-resolution techniques\nSelf-designing coordinate systems; Topology-aware discretization; Geometry-informed basis functions\nDealing with moving boundaries; Handling topological changes; Ensuring mesh quality\n\n\nMulti-physics Coupling\nManual coupling between different physics models; Limited cross-scale interactions\nAutomated detection of relevant physics; Seamless coupling across scales; Self-adaptive model selection\nCreation of unified multi-physics formulations; Development of scale-bridging operators; Automatic derivation of reduced-order models\nMaintaining conservation properties; Handling disparate time scales; Managing computational complexity\n\n\nError Control & Stability\nFixed error estimators; Predefined stability criteria; Manual parameter tuning\nReal-time error prediction; Adaptive stability preservation; Automated parameter optimization\nDevelopment of new error metrics; Creation of self-stabilizing schemes; Learning-based error estimation\nGuaranteeing global stability; Balancing accuracy vs. efficiency; Handling chaos and sensitivity\n\n\nComputational Methods\nFixed numerical schemes; Limited parallelization; Domain-specific optimizations\nDynamic algorithm selection; Automated parallelization strategies; Problem-specific method synthesis\nCreation of new numerical algorithms; Development of quantum-inspired methods; Adaptive hybrid schemes\nScaling to large problems; Managing memory hierarchy; Ensuring reproducibility\n\n\nUser Interaction\nLimited feedback on solution quality; Fixed visualization options; Preset parameter ranges\nInteractive problem refinement; Adaptive visualization; Automated parameter exploration\nDevelopment of intuitive interfaces; Creation of explanation systems; Generation of physical insights\nCommunicating complex concepts; Handling ambiguous specifications; Providing meaningful feedback\n\n\nPhysical Consistency\nManual enforcement of conservation laws; Fixed constitutive relations; Predefined material models\nAutomatic constraint preservation; Learning-based constitutive relations; Adaptive material modeling\nDiscovery of new conservation principles; Creation of physics-informed neural operators; Development of universal material models\nEnsuring physical realizability; Handling unknown physics; Maintaining causality\n\n\nData Integration\nLimited use of experimental data; Fixed model parameters; Separate calibration steps\nReal-time data assimilation; Automated model calibration; Dynamic parameter updating\nDevelopment of physics-data hybrid methods; Creation of adaptive measurement operators; Automated experiment design\nHandling noisy data; Dealing with sparse measurements; Ensuring model validity"
  },
  {
    "objectID": "Slides/slides.html",
    "href": "Slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Plasma Surrogate Modelling using FNO - IAEA Workshop on AI in Fusion (December 2023)"
  },
  {
    "objectID": "Personal/visual_art_portfoilo.html",
    "href": "Personal/visual_art_portfoilo.html",
    "title": "Visual Art",
    "section": "",
    "text": "Kovalam December 2024"
  },
  {
    "objectID": "Blog/blog.html",
    "href": "Blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Geometry for Neural PDEs\nUniversal Physics Engine\n\nSome of these blogposts might seem rather crude as they maybe written as notes to self. You can find some of my earlier writing in Medium."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "href": "Blog/Inverse_kernel.html#the-mathematical-foundation-of-regularized-inversion",
    "title": "Integration by way of Convolution",
    "section": "The Mathematical Foundation of Regularized Inversion",
    "text": "The Mathematical Foundation of Regularized Inversion\nWhen we have a signal \\(y\\) that results from the convolution of an unknown signal \\(f\\) with a known kernel \\(g\\):\n\\[y = f * g\\]\nIn the frequency domain (using the Fourier transform), this becomes:\n\\[Y(\\omega) = F(\\omega) \\cdot G(\\omega)\\]\nWhere \\(Y\\), \\(F\\), and \\(G\\) are the Fourier transforms of \\(y\\), \\(f\\), and \\(g\\) respectively.\nIdeally, we could recover \\(f\\) by:\n\\[F(\\omega) = \\frac{Y(\\omega)}{G(\\omega)}\\]\nAnd then applying the inverse Fourier transform to get \\(f\\) in the time domain.\n\nThe Problem of Ill-Conditioning\nThe difficulty arises when \\(G(\\omega)\\) approaches zero at certain frequencies. This occurs with many important kernels, including our [1, -2, 1] second derivative kernel.\nFor the second derivative kernel, the frequency response is approximately:\n\\[G(\\omega) \\approx -\\omega^2\\]\nThis means \\(G(\\omega)\\) is very small near \\(\\omega = 0\\) (the DC component and low frequencies). Division by these small values causes numerical instability, amplifying noise and errors.\n\n\nTikhonov Regularization\nWhat we’re doing with epsilon is a form of Tikhonov regularization, which can be mathematically represented as:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{G(\\omega) + \\epsilon}\\]\nThis is equivalent to finding the solution to the minimization problem:\n\\[\\min_f \\|g * f - y\\|^2 + \\epsilon \\|f\\|^2\\]\nWhere the first term measures how well our recovered signal explains the observed data, and the second term penalizes large values in the solution, providing stability.\n\n\nMathematical Properties of the Regularization\nTo understand what epsilon does mathematically, let’s analyze its effect at different frequencies:\n\nWhere \\(|G(\\omega)| \\gg \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{G(\\omega)} \\approx F(\\omega)\\] The recovery is accurate at frequencies where the kernel has significant response.\nWhere \\(|G(\\omega)| \\ll \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{\\epsilon} \\approx 0\\] The recovery suppresses components at frequencies where the kernel has near-zero response.\nWhere \\(|G(\\omega)| \\approx \\epsilon\\): \\[F_{\\epsilon}(\\omega) \\approx \\frac{Y(\\omega)}{2G(\\omega)} \\approx \\frac{F(\\omega)}{2}\\] The recovery partially retrieves information, with some attenuation.\n\nThis creates a smooth transition between fully recovered frequencies and suppressed frequencies, avoiding the sharp discontinuities that would cause ringing artifacts."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "href": "Blog/Inverse_kernel.html#alternative-approaches-to-signal-recovery",
    "title": "Integration by way of Convolution",
    "section": "Alternative Approaches to Signal Recovery",
    "text": "Alternative Approaches to Signal Recovery\nThere are several alternative approaches for recovering a signal after convolution, especially for the case of integration following differentiation:\n\n1. Direct Integration (for Differential Kernels)\nSince our [1, -2, 1] kernel approximates the second derivative, integration is a natural inverse operation. We can recover an approximation to the original signal by integrating twice:\n\\[\\hat{f}(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nWhere \\(C_1\\) and \\(C_2\\) are integration constants that need to be determined from boundary conditions or additional information.\nFor discrete signals, this becomes cumulative summation:\ndef double_integrate(signal):\n    # First integration (cumulative sum)\n    first_integral = np.cumsum(signal)\n    # Second integration\n    second_integral = np.cumsum(first_integral)\n    return second_integral\nThe challenge is determining the correct integration constants, which represent the linear and constant components lost during differentiation.\n\n\n2. Wiener Deconvolution\nWiener deconvolution incorporates knowledge about the signal-to-noise ratio (SNR):\n\\[F_{\\text{Wiener}}(\\omega) = \\frac{G^*(\\omega)}{|G(\\omega)|^2 + \\frac{1}{\\text{SNR}(\\omega)}} \\cdot Y(\\omega)\\]\nWhere \\(G^*(\\omega)\\) is the complex conjugate of \\(G(\\omega)\\) and \\(\\text{SNR}(\\omega)\\) is the signal-to-noise ratio at each frequency.\nThis approach is more adaptive than simple regularization, as it adjusts the regularization based on the expected noise level at each frequency.\n\n\n3. Iterative Methods\nFor very ill-conditioned problems, iterative methods like conjugate gradient or LSMR can be more stable:\n\\[f_{k+1} = f_k + \\alpha_k(g^* * (y - g * f_k))\\]\nWhere \\(g^*\\) is the adjoint (time-reversed) kernel and \\(\\alpha_k\\) is a step size.\nThese methods gradually refine the solution, avoiding direct division in the frequency domain.\n\n\n4. Wavelet-Based Deconvolution\nWavelets provide localization in both time and frequency, making them well-suited for deconvolution problems:\n\nTransform the signal to the wavelet domain\nApply regularized inversion in the wavelet domain\nTransform back to the time domain\n\nThis approach can better handle signals with localized features and non-stationary properties."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "href": "Blog/Inverse_kernel.html#specific-case-integration-after-differentiation",
    "title": "Integration by way of Convolution",
    "section": "Specific Case: Integration After Differentiation",
    "text": "Specific Case: Integration After Differentiation\nFor your specific interest in performing integration after a differential kernel has been applied, let me explain the mathematical connection more explicitly.\nIf we have applied the second derivative kernel [1, -2, 1] to a signal \\(f\\), obtaining \\(y\\):\n\\[y[n] = f[n+1] - 2f[n] + f[n-1] \\approx \\frac{d^2f}{dt^2}\\]\nThen to recover \\(f\\), we need to integrate \\(y\\) twice. In the continuous domain, this would be:\n\\[f(t) = \\iint y(t) \\, dt\\, dt + C_1 t + C_2\\]\nIn the frequency domain, integration corresponds to division by \\(j\\omega\\). So double integration is division by \\((j\\omega)^2 = -\\omega^2\\). The frequency response of our [1, -2, 1] kernel is approximately \\(-\\omega^2\\), which means the ideal recovery filter would be \\(\\frac{1}{-\\omega^2} = -\\frac{1}{\\omega^2}\\).\nThis perfectly matches our regularized inverse:\n\\[F_{\\epsilon}(\\omega) = \\frac{Y(\\omega)}{-\\omega^2 + \\epsilon}\\]\nThe regularization term \\(\\epsilon\\) prevents division by zero at \\(\\omega = 0\\), which corresponds to the integration constants we would need to determine in the time domain approach."
  },
  {
    "objectID": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "href": "Blog/Inverse_kernel.html#practical-implementation-for-integration-recovery",
    "title": "Integration by way of Convolution",
    "section": "Practical Implementation for Integration Recovery",
    "text": "Practical Implementation for Integration Recovery\nLet me outline a robust approach for your integration task:\n\nSpectral Domain Method with Regularization\ndef recover_by_integration_spectral(signal, kernel, epsilon=1e-6):\n    n_fft = len(signal) + len(kernel) - 1\n    padded_signal = F.pad(signal, (0, n_fft - len(signal)))\n    padded_kernel = F.pad(kernel, (0, n_fft - len(kernel)))\n\n    # FFT\n    signal_fft = torch.fft.rfft(padded_signal)\n    kernel_fft = torch.fft.rfft(padded_kernel)\n\n    # Inverse filtering with regularization\n    recovered_fft = signal_fft / (kernel_fft + epsilon)\n\n    # IFFT\n    recovered = torch.fft.irfft(recovered_fft)\n\n    return recovered[:len(signal)]\nTime Domain Integration with Boundary Correction\ndef recover_by_double_integration(signal, boundary_values=None):\n    # First integration\n    first_integral = torch.cumsum(signal, dim=0)\n\n    # Correct for linear drift (first integration constant)\n    if boundary_values and 'start_slope' in boundary_values:\n        first_integral = first_integral + boundary_values['start_slope'] * torch.arange(len(signal))\n\n    # Second integration\n    second_integral = torch.cumsum(first_integral, dim=0)\n\n    # Correct for constant offset (second integration constant)\n    if boundary_values and 'start_value' in boundary_values:\n        second_integral = second_integral + boundary_values['start_value']\n\n    return second_integral\nHybrid Approach\nFor the most robust recovery, we can combine spectral and time domain methods:\ndef hybrid_recovery(signal, kernel, epsilon=1e-6):\n    # Spectral recovery for high frequencies\n    spectral_recovery = recover_by_integration_spectral(signal, kernel, epsilon)\n\n    # Time domain integration for low frequency components\n    time_recovery = recover_by_double_integration(signal)\n\n    # High-pass filter for spectral recovery\n    high_freq = high_pass_filter(spectral_recovery)\n\n    # Low-pass filter for time domain recovery\n    low_freq = low_pass_filter(time_recovery)\n\n    # Combine the two\n    return high_freq + low_freq"
  },
  {
    "objectID": "Blog/Inverse_kernel.html#conclusion",
    "href": "Blog/Inverse_kernel.html#conclusion",
    "title": "Integration by way of Convolution",
    "section": "Conclusion",
    "text": "Conclusion\nThe epsilon regularization parameter provides a mathematically sound approximation to the inverse problem, creating a balance between recovery fidelity and numerical stability. It’s essentially solving a regularized least-squares problem that finds the solution with the best trade-off between data fidelity and solution stability.\nFor the specific case of recovering a signal after applying a differential kernel, direct integration methods can be combined with spectral approaches for robust results. The key challenge is determining the integration constants (or equivalently, the low-frequency components) that were lost during differentiation.\nThe most effective approach often combines multiple methods, using the strengths of each to compensate for the weaknesses of others. This hybrid approach can provide more reliable signal recovery across a wide range of practical scenarios."
  }
]